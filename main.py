# -*- coding: utf-8 -*-
"""
åŒ»å­¦çŸ¥è¯†é—®ç­”RAGç³»ç»Ÿ - ä¸»æ‰§è¡Œè„šæœ¬
æ”¯æŒå®Œæ•´Pipelineã€å‘é‡æ•°æ®åº“é‡å»ºã€ç³»ç»Ÿè¯„ä¼°ã€Webç•Œé¢
"""

import argparse
import sys
import io
from pathlib import Path

# è®¾ç½®UTF-8ç¼–ç 
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

sys.path.append(str(Path(__file__).parent))

from config.config import *
from src.utils.logger import setup_logger

logger = setup_logger("main", LOGS_DIR / "main.log")


def run_data_collection(max_per_topic: int = 20000, workers: int = 3, clear: bool = False):
    """æ­¥éª¤1: æ•°æ®é‡‡é›†ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰"""
    logger.info("\n" + "="*60)
    logger.info("æ­¥éª¤1: æ•°æ®é‡‡é›† - PubMedåŒ»å­¦æ–‡çŒ®")
    logger.info("="*60)
    
    from src.data_processing.pubmed_crawler import PubMedCrawler
    crawler = PubMedCrawler(
        email=PUBMED_EMAIL,
        api_key=PUBMED_API_KEY,
        max_workers=workers
    )
    
    if clear:
        crawler.clear_checkpoints()
    
    crawler.crawl_all_topics(max_per_topic=max_per_topic)


def run_data_processing():
    """æ­¥éª¤2: æ•°æ®é¢„å¤„ç†"""
    logger.info("\n" + "="*60)
    logger.info("æ­¥éª¤2: æ•°æ®é¢„å¤„ç† - Sparkåˆ†å¸ƒå¼å¤„ç†")
    logger.info("="*60)
    
    from src.data_processing.data_processor import main as processor_main
    processor_main()


def run_embedding():
    """æ­¥éª¤3: å‘é‡åŒ–"""
    logger.info("\n" + "="*60)
    logger.info("æ­¥éª¤3: æ–‡æœ¬å‘é‡åŒ–")
    logger.info("="*60)
    
    from src.embedding.embedder import main as embedder_main
    embedder_main()


def run_vector_db_setup():
    """æ­¥éª¤4: æ„å»ºå‘é‡æ•°æ®åº“"""
    logger.info("\n" + "="*60)
    logger.info("æ­¥éª¤4: æ„å»ºMilvuså‘é‡æ•°æ®åº“")
    logger.info("="*60)
    
    from src.retrieval.milvus_manager import main as milvus_main
    milvus_main()


def run_evaluation(mode: str = "full", scale_factor: int = 10):
    """æ­¥éª¤5: ç³»ç»Ÿè¯„ä¼°"""
    logger.info("\n" + "="*60)
    logger.info("æ­¥éª¤5: RAGç³»ç»Ÿè¯„ä¼°")
    logger.info("="*60)
    
    from src.evaluation.unified_evaluator import UnifiedEvaluator
    evaluator = UnifiedEvaluator()
    
    if mode == "rag":
        results = evaluator.evaluate_rag_retrieval()
    elif mode == "storage":
        results = evaluator.evaluate_storage_performance()
    elif mode == "pyspark":
        results = evaluator.evaluate_pyspark_processing(scale_factor=scale_factor)
    else:
        results = evaluator.run_full_evaluation()
    
    if isinstance(results, dict) and 'overall_score' in results:
        logger.info(f"ç»¼åˆè¯„åˆ†: {results['overall_score']}/100")
    
    return results


def run_expand_data(scale_factor: int = 10):
    """æ‰©å±•æ•°æ®é›†ç”¨äºå¤§æ•°æ®æµ‹è¯•"""
    logger.info("\n" + "="*60)
    logger.info(f"ğŸ“Š æ‰©å±•æ•°æ®é›† ({scale_factor}x)")
    logger.info("="*60)
    
    from src.evaluation.data_scaler import create_scaled_dataset
    path = create_scaled_dataset(scale_factor=scale_factor)
    
    logger.info(f"âœ… æ‰©å±•æ•°æ®é›†å·²åˆ›å»º: {path}")
    return path


def run_rebuild_database(resume: bool = False, batch_size: int = 128):
    """é‡å»ºå‘é‡æ•°æ®åº“ï¼ˆæ”¯æŒ150ä¸‡æ•°æ®ï¼‰"""
    logger.info("\n" + "="*60)
    logger.info("ğŸ”„ é‡å»ºå‘é‡æ•°æ®åº“")
    logger.info(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
    logger.info(f"   æ–­ç‚¹ç»­ä¼ : {'æ˜¯' if resume else 'å¦'}")
    logger.info("="*60)
    
    from src.retrieval.milvus_manager import rebuild_database
    rebuild_database(resume=resume, batch_size=batch_size)
    
    logger.info("\nâœ… å‘é‡æ•°æ®åº“é‡å»ºå®Œæˆï¼")
    logger.info("   å¯ä»¥è¿è¡Œ python main.py --web å¯åŠ¨Webç•Œé¢æµ‹è¯•")


def run_web_interface():
    """æ­¥éª¤6: å¯åŠ¨Webç•Œé¢"""
    logger.info("\n" + "="*60)
    logger.info("æ­¥éª¤6: å¯åŠ¨Webç•Œé¢")
    logger.info("="*60)
    
    from web.app import main as web_main
    web_main()


def run_full_pipeline():
    """è¿è¡Œå®Œæ•´Pipeline"""
    logger.info("="*60)
    logger.info("ğŸš€ å¼€å§‹æ‰§è¡Œå®Œæ•´Pipeline")
    logger.info("="*60)
    
    try:
        # æ­¥éª¤1: æ•°æ®é‡‡é›†
        run_data_collection()
        
        # æ­¥éª¤2: æ•°æ®å¤„ç†
        run_data_processing()
        
        # æ­¥éª¤3: å‘é‡åŒ–
        run_embedding()
        
        # æ­¥éª¤4: æ„å»ºå‘é‡æ•°æ®åº“
        run_vector_db_setup()
        
        # æ­¥éª¤5: è¯„ä¼°
        run_evaluation()
        
        logger.info("\n" + "="*60)
        logger.info("âœ… å®Œæ•´Pipelineæ‰§è¡ŒæˆåŠŸï¼")
        logger.info("="*60)
        logger.info("\nç°åœ¨å¯ä»¥å¯åŠ¨Webç•Œé¢è¿›è¡Œæµ‹è¯•:")
        logger.info("  python main.py --web")
        
    except Exception as e:
        logger.error(f"\nâŒ Pipelineæ‰§è¡Œå¤±è´¥: {e}")
        raise


def run_spark_cluster():
    """å¯åŠ¨Sparké›†ç¾¤ï¼ˆDockerï¼‰"""
    logger.info("ğŸš€ å¯åŠ¨Sparké›†ç¾¤...")
    import subprocess
    subprocess.run(["docker", "compose", "-f", "docker/docker-compose-spark.yml", "up", "-d"])
    logger.info("âœ… Sparké›†ç¾¤å·²å¯åŠ¨")
    logger.info("   Master UI: http://localhost:8080")
    logger.info("   Master URL: spark://localhost:7077")


def run_spark_embed(use_cluster: bool = False):
    """ä½¿ç”¨Sparkåˆ†å¸ƒå¼å‘é‡åŒ–"""
    logger.info("âš¡ Sparkåˆ†å¸ƒå¼å‘é‡åŒ–")
    from src.embedding.spark_embedder import SparkEmbedder
    
    embedder = SparkEmbedder(use_cluster=use_cluster)
    input_path = PROCESSED_DATA_DIR / "parquet" / "medical_chunks.parquet"
    output_path = EMBEDDING_DATA_DIR / "spark_embeddings"
    
    if input_path.exists():
        embedder.embed_with_pandas_udf(input_path, output_path)
    embedder.stop()


def run_incremental_index(use_spark: bool = False):
    """å¯åŠ¨å¢é‡ç´¢å¼•"""
    logger.info("ğŸ“¡ å¯åŠ¨å¢é‡ç´¢å¼•...")
    from src.retrieval.spark_streaming import IncrementalIndexer
    
    indexer = IncrementalIndexer(use_spark=use_spark)
    if use_spark:
        indexer.start_spark_streaming()
    else:
        indexer.start_file_watcher(interval=30)
    
    logger.info("ç›‘å¬ä¸­... æŒ‰Ctrl+Cåœæ­¢")
    try:
        import time
        while True:
            time.sleep(10)
    except KeyboardInterrupt:
        indexer.stop()


def run_cache_prewarm():
    """é¢„çƒ­Redisç¼“å­˜"""
    logger.info("ğŸ”¥ é¢„çƒ­Redisç¼“å­˜...")
    from src.caching.redis_cache import RedisCache, VectorCacheManager
    
    cache = RedisCache()
    manager = VectorCacheManager(cache)
    
    # é¢„çƒ­å¸¸ç”¨åŒ»å­¦æŸ¥è¯¢
    common_queries = [
        "diabetes symptoms treatment",
        "cardiovascular disease prevention",
        "cancer chemotherapy side effects",
        "hypertension medication",
        "covid-19 vaccine effectiveness",
    ]
    manager.prewarm_cache(common_queries)
    logger.info(f"âœ… ç¼“å­˜é¢„çƒ­å®Œæˆ: {manager.get_stats()}")


# ==================== Kafka + Airflow åŠŸèƒ½ ====================

def run_kafka_services():
    """å¯åŠ¨Kafkaç›¸å…³æœåŠ¡"""
    logger.info("ğŸš€ å¯åŠ¨KafkaæœåŠ¡...")
    import subprocess
    
    # å…ˆåˆ›å»ºç½‘ç»œï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    subprocess.run(["docker", "network", "create", "docker_rag-network"], 
                   capture_output=True)
    
    # å¯åŠ¨Kafka + Airflow
    result = subprocess.run([
        "docker", "compose", 
        "-f", "docker/docker-compose-kafka-airflow.yml", 
        "up", "-d"
    ])
    
    if result.returncode == 0:
        logger.info("âœ… Kafka + Airflow æœåŠ¡å·²å¯åŠ¨")
        logger.info("   Kafka: localhost:9092")
        logger.info("   Kafka UI: http://localhost:8082")
        logger.info("   Airflow: http://localhost:8081 (admin/admin)")
    else:
        logger.error("âŒ æœåŠ¡å¯åŠ¨å¤±è´¥")


def run_kafka_topics_setup():
    """åˆ›å»ºKafka Topics"""
    logger.info("ğŸ“‹ åˆ›å»ºKafka Topics...")
    from src.messaging.kafka_producer import KafkaTopicManager
    
    manager = KafkaTopicManager()
    manager.create_topics()
    
    topics = manager.list_topics()
    logger.info(f"âœ… å½“å‰Topics: {topics}")


def run_kafka_crawler(use_kafka: bool = True):
    """ä½¿ç”¨Kafkaé›†æˆçš„çˆ¬è™«"""
    logger.info("ğŸ•·ï¸ å¯åŠ¨Kafkaé›†æˆçˆ¬è™«...")
    from src.messaging.kafka_integrated_crawler import KafkaIntegratedCrawler
    
    crawler = KafkaIntegratedCrawler(use_kafka=use_kafka)
    crawler.crawl_with_kafka()
    crawler.close()


def run_kafka_consumer(consumer_type: str = "processor"):
    """å¯åŠ¨Kafkaæ¶ˆè´¹è€…"""
    logger.info(f"ğŸ‘‚ å¯åŠ¨Kafkaæ¶ˆè´¹è€…: {consumer_type}")
    
    if consumer_type == "processor":
        from src.messaging.kafka_consumer import DataProcessingConsumer
        consumer = DataProcessingConsumer()
        consumer.start()
    elif consumer_type == "embedder":
        from src.messaging.kafka_consumer import EmbeddingConsumer
        consumer = EmbeddingConsumer()
        consumer.start()
    else:
        logger.error(f"æœªçŸ¥æ¶ˆè´¹è€…ç±»å‹: {consumer_type}")


def run_kafka_pipeline():
    """è¿è¡ŒKafkaé›†æˆçš„å®Œæ•´Pipeline"""
    logger.info("ğŸ”„ å¯åŠ¨Kafka Pipeline...")
    from src.messaging.kafka_integrated_crawler import run_kafka_pipeline as kafka_pipeline
    kafka_pipeline()


def show_kafka_stats():
    """æ˜¾ç¤ºKafkaç»Ÿè®¡ä¿¡æ¯"""
    logger.info("ğŸ“Š Kafkaç»Ÿè®¡ä¿¡æ¯")
    try:
        from kafka import KafkaConsumer
        from src.messaging.kafka_producer import (
            KAFKA_BOOTSTRAP_SERVERS, 
            KAFKA_TOPIC_RAW, 
            KAFKA_TOPIC_PROCESSED,
            KAFKA_TOPIC_EMBEDDINGS
        )
        
        consumer = KafkaConsumer(bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS)
        topics = consumer.topics()
        
        logger.info(f"   å¯ç”¨Topics: {list(topics)}")
        
        # è·å–å„Topicçš„æ¶ˆæ¯æ•°é‡
        for topic in [KAFKA_TOPIC_RAW, KAFKA_TOPIC_PROCESSED, KAFKA_TOPIC_EMBEDDINGS]:
            if topic in topics:
                partitions = consumer.partitions_for_topic(topic)
                logger.info(f"   {topic}: {len(partitions) if partitions else 0} åˆ†åŒº")
        
        consumer.close()
    except Exception as e:
        logger.error(f"è·å–Kafkaç»Ÿè®¡å¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="åŒ»å­¦çŸ¥è¯†é—®ç­”RAGç³»ç»Ÿ")
    
    # Pipelineæ­¥éª¤
    parser.add_argument("--collect", action="store_true", help="æ•°æ®é‡‡é›†(æ”¯æŒæ–­ç‚¹ç»­ä¼ )")
    parser.add_argument("--max-per-topic", type=int, default=20000, help="æ¯ä¸»é¢˜æœ€å¤§æ–‡ç« æ•°")
    parser.add_argument("--workers", type=int, default=3, help="çˆ¬è™«å¹¶è¡Œçº¿ç¨‹æ•°")
    parser.add_argument("--clear-checkpoint", action="store_true", help="æ¸…é™¤checkpointé‡æ–°çˆ¬å–")
    parser.add_argument("--process", action="store_true", help="æ•°æ®å¤„ç†")
    parser.add_argument("--embed", action="store_true", help="å‘é‡åŒ–")
    parser.add_argument("--setup-db", action="store_true", help="æ„å»ºå‘é‡æ•°æ®åº“")
    parser.add_argument("--full", action="store_true", help="è¿è¡Œå®Œæ•´Pipeline")
    
    # æ•°æ®åº“é‡å»º
    parser.add_argument("--rebuild", action="store_true", help="é‡å»ºå‘é‡æ•°æ®åº“")
    parser.add_argument("--resume", action="store_true", help="æ–­ç‚¹ç»­ä¼ æ¨¡å¼(é…åˆ--rebuild)")
    parser.add_argument("--batch-size", type=int, default=128, help="æ‰¹æ¬¡å¤§å°(é»˜è®¤128)")
    
    # è¯„ä¼°
    parser.add_argument("--eval", action="store_true", help="å®Œæ•´ç³»ç»Ÿè¯„ä¼°")
    parser.add_argument("--eval-rag", action="store_true", help="ä»…RAGæ£€ç´¢è¯„ä¼°")
    parser.add_argument("--eval-storage", action="store_true", help="ä»…å­˜å‚¨æ€§èƒ½è¯„ä¼°")
    parser.add_argument("--eval-pyspark", action="store_true", help="PySparkå¤§æ•°æ®å¤„ç†è¯„ä¼°")
    
    # æ•°æ®æ‰©å±•
    parser.add_argument("--expand-data", action="store_true", help="æ‰©å±•æ•°æ®é›†ç”¨äºå¤§æ•°æ®æµ‹è¯•")
    parser.add_argument("--scale", type=int, default=10, help="æ•°æ®æ‰©å±•å€æ•°(é»˜è®¤10x)")
    
    # Sparkå¢å¼ºåŠŸèƒ½
    parser.add_argument("--spark-cluster", action="store_true", help="å¯åŠ¨Sparké›†ç¾¤(Docker)")
    parser.add_argument("--spark-embed", action="store_true", help="Sparkåˆ†å¸ƒå¼å‘é‡åŒ–")
    parser.add_argument("--use-cluster", action="store_true", help="ä½¿ç”¨Sparké›†ç¾¤æ¨¡å¼")
    parser.add_argument("--incremental", action="store_true", help="å¯åŠ¨å¢é‡ç´¢å¼•")
    parser.add_argument("--cache-prewarm", action="store_true", help="é¢„çƒ­Redisç¼“å­˜")
    
    # Kafka + Airflow åŠŸèƒ½
    parser.add_argument("--kafka-start", action="store_true", help="å¯åŠ¨Kafka+AirflowæœåŠ¡")
    parser.add_argument("--kafka-topics", action="store_true", help="åˆ›å»ºKafka Topics")
    parser.add_argument("--kafka-crawl", action="store_true", help="Kafkaé›†æˆçˆ¬è™«")
    parser.add_argument("--kafka-consumer", type=str, choices=["processor", "embedder"], 
                        help="å¯åŠ¨Kafkaæ¶ˆè´¹è€…")
    parser.add_argument("--kafka-pipeline", action="store_true", help="Kafkaå®Œæ•´Pipeline")
    parser.add_argument("--kafka-stats", action="store_true", help="æ˜¾ç¤ºKafkaç»Ÿè®¡")
    
    # Webç•Œé¢
    parser.add_argument("--web", action="store_true", help="å¯åŠ¨Webç•Œé¢")
    
    args = parser.parse_args()
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šä»»ä½•å‚æ•°ï¼Œæ˜¾ç¤ºå¸®åŠ©
    if not any(vars(args).values()):
        parser.print_help()
        print("\n" + "="*50)
        print("å¸¸ç”¨å‘½ä»¤:")
        print("  python main.py --full           # è¿è¡Œå®Œæ•´Pipeline")
        print("  python main.py --rebuild        # é‡å»ºå‘é‡æ•°æ®åº“")
        print("  python main.py --eval           # å®Œæ•´ç³»ç»Ÿè¯„ä¼°")
        print("  python main.py --web            # å¯åŠ¨Webç•Œé¢")
        print("\nSparkå¢å¼º:")
        print("  python main.py --spark-cluster  # å¯åŠ¨Sparké›†ç¾¤")
        print("  python main.py --spark-embed    # Sparkåˆ†å¸ƒå¼å‘é‡åŒ–")
        print("  python main.py --incremental    # å¢é‡ç´¢å¼•")
        print("  python main.py --cache-prewarm  # é¢„çƒ­ç¼“å­˜")
        print("\nKafka + Airflow:")
        print("  python main.py --kafka-start    # å¯åŠ¨Kafka+AirflowæœåŠ¡")
        print("  python main.py --kafka-topics   # åˆ›å»ºKafka Topics")
        print("  python main.py --kafka-crawl    # Kafkaé›†æˆçˆ¬è™«")
        print("  python main.py --kafka-consumer processor  # å¯åŠ¨å¤„ç†æ¶ˆè´¹è€…")
        print("  python main.py --kafka-pipeline # Kafkaå®Œæ•´Pipeline")
        return
    
    # æ‰§è¡Œå¯¹åº”æ­¥éª¤
    # Kafka + Airflow å‘½ä»¤
    if args.kafka_start:
        run_kafka_services()
    elif args.kafka_topics:
        run_kafka_topics_setup()
    elif args.kafka_crawl:
        run_kafka_crawler(use_kafka=True)
    elif args.kafka_consumer:
        run_kafka_consumer(args.kafka_consumer)
    elif args.kafka_pipeline:
        run_kafka_pipeline()
    elif args.kafka_stats:
        show_kafka_stats()
    # Sparkå‘½ä»¤
    elif args.spark_cluster:
        run_spark_cluster()
    elif args.spark_embed:
        run_spark_embed(use_cluster=args.use_cluster)
    elif args.incremental:
        run_incremental_index(use_spark=args.use_cluster)
    elif args.cache_prewarm:
        run_cache_prewarm()
    elif args.full:
        run_full_pipeline()
    elif args.rebuild:
        run_rebuild_database(resume=args.resume, batch_size=args.batch_size)
    elif args.eval or args.eval_rag or args.eval_storage or args.eval_pyspark:
        if args.eval_pyspark:
            run_evaluation("pyspark", scale_factor=args.scale)
        else:
            mode = "rag" if args.eval_rag else "storage" if args.eval_storage else "full"
            run_evaluation(mode)
    else:
        if args.collect:
            run_data_collection(
                max_per_topic=args.max_per_topic,
                workers=args.workers,
                clear=args.clear_checkpoint
            )
        if args.process:
            run_data_processing()
        if args.embed:
            run_embedding()
        if args.setup_db:
            run_vector_db_setup()
        if args.web:
            run_web_interface()


if __name__ == "__main__":
    main()
