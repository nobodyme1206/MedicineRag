# -*- coding: utf-8 -*-
"""
RAGç³»ç»Ÿæ ¸å¿ƒ - æ£€ç´¢å¢å¼ºç”Ÿæˆ
æ”¯æŒ: Reranké‡æ’åºã€Redisç¼“å­˜ã€MongoDBæ–‡æ¡£å­˜å‚¨ã€MinIOå¯¹è±¡å­˜å‚¨
"""

import time
import hashlib
import json
import asyncio
from concurrent.futures import ThreadPoolExecutor
from pathlib import Path
from typing import List, Dict, Tuple, Optional, Any
import sys
sys.path.append(str(Path(__file__).parent.parent.parent))

from config.config import *
from src.utils.logger import setup_logger
from src.embedding.embedder import TextEmbedder
from src.retrieval.milvus_manager import MilvusManager
from openai import OpenAI

logger = setup_logger("rag_system", LOGS_DIR / "rag.log")

# å…¨å±€å­˜å‚¨å®ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
_mongodb_instance = None
_minio_instance = None


def get_mongodb() -> Optional[Any]:
    """è·å–MongoDBå•ä¾‹å®ä¾‹"""
    global _mongodb_instance
    if _mongodb_instance is None:
        try:
            from src.storage.mongodb_storage import MongoDBStorage
            _mongodb_instance = MongoDBStorage(
                host=MONGODB_HOST,
                port=MONGODB_PORT,
                database=MONGODB_DATABASE
            )
        except Exception as e:
            logger.warning(f"MongoDBåˆå§‹åŒ–å¤±è´¥: {e}")
    return _mongodb_instance


def get_minio() -> Optional[Any]:
    """è·å–MinIOå•ä¾‹å®ä¾‹"""
    global _minio_instance
    if _minio_instance is None:
        try:
            from src.storage.minio_storage import MinIOStorage
            _minio_instance = MinIOStorage(
                endpoint=MINIO_ENDPOINT,
                access_key=MINIO_ACCESS_KEY,
                secret_key=MINIO_SECRET_KEY,
                secure=MINIO_SECURE
            )
            # ç¡®ä¿å¿…è¦çš„bucketå­˜åœ¨
            _minio_instance.create_bucket(MINIO_BUCKET_MODELS)
            _minio_instance.create_bucket(MINIO_BUCKET_BACKUPS)
            _minio_instance.create_bucket(MINIO_BUCKET_DATA)
        except Exception as e:
            logger.warning(f"MinIOåˆå§‹åŒ–å¤±è´¥: {e}")
    return _minio_instance


class RAGSystem:
    """RAGé—®ç­”ç³»ç»Ÿï¼ˆæ”¯æŒRerankã€æ··åˆæ£€ç´¢ã€HyDEã€Redisç¼“å­˜ã€MongoDBæ—¥å¿—ã€MinIOå­˜å‚¨ï¼‰"""
    
    def __init__(self, use_rerank: bool = None, use_hybrid: bool = None, 
                 use_query_rewrite: bool = False, use_hyde: bool = None,
                 use_ensemble: bool = False, use_cache: bool = True):
        """
        åˆå§‹åŒ–RAGç³»ç»Ÿ
        
        Args:
            use_rerank: æ˜¯å¦ä½¿ç”¨Rerankï¼ŒNoneåˆ™ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é»˜è®¤å€¼
            use_hybrid: æ˜¯å¦ä½¿ç”¨æ··åˆæ£€ç´¢(BM25+å‘é‡)
            use_query_rewrite: æ˜¯å¦ä½¿ç”¨æŸ¥è¯¢æ”¹å†™
            use_hyde: æ˜¯å¦ä½¿ç”¨HyDEå‡è®¾æ–‡æ¡£åµŒå…¥ï¼ŒNoneåˆ™ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é»˜è®¤å€¼
            use_ensemble: æ˜¯å¦ä½¿ç”¨é›†æˆæ£€ç´¢ï¼ˆRRFå¤šè·¯èåˆï¼‰
            use_cache: æ˜¯å¦ä½¿ç”¨Redisç¼“å­˜
        """
        logger.info("åˆå§‹åŒ–RAGç³»ç»Ÿ...")
        
        # é…ç½®åŠŸèƒ½å¼€å…³
        self.use_rerank = use_rerank if use_rerank is not None else USE_RERANK
        # BM25æ•ˆæœå·®(MRR=0.57)ä¸”æ…¢(1825ms)ï¼Œé»˜è®¤ç¦ç”¨æ··åˆæ£€ç´¢
        from config.config import USE_HYBRID_SEARCH
        self.use_hybrid = use_hybrid if use_hybrid is not None else USE_HYBRID_SEARCH
        self.use_query_rewrite = use_query_rewrite
        self.use_hyde = use_hyde if use_hyde is not None else getattr(__import__('config.config', fromlist=['USE_HYDE']), 'USE_HYDE', False)
        self.use_ensemble = use_ensemble
        self.use_cache = use_cache
        self.reranker = None
        self.hybrid_searcher = None
        self.query_rewriter = None
        self.hyde = None
        self.ensemble_retriever = None
        self.redis_cache = None
        
        # 0. åˆå§‹åŒ–å­˜å‚¨æœåŠ¡ï¼ˆMongoDB + MinIO + Redisï¼‰
        logger.info("åˆå§‹åŒ–å­˜å‚¨æœåŠ¡...")
        self._init_storage_services()
        
        # 1. åŠ è½½Embeddingæ¨¡å‹
        logger.info("åŠ è½½Embeddingæ¨¡å‹...")
        self.embedder = TextEmbedder()
        
        # 2. è¿æ¥å‘é‡æ•°æ®åº“
        logger.info("è¿æ¥å‘é‡æ•°æ®åº“...")
        self.milvus = MilvusManager()
        self.milvus.load_collection()
        
        # 3. åˆå§‹åŒ–LLMå®¢æˆ·ç«¯ï¼ˆç¡…åŸºæµåŠ¨ï¼‰
        logger.info("åˆå§‹åŒ–LLMå®¢æˆ·ç«¯ï¼ˆç¡…åŸºæµåŠ¨ï¼‰...")
        self.llm_client = OpenAI(
            api_key=SILICONFLOW_API_KEY,
            base_url=SILICONFLOW_BASE_URL
        )
        
        # 4. åˆå§‹åŒ–Rerankerï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.use_rerank:
            logger.info("åŠ è½½Rerankæ¨¡å‹...")
            try:
                from src.retrieval.reranker import Reranker
                self.reranker = Reranker()
                logger.info("âœ… Rerankæ¨¡å‹åŠ è½½æˆåŠŸ")
            except Exception as e:
                logger.warning(f"Rerankæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°†ä¸ä½¿ç”¨Rerank: {e}")
                self.use_rerank = False
        
        # 5. åˆå§‹åŒ–æ··åˆæ£€ç´¢ï¼ˆé»˜è®¤ç¦ç”¨ï¼ŒBM25æ•ˆæœå·®ï¼‰
        if self.use_hybrid:
            logger.info("åˆå§‹åŒ–æ··åˆæ£€ç´¢å™¨ï¼ˆBM25+å‘é‡ï¼‰...")
            try:
                from src.retrieval.hybrid_searcher import HybridSearcher
                self.hybrid_searcher = HybridSearcher()
                logger.info("âœ… æ··åˆæ£€ç´¢å™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.warning(f"æ··åˆæ£€ç´¢å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                self.use_hybrid = False
        else:
            logger.info("è·³è¿‡æ··åˆæ£€ç´¢ï¼ˆBM25å·²ç¦ç”¨ï¼Œä½¿ç”¨çº¯å‘é‡+Rerankï¼‰")
        
        # 6. åˆå§‹åŒ–æŸ¥è¯¢æ”¹å†™å™¨ï¼ˆé»˜è®¤å¯ç”¨æœ¬åœ°å¢å¼ºï¼Œä¸è°ƒç”¨LLMï¼‰
        logger.info("åˆå§‹åŒ–æŸ¥è¯¢æ”¹å†™å™¨...")
        try:
            from src.rag.query_rewriter import QueryRewriter
            self.query_rewriter = QueryRewriter(use_llm=False)  # æœ¬åœ°åŒä¹‰è¯æ‰©å±•ï¼Œé€Ÿåº¦å¿«
            logger.info("âœ… æŸ¥è¯¢æ”¹å†™å™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"æŸ¥è¯¢æ”¹å†™å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.query_rewriter = None
        
        # 7. åˆå§‹åŒ–HyDEï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.use_hyde:
            logger.info("åˆå§‹åŒ–HyDEæ¨¡å—...")
            try:
                from src.retrieval.hyde import HyDE
                self.hyde = HyDE()
                logger.info("âœ… HyDEæ¨¡å—åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.warning(f"HyDEæ¨¡å—åˆå§‹åŒ–å¤±è´¥: {e}")
                self.use_hyde = False
        
        # 8. åˆå§‹åŒ–é›†æˆæ£€ç´¢ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.use_ensemble:
            logger.info("åˆå§‹åŒ–é›†æˆæ£€ç´¢å™¨ï¼ˆRRFèåˆï¼‰...")
            try:
                from src.retrieval.rrf_fusion import EnsembleRetriever
                self.ensemble_retriever = EnsembleRetriever(
                    embedder=self.embedder,
                    milvus_manager=self.milvus,
                    hybrid_searcher=self.hybrid_searcher if self.use_hybrid else None
                )
                logger.info("âœ… é›†æˆæ£€ç´¢å™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.warning(f"é›†æˆæ£€ç´¢å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                self.use_ensemble = False
        
        # 9. åˆå§‹åŒ–è¯­ä¹‰ç¼“å­˜
        self.semantic_cache = None
        try:
            from config.config import SEMANTIC_CACHE_ENABLED, SEMANTIC_CACHE_THRESHOLD, SEMANTIC_CACHE_TTL
            if SEMANTIC_CACHE_ENABLED and self.redis_cache:
                from src.caching.redis_cache import SemanticCache
                self.semantic_cache = SemanticCache(
                    self.redis_cache,
                    embedder=self.embedder,
                    similarity_threshold=SEMANTIC_CACHE_THRESHOLD,
                    ttl=SEMANTIC_CACHE_TTL
                )
                logger.info("âœ… è¯­ä¹‰ç¼“å­˜åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.warning(f"è¯­ä¹‰ç¼“å­˜åˆå§‹åŒ–å¤±è´¥: {e}")
        
        # 10. åˆå§‹åŒ–çº¿ç¨‹æ± ï¼ˆç”¨äºå¼‚æ­¥å¹¶å‘ï¼‰
        self._executor = ThreadPoolExecutor(max_workers=4)
        
        logger.info("âœ… RAGç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def _init_storage_services(self):
        """åˆå§‹åŒ–æ‰€æœ‰å­˜å‚¨æœåŠ¡ï¼šRedisç¼“å­˜ã€MongoDBæ–‡æ¡£å­˜å‚¨ã€MinIOå¯¹è±¡å­˜å‚¨"""
        # Redisç¼“å­˜ + å‘é‡ç¼“å­˜ç®¡ç†å™¨
        if self.use_cache:
            logger.info("  â†’ åˆå§‹åŒ–Redisç¼“å­˜...")
            try:
                from src.caching.redis_cache import RedisCache, VectorCacheManager
                self.redis_cache = RedisCache()
                self.vector_cache = VectorCacheManager(self.redis_cache)
                logger.info("  âœ… Redisç¼“å­˜ + å‘é‡ç¼“å­˜åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.warning(f"  âš ï¸ Redisç¼“å­˜åˆå§‹åŒ–å¤±è´¥: {e}")
                self.use_cache = False
                self.vector_cache = None
        else:
            self.vector_cache = None
        
        # MongoDBæ–‡æ¡£å­˜å‚¨
        logger.info("  â†’ åˆå§‹åŒ–MongoDBæ–‡æ¡£å­˜å‚¨...")
        self.mongodb = get_mongodb()
        if self.mongodb:
            logger.info("  âœ… MongoDBæ–‡æ¡£å­˜å‚¨åˆå§‹åŒ–æˆåŠŸ")
        else:
            logger.warning("  âš ï¸ MongoDBä¸å¯ç”¨ï¼ŒæŸ¥è¯¢æ—¥å¿—å°†ä¸ä¼šä¿å­˜")
        
        # MinIOå¯¹è±¡å­˜å‚¨
        logger.info("  â†’ åˆå§‹åŒ–MinIOå¯¹è±¡å­˜å‚¨...")
        self.minio = get_minio()
        if self.minio:
            logger.info("  âœ… MinIOå¯¹è±¡å­˜å‚¨åˆå§‹åŒ–æˆåŠŸ")
        else:
            logger.warning("  âš ï¸ MinIOä¸å¯ç”¨ï¼Œå¤‡ä»½åŠŸèƒ½å°†ä¸å¯ç”¨")
    
    def _get_cache_key(self, query: str, top_k: int) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        config_str = f"{self.use_rerank}_{self.use_hybrid}_{self.use_hyde}_{top_k}"
        return hashlib.md5(f"{query}_{config_str}".encode()).hexdigest()
    
    def retrieve(self, query: str, top_k: int = RETRIEVAL_TOP_K) -> List[Dict]:
        """
        æ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼ˆæ”¯æŒæŸ¥è¯¢æ”¹å†™ã€æ··åˆæ£€ç´¢ã€HyDEã€é›†æˆæ£€ç´¢ã€Reranké‡æ’åºã€Redisç¼“å­˜ï¼‰
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            top_k: è¿”å›Top-Kç»“æœ
            
        Returns:
            æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        original_query = query
        
        # 0. æ£€æŸ¥Redisç¼“å­˜
        cache_key = self._get_cache_key(query, top_k)
        if self.use_cache and self.redis_cache:
            cached_result = self.redis_cache.get_query_cache(cache_key)
            if cached_result:
                logger.info(f"âœ… Redisç¼“å­˜å‘½ä¸­: {query[:30]}...")
                return cached_result
        
        # 1. å¦‚æœå¯ç”¨é›†æˆæ£€ç´¢ï¼Œä½¿ç”¨EnsembleRetriever
        if self.use_ensemble and self.ensemble_retriever:
            logger.info("ä½¿ç”¨é›†æˆæ£€ç´¢ï¼ˆRRFå¤šè·¯èåˆï¼‰...")
            candidates = self.ensemble_retriever.retrieve_ensemble(
                original_query,
                top_k=top_k * 3 if self.use_rerank else top_k,
                use_hyde=self.use_hyde,
                use_hybrid=self.use_hybrid
            )
            # å¦‚æœå¯ç”¨Rerankï¼Œå¯¹é›†æˆç»“æœé‡æ’åº
            if self.use_rerank and self.reranker:
                logger.info(f"ä½¿ç”¨Rerankå¯¹ {len(candidates)} ä¸ªå€™é€‰è¿›è¡Œé‡æ’åº...")
                candidates = self.reranker.rerank(original_query, candidates, top_k=top_k)
                logger.info(f"Rerankåä¿ç•™Top-{len(candidates)}ç»“æœ")
            
            final_results = candidates[:top_k]
            # ä¿å­˜åˆ°Redisç¼“å­˜
            if self.use_cache and self.redis_cache:
                self.redis_cache.set_query_cache(cache_key, final_results, ttl=3600)
            return final_results
        
        # 1. æŸ¥è¯¢æ ‡å‡†åŒ–ï¼ˆåŒ»å­¦æœ¯è¯­æ ‡å‡†åŒ–ï¼Œå±•å¼€ç¼©å†™ï¼‰
        if self.query_rewriter:
            normalized_query = self.query_rewriter.normalize_query(query)
            if normalized_query != query:
                logger.info(f"æŸ¥è¯¢æ ‡å‡†åŒ–: {query} -> {normalized_query}")
                query = normalized_query
        
        # 2. HyDEå‡è®¾æ–‡æ¡£åµŒå…¥ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.use_hyde and self.hyde:
            logger.info("ä½¿ç”¨HyDEç”Ÿæˆå‡è®¾æ–‡æ¡£...")
            query = self.hyde.get_hyde_query(original_query)
            logger.info(f"HyDEå‡è®¾æ–‡æ¡£: {query[:100]}...")
        
        # 3. Queryå‘é‡åŒ–ï¼ˆä¼˜å…ˆä½¿ç”¨å‘é‡ç¼“å­˜ï¼‰
        if self.vector_cache:
            query_embedding = self.vector_cache.get_or_compute_vector(query)
        else:
            query_embedding = self.embedder.encode_single(query)
        query_embedding = query_embedding.reshape(1, -1)
        
        # 4. å‘é‡æ£€ç´¢
        search_top_k = top_k * 3 if (self.use_rerank or self.use_hybrid) else top_k
        results = self.milvus.search(query_embedding, top_k=search_top_k)
        
        if not results or not results[0]:
            return []
        
        candidates = results[0]
        
        # 5. æ··åˆæ£€ç´¢ï¼ˆå¦‚æœå¯ç”¨ï¼‰- ä½¿ç”¨RRFèåˆ
        if self.use_hybrid and self.hybrid_searcher:
            # ä½¿ç”¨å¢å¼ºæŸ¥è¯¢è¿›è¡ŒBM25æ£€ç´¢
            bm25_query = original_query
            if self.query_rewriter:
                bm25_query = self.query_rewriter.get_enhanced_query(original_query)
            
            logger.info(f"ä½¿ç”¨æ··åˆæ£€ç´¢èåˆBM25å’Œå‘é‡ç»“æœï¼ˆRRFï¼‰...")
            candidates = self.hybrid_searcher.hybrid_search(
                bm25_query,  # ä½¿ç”¨å¢å¼ºæŸ¥è¯¢åšBM25
                candidates,
                alpha=0.6,  # å‘é‡æ£€ç´¢æƒé‡60%ï¼ŒBM25æƒé‡40%
                top_k=search_top_k,
                use_rrf=True  # ä½¿ç”¨RRFèåˆï¼Œæ›´ç¨³å®š
            )
        
        # 6. Reranké‡æ’åºï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.use_rerank and self.reranker:
            logger.info(f"ä½¿ç”¨Rerankå¯¹ {len(candidates)} ä¸ªå€™é€‰è¿›è¡Œé‡æ’åº...")
            candidates = self.reranker.rerank(query, candidates, top_k=top_k)
            logger.info(f"Rerankåä¿ç•™Top-{len(candidates)}ç»“æœ")
        
        final_results = candidates[:top_k]
        
        # 7. ä¿å­˜åˆ°Redisç¼“å­˜
        if self.use_cache and self.redis_cache:
            self.redis_cache.set_query_cache(cache_key, final_results, ttl=3600)
            logger.info(f"å·²ç¼“å­˜æŸ¥è¯¢ç»“æœ: {query[:30]}...")
        
        return final_results
    
    def generate(self, query: str, contexts: List[Dict]) -> Tuple[str, Dict]:
        """
        åŸºäºæ£€ç´¢ç»“æœç”Ÿæˆç­”æ¡ˆ
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            contexts: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
            
        Returns:
            (ç­”æ¡ˆ, å…ƒä¿¡æ¯)
        """
        if not contexts:
            return "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚", {}
        
        # æ„å»ºprompt
        context_text = "\n\n".join([
            f"[æ–‡æ¡£{i+1}] {ctx['text']}"
            for i, ctx in enumerate(contexts[:RERANK_TOP_K])
        ])
        
        system_prompt = """You are an expert medical knowledge assistant with deep expertise in clinical medicine, diagnostics, and treatment protocols.

Your task: Provide accurate, comprehensive, evidence-based answers using ONLY the provided scientific literature.

Core Principles:
1. **Evidence-Based Medicine**: Base ALL statements on provided documents. Never speculate or add external knowledge
2. **Clinical Precision**: Use accurate medical terminology, cite specific mechanisms, dosages, and clinical findings
3. **Comprehensive Coverage**: Address ALL relevant aspects:
   - Definition & Pathophysiology
   - Risk factors & Epidemiology
   - Clinical manifestations & Symptoms
   - Diagnostic criteria & Tests
   - Treatment options & Management
   - Prognosis & Outcomes
4. **Structured Response**: Organize information clearly with distinct sections
5. **Source Attribution**: Reference specific documents [Document N] for key claims
6. **Honesty**: If literature is insufficient, explicitly state "The provided literature does not contain information about..."
7. **Language Matching**: Respond in the SAME language as the question

Response Format:
- Start with a direct, concise answer to the core question
- Follow with detailed explanation organized by topic
- Include relevant clinical details (values, percentages, mechanisms)
- End with practical implications or key takeaways"""
        
        user_prompt = f"""### REFERENCE LITERATURE:
{context_text}

### MEDICAL QUESTION:
{query}

### INSTRUCTIONS:
Analyze the above literature and provide a comprehensive, evidence-based answer. Include:
- Direct answer to the question
- Supporting evidence from the documents (with citations)
- Relevant clinical details and mechanisms
- Key points summary

Answer:"""
        
        # è°ƒç”¨LLM
        try:
            start_time = time.time()
            
            response = self.llm_client.chat.completions.create(
                model=SILICONFLOW_MODEL,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=LLM_TEMPERATURE,
                max_tokens=LLM_MAX_TOKENS,
                top_p=LLM_TOP_P
            )
            
            generation_time = time.time() - start_time
            
            answer = response.choices[0].message.content
            
            # å…ƒä¿¡æ¯
            metadata = {
                "model": SILICONFLOW_MODEL,
                "generation_time": generation_time,
                "tokens": response.usage.total_tokens if hasattr(response, 'usage') else 0,
                "num_contexts": len(contexts)
            }
            
            return answer, metadata
            
        except Exception as e:
            logger.error(f"LLMç”Ÿæˆå¤±è´¥: {e}")
            return f"ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™: {str(e)}", {}
    
    def answer(self, query: str, return_contexts: bool = True) -> Dict:
        """
        å®Œæ•´çš„é—®ç­”æµç¨‹
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            return_contexts: æ˜¯å¦è¿”å›æ£€ç´¢çš„ä¸Šä¸‹æ–‡
            
        Returns:
            é—®ç­”ç»“æœå­—å…¸
        """
        logger.info(f"ç”¨æˆ·æé—®: {query}")
        
        start_time = time.time()
        
        # 1. æ£€ç´¢
        retrieval_start = time.time()
        contexts = self.retrieve(query, top_k=RETRIEVAL_TOP_K)
        retrieval_time = time.time() - retrieval_start
        
        logger.info(f"æ£€ç´¢åˆ° {len(contexts)} ä¸ªç›¸å…³æ–‡æ¡£ï¼Œè€—æ—¶ {retrieval_time:.3f}ç§’")
        
        # 2. ç”Ÿæˆ
        answer, gen_metadata = self.generate(query, contexts)
        
        total_time = time.time() - start_time
        
        # æ„å»ºç»“æœ
        result = {
            "query": query,
            "answer": answer,
            "retrieval_time": retrieval_time,
            "generation_time": gen_metadata.get("generation_time", 0),
            "total_time": total_time,
            "num_contexts": len(contexts)
        }
        
        if return_contexts:
            result["contexts"] = [
                {
                    "pmid": ctx["pmid"],
                    "text": ctx["text"],
                    "score": ctx["score"]
                }
                for ctx in contexts[:RERANK_TOP_K]
            ]
        
        # 3. è®°å½•æŸ¥è¯¢æ—¥å¿—åˆ°MongoDB
        self._log_query_to_mongodb(query, contexts, result)
        
        logger.info(f"å›ç­”ç”Ÿæˆå®Œæˆï¼Œæ€»è€—æ—¶ {total_time:.3f}ç§’")
        
        return result
    
    def _log_query_to_mongodb(self, query: str, contexts: List[Dict], result: Dict):
        """è®°å½•æŸ¥è¯¢æ—¥å¿—åˆ°MongoDB"""
        if not self.mongodb:
            return
        try:
            metrics = {
                "retrieval_time_ms": result["retrieval_time"] * 1000,
                "generation_time_ms": result["generation_time"] * 1000,
                "total_time_ms": result["total_time"] * 1000,
                "num_contexts": len(contexts),
                "use_hybrid": self.use_hybrid,
                "use_rerank": self.use_rerank,
                "cache_hit": False  # å¦‚æœèµ°åˆ°è¿™é‡Œè¯´æ˜æ²¡æœ‰å‘½ä¸­ç¼“å­˜
            }
            self.mongodb.log_query(query, contexts, metrics)
        except Exception as e:
            logger.warning(f"MongoDBæ—¥å¿—è®°å½•å¤±è´¥: {e}")
    
    def batch_answer(self, queries: List[str]) -> List[Dict]:
        """æ‰¹é‡é—®ç­”"""
        results = []
        for query in queries:
            result = self.answer(query)
            results.append(result)
        return results
    
    def save_evaluation_to_mongodb(self, eval_results: Dict) -> bool:
        """ä¿å­˜è¯„ä¼°ç»“æœåˆ°MongoDB"""
        if not self.mongodb:
            logger.warning("MongoDBä¸å¯ç”¨ï¼Œæ— æ³•ä¿å­˜è¯„ä¼°ç»“æœ")
            return False
        try:
            self.mongodb.save_evaluation_results(eval_results)
            logger.info("âœ… è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°MongoDB")
            return True
        except Exception as e:
            logger.error(f"ä¿å­˜è¯„ä¼°ç»“æœå¤±è´¥: {e}")
            return False
    
    def get_query_statistics(self) -> Dict:
        """ä»MongoDBè·å–æŸ¥è¯¢ç»Ÿè®¡"""
        if not self.mongodb:
            return {}
        try:
            return self.mongodb.get_query_statistics()
        except Exception as e:
            logger.error(f"è·å–æŸ¥è¯¢ç»Ÿè®¡å¤±è´¥: {e}")
            return {}
    
    def backup_to_minio(self, file_path: Path, object_name: str = None, 
                        bucket: str = None) -> bool:
        """å¤‡ä»½æ–‡ä»¶åˆ°MinIO"""
        if not self.minio:
            logger.warning("MinIOä¸å¯ç”¨ï¼Œæ— æ³•å¤‡ä»½")
            return False
        try:
            bucket = bucket or MINIO_BUCKET_BACKUPS
            object_name = object_name or file_path.name
            return self.minio.upload_file(bucket, object_name, file_path)
        except Exception as e:
            logger.error(f"å¤‡ä»½åˆ°MinIOå¤±è´¥: {e}")
            return False
    
    def list_minio_backups(self, bucket: str = None) -> List[Dict]:
        """åˆ—å‡ºMinIOä¸­çš„å¤‡ä»½æ–‡ä»¶"""
        if not self.minio:
            return []
        try:
            bucket = bucket or MINIO_BUCKET_BACKUPS
            return self.minio.list_objects(bucket)
        except Exception as e:
            logger.error(f"åˆ—å‡ºMinIOå¤‡ä»½å¤±è´¥: {e}")
            return []
    
    def get_storage_status(self) -> Dict:
        """è·å–æ‰€æœ‰å­˜å‚¨æœåŠ¡çŠ¶æ€"""
        status = {
            "redis": {"enabled": self.use_cache, "connected": self.redis_cache is not None},
            "mongodb": {"enabled": True, "connected": self.mongodb is not None},
            "minio": {"enabled": True, "connected": self.minio is not None},
            "milvus": {"enabled": True, "connected": True}
        }
        
        # è·å–è¯¦ç»†ç»Ÿè®¡
        if self.mongodb:
            try:
                status["mongodb"]["query_logs"] = self.mongodb.get_collection_stats("query_logs")
                status["mongodb"]["evaluations"] = self.mongodb.get_collection_stats("evaluation_results")
            except:
                pass
        
        if self.minio:
            try:
                status["minio"]["backups"] = len(self.minio.list_objects(MINIO_BUCKET_BACKUPS))
            except:
                pass
        
        return status
    
    def generate_without_context(self, query: str) -> Tuple[str, Dict]:
        """
        ä¸ä½¿ç”¨æ£€ç´¢ç»“æœï¼Œç›´æ¥ç”¨LLMå›ç­”ï¼ˆç”¨äºåŸºçº¿å¯¹æ¯”ï¼‰
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            
        Returns:
            (ç­”æ¡ˆ, å…ƒä¿¡æ¯)
        """
        system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»å­¦çŸ¥è¯†é—®ç­”åŠ©æ‰‹ã€‚è¯·åŸºäºä½ çš„åŒ»å­¦çŸ¥è¯†å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

è¦æ±‚ï¼š
1. å›ç­”è¦ä¸“ä¸šã€å‡†ç¡®ã€ç®€æ´
2. å¦‚æœä¸ç¡®å®šï¼Œè¯·æ˜ç¡®å‘ŠçŸ¥
3. ä½¿ç”¨ä¸­æ–‡å›ç­”"""
        
        user_prompt = f"ç”¨æˆ·é—®é¢˜ï¼š{query}\n\nè¯·å›ç­”è¿™ä¸ªåŒ»å­¦é—®é¢˜ï¼š"
        
        try:
            start_time = time.time()
            
            response = self.llm_client.chat.completions.create(
                model=SILICONFLOW_MODEL,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                temperature=LLM_TEMPERATURE,
                max_tokens=LLM_MAX_TOKENS,
                top_p=LLM_TOP_P
            )
            
            generation_time = time.time() - start_time
            
            answer = response.choices[0].message.content
            
            metadata = {
                "model": SILICONFLOW_MODEL,
                "generation_time": generation_time,
                "tokens": response.usage.total_tokens if hasattr(response, 'usage') else 0,
                "method": "baseline_no_context"
            }
            
            return answer, metadata
            
        except Exception as e:
            logger.error(f"åŸºçº¿LLMç”Ÿæˆå¤±è´¥: {e}")
            return f"ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™: {str(e)}", {}
    
    def answer_baseline(self, query: str) -> Dict:
        """
        åŸºçº¿é—®ç­”ï¼ˆä¸ä½¿ç”¨RAGæ£€ç´¢ï¼‰
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            
        Returns:
            é—®ç­”ç»“æœå­—å…¸
        """
        logger.info(f"åŸºçº¿é—®ç­”ï¼ˆæ— RAGï¼‰: {query}")
        
        start_time = time.time()
        
        # ç›´æ¥ç”Ÿæˆ
        answer, gen_metadata = self.generate_without_context(query)
        
        total_time = time.time() - start_time
        
        result = {
            "query": query,
            "answer": answer,
            "retrieval_time": 0,
            "generation_time": gen_metadata.get("generation_time", 0),
            "total_time": total_time,
            "method": "baseline_no_rag"
        }
        
        return result
    
    def vector_search(self, query: str, top_k: int = RETRIEVAL_TOP_K) -> List[Dict]:
        """
        çº¯å‘é‡æ£€ç´¢ï¼ˆç”¨äºè¯„ä¼°åŸºçº¿ï¼‰
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            top_k: è¿”å›Top-Kç»“æœ
            
        Returns:
            æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        # Queryå‘é‡åŒ–
        if self.vector_cache:
            query_embedding = self.vector_cache.get_or_compute_vector(query)
        else:
            query_embedding = self.embedder.encode_single(query)
        query_embedding = query_embedding.reshape(1, -1)
        
        # å‘é‡æ£€ç´¢
        results = self.milvus.search(query_embedding, top_k=top_k)
        
        if not results or not results[0]:
            return []
        
        return results[0]
    
    # ==================== å¼‚æ­¥å¹¶å‘æ£€ç´¢ ====================
    
    async def async_retrieve(self, query: str, top_k: int = RETRIEVAL_TOP_K) -> List[Dict]:
        """
        å¼‚æ­¥æ£€ç´¢ï¼ˆéé˜»å¡ï¼‰
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            top_k: è¿”å›Top-Kç»“æœ
            
        Returns:
            æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(self._executor, self.retrieve, query, top_k)
    
    async def async_answer(self, query: str, return_contexts: bool = True) -> Dict:
        """
        å¼‚æ­¥é—®ç­”ï¼ˆéé˜»å¡ï¼‰
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            return_contexts: æ˜¯å¦è¿”å›æ£€ç´¢çš„ä¸Šä¸‹æ–‡
            
        Returns:
            é—®ç­”ç»“æœå­—å…¸
        """
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            self._executor, 
            lambda: self.answer(query, return_contexts)
        )
    
    async def async_batch_answer(self, queries: List[str], max_concurrency: int = 4) -> List[Dict]:
        """
        å¼‚æ­¥æ‰¹é‡é—®ç­”ï¼ˆå¹¶å‘å¤„ç†ï¼‰
        
        Args:
            queries: é—®é¢˜åˆ—è¡¨
            max_concurrency: æœ€å¤§å¹¶å‘æ•°
            
        Returns:
            é—®ç­”ç»“æœåˆ—è¡¨
        """
        semaphore = asyncio.Semaphore(max_concurrency)
        
        async def process_with_semaphore(query: str) -> Dict:
            async with semaphore:
                return await self.async_answer(query)
        
        tasks = [process_with_semaphore(q) for q in queries]
        return await asyncio.gather(*tasks)
    
    def answer_with_semantic_cache(self, query: str, return_contexts: bool = True) -> Dict:
        """
        å¸¦è¯­ä¹‰ç¼“å­˜çš„é—®ç­”ï¼ˆç›¸ä¼¼é—®é¢˜å¤ç”¨ç­”æ¡ˆï¼‰
        
        Args:
            query: ç”¨æˆ·é—®é¢˜
            return_contexts: æ˜¯å¦è¿”å›æ£€ç´¢çš„ä¸Šä¸‹æ–‡
            
        Returns:
            é—®ç­”ç»“æœå­—å…¸
        """
        # æ£€æŸ¥è¯­ä¹‰ç¼“å­˜
        if self.semantic_cache:
            cached = self.semantic_cache.get(query)
            if cached:
                logger.info(f"è¯­ä¹‰ç¼“å­˜å‘½ä¸­: similarity={cached.get('similarity', 0):.3f}")
                return {
                    "query": query,
                    "answer": cached["answer"],
                    "contexts": cached.get("contexts", []) if return_contexts else [],
                    "retrieval_time": 0,
                    "generation_time": 0,
                    "total_time": 0,
                    "num_contexts": len(cached.get("contexts", [])),
                    "cache_hit": True,
                    "similarity": cached.get("similarity", 1.0)
                }
        
        # æ­£å¸¸é—®ç­”
        result = self.answer(query, return_contexts)
        
        # å­˜å…¥è¯­ä¹‰ç¼“å­˜
        if self.semantic_cache:
            self.semantic_cache.set(
                query,
                result["answer"],
                result.get("contexts", []),
                {"retrieval_time": result.get("retrieval_time", 0)}
            )
        
        result["cache_hit"] = False
        return result
    
    def prewarm_hot_queries(self, queries: List[str]) -> Dict[str, int]:
        """
        é¢„çƒ­çƒ­é—¨æŸ¥è¯¢ï¼ˆé¢„è®¡ç®—embeddingå¹¶ç¼“å­˜ç­”æ¡ˆï¼‰
        
        Args:
            queries: çƒ­é—¨æŸ¥è¯¢åˆ—è¡¨
            
        Returns:
            é¢„çƒ­ç»Ÿè®¡
        """
        logger.info(f"ğŸ”¥ é¢„çƒ­çƒ­é—¨æŸ¥è¯¢: {len(queries)} æ¡")
        
        stats = {"embeddings": 0, "answers": 0, "errors": 0}
        
        for i, query in enumerate(queries):
            try:
                # é¢„çƒ­embedding
                if self.vector_cache:
                    self.vector_cache.get_or_compute_vector(query)
                    stats["embeddings"] += 1
                
                # é¢„çƒ­ç­”æ¡ˆï¼ˆå­˜å…¥è¯­ä¹‰ç¼“å­˜ï¼‰
                if self.semantic_cache:
                    result = self.answer(query, return_contexts=True)
                    self.semantic_cache.set(
                        query,
                        result["answer"],
                        result.get("contexts", [])
                    )
                    stats["answers"] += 1
                
                if (i + 1) % 10 == 0:
                    logger.info(f"   é¢„çƒ­è¿›åº¦: {i+1}/{len(queries)}")
                    
            except Exception as e:
                logger.warning(f"é¢„çƒ­å¤±è´¥ [{query[:30]}...]: {e}")
                stats["errors"] += 1
        
        logger.info(f"âœ… é¢„çƒ­å®Œæˆ: embeddings={stats['embeddings']}, answers={stats['answers']}")
        return stats
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–æ‰€æœ‰ç¼“å­˜ç»Ÿè®¡"""
        stats = {}
        
        if self.redis_cache:
            stats["redis"] = self.redis_cache.get_stats()
        
        if self.vector_cache:
            stats["vector_cache"] = self.vector_cache.get_stats()
        
        if self.semantic_cache:
            stats["semantic_cache"] = self.semantic_cache.get_stats()
        
        return stats


def main():
    """æµ‹è¯•RAGç³»ç»Ÿ"""
    logger.info("="*50)
    logger.info("æµ‹è¯•RAGç³»ç»Ÿ")
    logger.info("="*50)
    
    # åˆå§‹åŒ–ç³»ç»Ÿ
    rag = RAGSystem()
    
    # æµ‹è¯•é—®é¢˜
    test_queries = [
        "ä»€ä¹ˆæ˜¯ç³–å°¿ç—…ï¼Ÿ",
        "å¦‚ä½•é¢„é˜²å¿ƒè¡€ç®¡ç–¾ç—…ï¼Ÿ",
        "ç™Œç—‡çš„å¸¸è§æ²»ç–—æ–¹æ³•æœ‰å“ªäº›ï¼Ÿ"
    ]
    
    for query in test_queries:
        logger.info(f"\n{'='*50}")
        logger.info(f"é—®é¢˜: {query}")
        logger.info(f"{'='*50}")
        
        result = rag.answer(query)
        
        logger.info(f"\nç­”æ¡ˆ:\n{result['answer']}")
        logger.info(f"\næ€§èƒ½æŒ‡æ ‡:")
        logger.info(f"  æ£€ç´¢æ—¶é—´: {result['retrieval_time']:.3f}ç§’")
        logger.info(f"  ç”Ÿæˆæ—¶é—´: {result['generation_time']:.3f}ç§’")
        logger.info(f"  æ€»æ—¶é—´: {result['total_time']:.3f}ç§’")
        logger.info(f"  å‚è€ƒæ–‡æ¡£æ•°: {result['num_contexts']}")


if __name__ == "__main__":
    main()
