# -*- coding: utf-8 -*-
"""
æ–¹æ¡ˆC: Spark Streamingå¢é‡ç´¢å¼•
å®æ—¶ç›‘å¬æ–°æ•°æ®å¹¶è‡ªåŠ¨æ›´æ–°Milvuså‘é‡ç´¢å¼•
"""

import sys
import time
import json
from pathlib import Path
from typing import Dict, List
import threading

sys.path.append(str(Path(__file__).parent.parent.parent))
from config.config import *
from src.utils.logger import setup_logger

logger = setup_logger("spark_streaming", LOGS_DIR / "spark_streaming.log")


class IncrementalIndexer:
    """å¢é‡ç´¢å¼•ç®¡ç†å™¨ - åŸºäºSpark Structured Streaming"""
    
    def __init__(self, watch_dir: Path = None, use_spark: bool = True):
        """
        åˆå§‹åŒ–å¢é‡ç´¢å¼•å™¨
        
        Args:
            watch_dir: ç›‘å¬çš„æ•°æ®ç›®å½•
            use_spark: æ˜¯å¦ä½¿ç”¨Spark Streamingï¼ˆFalseåˆ™ç”¨ç®€å•æ–‡ä»¶ç›‘å¬ï¼‰
        """
        self.watch_dir = watch_dir or RAW_DATA_DIR
        self.use_spark = use_spark
        self.spark = None
        self.milvus = None
        self.embedder = None
        self.is_running = False
        self.processed_files = set()
        self.stats = {"files_processed": 0, "vectors_added": 0, "errors": 0}
        
        logger.info(f"ğŸ“¡ å¢é‡ç´¢å¼•å™¨åˆå§‹åŒ–")
        logger.info(f"   ç›‘å¬ç›®å½•: {self.watch_dir}")
        logger.info(f"   ä½¿ç”¨Spark: {use_spark}")
    
    def _init_components(self):
        """å»¶è¿Ÿåˆå§‹åŒ–ç»„ä»¶"""
        if self.embedder is None:
            from src.embedding.embedder import TextEmbedder
            self.embedder = TextEmbedder()
        
        if self.milvus is None:
            from src.retrieval.milvus_manager import MilvusManager
            self.milvus = MilvusManager()
            try:
                self.milvus.load_collection()
            except:
                self.milvus.create_collection()

    def _init_spark_streaming(self):
        """åˆå§‹åŒ–Spark Structured Streaming"""
        if self.spark is not None:
            return
        
        from pyspark.sql import SparkSession
        
        self.spark = SparkSession.builder \
            .appName("MedicalRAG-IncrementalIndex") \
            .master("local[2]") \
            .config("spark.driver.memory", "4g") \
            .config("spark.sql.streaming.schemaInference", "true") \
            .getOrCreate()
        
        self.spark.sparkContext.setLogLevel("WARN")
        logger.info("âœ… Spark Streamingåˆå§‹åŒ–å®Œæˆ")
    
    def process_new_file(self, file_path: Path) -> Dict:
        """å¤„ç†å•ä¸ªæ–°æ–‡ä»¶å¹¶æ›´æ–°ç´¢å¼•"""
        logger.info(f"ğŸ“„ å¤„ç†æ–°æ–‡ä»¶: {file_path.name}")
        
        self._init_components()
        
        try:
            # è¯»å–æ•°æ®
            with open(file_path, 'r', encoding='utf-8') as f:
                articles = json.load(f)
            
            if not articles:
                return {"status": "empty", "count": 0}
            
            # æå–æ–‡æœ¬
            texts = []
            metadata = []
            for article in articles:
                text = article.get('full_text') or article.get('abstract', '')
                if text and len(text) > 100:
                    texts.append(text[:2000])  # æˆªæ–­
                    metadata.append({
                        'pmid': str(article.get('pmid', '')),
                        'chunk_text': text[:500]
                    })
            
            if not texts:
                return {"status": "no_valid_text", "count": 0}
            
            # å‘é‡åŒ–
            logger.info(f"   å‘é‡åŒ– {len(texts)} æ¡æ–‡æœ¬...")
            embeddings = self.embedder.encode_batch(texts, batch_size=64)
            
            # æ’å…¥Milvus
            logger.info(f"   æ’å…¥Milvus...")
            self.milvus.insert_vectors(embeddings, metadata)
            
            self.stats["files_processed"] += 1
            self.stats["vectors_added"] += len(texts)
            
            logger.info(f"âœ… å®Œæˆ: æ–°å¢ {len(texts)} ä¸ªå‘é‡")
            
            return {"status": "success", "count": len(texts)}
            
        except Exception as e:
            self.stats["errors"] += 1
            logger.error(f"âŒ å¤„ç†å¤±è´¥: {e}")
            return {"status": "error", "error": str(e)}
    
    def start_file_watcher(self, interval: int = 30):
        """å¯åŠ¨æ–‡ä»¶ç›‘å¬ï¼ˆç®€å•æ¨¡å¼ï¼‰"""
        logger.info(f"ğŸ‘€ å¯åŠ¨æ–‡ä»¶ç›‘å¬ï¼Œé—´éš”: {interval}ç§’")
        self.is_running = True
        
        def watch_loop():
            while self.is_running:
                try:
                    # æ‰«ææ–°æ–‡ä»¶
                    for file_path in self.watch_dir.glob("*.json"):
                        if file_path.name not in self.processed_files:
                            self.process_new_file(file_path)
                            self.processed_files.add(file_path.name)
                    
                    time.sleep(interval)
                except Exception as e:
                    logger.error(f"ç›‘å¬é”™è¯¯: {e}")
                    time.sleep(interval)
        
        self.watch_thread = threading.Thread(target=watch_loop, daemon=True)
        self.watch_thread.start()
        logger.info("âœ… æ–‡ä»¶ç›‘å¬å·²å¯åŠ¨")
    
    def start_spark_streaming(self):
        """å¯åŠ¨Spark Structured Streaming"""
        self._init_spark_streaming()
        self._init_components()
        
        logger.info("ğŸš€ å¯åŠ¨Spark Structured Streaming...")
        
        # åˆ›å»ºæµå¼è¯»å–
        schema = "pmid STRING, title STRING, abstract STRING, full_text STRING, topic STRING"
        
        stream_df = self.spark.readStream \
            .format("json") \
            .schema(schema) \
            .option("maxFilesPerTrigger", 1) \
            .load(str(self.watch_dir))
        
        # å¤„ç†å‡½æ•°
        def process_batch(batch_df, batch_id):
            if batch_df.count() == 0:
                return
            
            logger.info(f"ğŸ“¦ å¤„ç†æ‰¹æ¬¡ {batch_id}: {batch_df.count()} æ¡")
            
            # è½¬ä¸ºPandaså¤„ç†
            pdf = batch_df.toPandas()
            texts = pdf['full_text'].fillna(pdf['abstract']).tolist()
            texts = [t[:2000] for t in texts if t and len(str(t)) > 100]
            
            if texts:
                embeddings = self.embedder.encode_batch(texts, batch_size=64)
                metadata = [{'pmid': str(row['pmid']), 'chunk_text': texts[i][:500]} 
                           for i, row in pdf.iterrows() if i < len(texts)]
                self.milvus.insert_vectors(embeddings, metadata)
                self.stats["vectors_added"] += len(texts)
            
            logger.info(f"âœ… æ‰¹æ¬¡ {batch_id} å®Œæˆ: {len(texts)} å‘é‡")
        
        # å¯åŠ¨æµ
        query = stream_df.writeStream \
            .foreachBatch(process_batch) \
            .trigger(processingTime="30 seconds") \
            .start()
        
        self.is_running = True
        logger.info("âœ… Spark Streamingå·²å¯åŠ¨")
        
        return query
    
    def stop(self):
        """åœæ­¢å¢é‡ç´¢å¼•"""
        self.is_running = False
        if self.spark:
            self.spark.stop()
        logger.info("ğŸ›‘ å¢é‡ç´¢å¼•å·²åœæ­¢")
    
    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            **self.stats,
            "watch_dir": str(self.watch_dir),
            "is_running": self.is_running
        }


class BatchIncrementalUpdater:
    """æ‰¹é‡å¢é‡æ›´æ–°å™¨ - ç”¨äºå®šæœŸæ‰¹é‡æ›´æ–°ç´¢å¼•"""
    
    def __init__(self):
        self.embedder = None
        self.milvus = None
    
    def update_from_new_data(self, data_file: Path, batch_size: int = 1000) -> Dict:
        """ä»æ–°æ•°æ®æ–‡ä»¶æ‰¹é‡æ›´æ–°ç´¢å¼•"""
        logger.info(f"ğŸ“Š æ‰¹é‡å¢é‡æ›´æ–°: {data_file}")
        
        # åˆå§‹åŒ–
        if self.embedder is None:
            from src.embedding.embedder import TextEmbedder
            self.embedder = TextEmbedder()
        
        if self.milvus is None:
            from src.retrieval.milvus_manager import MilvusManager
            self.milvus = MilvusManager()
            self.milvus.load_collection()
        
        # è¯»å–æ•°æ®
        import pandas as pd
        if data_file.suffix == '.parquet':
            df = pd.read_parquet(data_file)
        else:
            df = pd.read_json(data_file)
        
        logger.info(f"   æ•°æ®é‡: {len(df):,} æ¡")
        
        # è·å–å½“å‰ç´¢å¼•æ•°é‡
        current_count = self.milvus.collection.num_entities
        logger.info(f"   å½“å‰ç´¢å¼•: {current_count:,} å‘é‡")
        
        # æ‰¹é‡å¤„ç†
        text_col = 'content' if 'content' in df.columns else 'chunk_text'
        total_added = 0
        
        for i in range(0, len(df), batch_size):
            batch_df = df.iloc[i:i+batch_size]
            texts = batch_df[text_col].tolist()
            
            # å‘é‡åŒ–
            embeddings = self.embedder.encode_batch(texts, batch_size=128)
            
            # å‡†å¤‡å…ƒæ•°æ®
            metadata = []
            for _, row in batch_df.iterrows():
                metadata.append({
                    'pmid': str(row.get('pmid', '')),
                    'chunk_text': str(row.get(text_col, ''))[:500]
                })
            
            # æ’å…¥
            self.milvus.insert_vectors(embeddings, metadata)
            total_added += len(texts)
            
            logger.info(f"   è¿›åº¦: {total_added:,}/{len(df):,}")
        
        # é‡å»ºç´¢å¼•
        self.milvus.create_index()
        self.milvus.load_collection()
        
        new_count = self.milvus.collection.num_entities
        
        result = {
            "previous_count": current_count,
            "added": total_added,
            "new_count": new_count,
            "status": "success"
        }
        
        logger.info(f"âœ… å¢é‡æ›´æ–°å®Œæˆ: {current_count:,} -> {new_count:,} (+{total_added:,})")
        
        return result


def main():
    """æµ‹è¯•å…¥å£"""
    print("=" * 60)
    print("ğŸš€ Spark Streamingå¢é‡ç´¢å¼•æµ‹è¯•")
    print("=" * 60)
    
    # ç®€å•æ–‡ä»¶ç›‘å¬æ¨¡å¼
    indexer = IncrementalIndexer(use_spark=False)
    indexer.start_file_watcher(interval=10)
    
    print("\nç›‘å¬ä¸­... æŒ‰Ctrl+Cåœæ­¢")
    try:
        while True:
            time.sleep(5)
            stats = indexer.get_stats()
            print(f"ç»Ÿè®¡: æ–‡ä»¶={stats['files_processed']}, å‘é‡={stats['vectors_added']}")
    except KeyboardInterrupt:
        indexer.stop()
        print("\nå·²åœæ­¢")


if __name__ == "__main__":
    main()
