# -*- coding: utf-8 -*-
"""
Milvuså‘é‡æ•°æ®åº“ç®¡ç†
"""

import numpy as np
import json
from pathlib import Path
from typing import List, Dict, Tuple
from pymilvus import (
    connections, Collection, CollectionSchema, FieldSchema,
    DataType, utility
)
import sys
sys.path.append(str(Path(__file__).parent.parent.parent))

from config.config import *
from src.utils.logger import setup_logger

logger = setup_logger("milvus_manager", LOGS_DIR / "milvus.log")


class MilvusManager:
    """Milvuså‘é‡æ•°æ®åº“ç®¡ç†å™¨"""
    
    def __init__(self, host: str = MILVUS_HOST, port: int = MILVUS_PORT):
        """
        åˆå§‹åŒ–Milvusè¿æ¥
        
        Args:
            host: MilvusæœåŠ¡åœ°å€
            port: MilvusæœåŠ¡ç«¯å£
        """
        logger.info(f"è¿æ¥Milvus: {host}:{port}")
        
        try:
            connections.connect(
                alias="default",
                host=host,
                port=port
            )
            logger.info("âœ… Milvusè¿æ¥æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ Milvusè¿æ¥å¤±è´¥: {e}")
            logger.info("è¯·ç¡®ä¿MilvusæœåŠ¡å·²å¯åŠ¨ (docker-compose up -d)")
            raise
        
        self.collection = None
    
    def create_collection(self, collection_name: str = MILVUS_COLLECTION_NAME,
                         dimension: int = EMBEDDING_DIMENSION):
        """
        åˆ›å»ºé›†åˆ
        
        Args:
            collection_name: é›†åˆåç§°
            dimension: å‘é‡ç»´åº¦
        """
        # å¦‚æœé›†åˆå·²å­˜åœ¨ï¼Œåˆ é™¤å®ƒ
        if utility.has_collection(collection_name):
            logger.warning(f"é›†åˆ {collection_name} å·²å­˜åœ¨ï¼Œå°†åˆ é™¤é‡å»º")
            utility.drop_collection(collection_name)
        
        # å®šä¹‰å­—æ®µschema
        fields = [
            FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
            FieldSchema(name="pmid", dtype=DataType.VARCHAR, max_length=50),
            FieldSchema(name="chunk_text", dtype=DataType.VARCHAR, max_length=2000),
            FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=dimension)
        ]
        
        schema = CollectionSchema(
            fields=fields,
            description="Medical Knowledge Base"
        )
        
        # åˆ›å»ºé›†åˆ
        self.collection = Collection(
            name=collection_name,
            schema=schema
        )
        
        logger.info(f"âœ… é›†åˆ {collection_name} åˆ›å»ºæˆåŠŸ")
        logger.info(f"   å‘é‡ç»´åº¦: {dimension}")
    
    def create_index(self):
        """åˆ›å»ºç´¢å¼•"""
        if not self.collection:
            logger.error("è¯·å…ˆåˆ›å»ºé›†åˆ")
            return
        
        logger.info("åˆ›å»ºç´¢å¼•...")
        
        index_params = {
            "index_type": MILVUS_INDEX_TYPE,
            "metric_type": MILVUS_METRIC_TYPE,
            "params": {"nlist": MILVUS_NLIST}
        }
        
        self.collection.create_index(
            field_name="embedding",
            index_params=index_params
        )
        
        logger.info(f"âœ… ç´¢å¼•åˆ›å»ºæˆåŠŸ")
        logger.info(f"   ç±»å‹: {MILVUS_INDEX_TYPE}")
        logger.info(f"   åº¦é‡: {MILVUS_METRIC_TYPE}")
    
    def insert_vectors(self, embeddings: np.ndarray, metadata: List[Dict],
                      batch_size: int = 1000):
        """
        æ’å…¥å‘é‡
        
        Args:
            embeddings: å‘é‡æ•°ç»„
            metadata: å…ƒæ•°æ®åˆ—è¡¨
            batch_size: æ‰¹é‡æ’å…¥å¤§å°
        """
        if not self.collection:
            logger.error("è¯·å…ˆåˆ›å»ºé›†åˆ")
            return
        
        total = len(embeddings)
        logger.info(f"å¼€å§‹æ’å…¥ {total:,} æ¡å‘é‡ï¼Œæ‰¹é‡å¤§å°: {batch_size}")
        
        inserted = 0
        for i in range(0, total, batch_size):
            batch_end = min(i + batch_size, total)
            batch_embeddings = embeddings[i:batch_end]
            batch_metadata = metadata[i:batch_end]
            
            # å‡†å¤‡æ•°æ®
            entities = [
                [m.get("pmid", "") for m in batch_metadata],
                [m.get("chunk_text", "")[:2000] for m in batch_metadata],  # æˆªæ–­åˆ°æœ€å¤§é•¿åº¦
                batch_embeddings.tolist()
            ]
            
            try:
                self.collection.insert(entities)
                inserted += len(batch_embeddings)
                
                if (i // batch_size + 1) % 10 == 0:
                    logger.info(f"  å·²æ’å…¥ {inserted:,}/{total:,} ({inserted/total*100:.1f}%)")
                    
            except Exception as e:
                logger.error(f"æ’å…¥æ‰¹æ¬¡å¤±è´¥: {e}")
                continue
        
        # åˆ·æ–°ç¡®ä¿æ•°æ®æŒä¹…åŒ–
        self.collection.flush()
        logger.info(f"âœ… å…±æ’å…¥ {inserted:,} æ¡å‘é‡")
        
        return inserted
    
    def load_collection(self, collection_name: str = MILVUS_COLLECTION_NAME):
        """åŠ è½½é›†åˆåˆ°å†…å­˜"""
        if not self.collection:
            self.collection = Collection(collection_name)
        
        self.collection.load()
        logger.info(f"âœ… é›†åˆ {collection_name} å·²åŠ è½½åˆ°å†…å­˜")
    
    def search(self, query_vectors: np.ndarray, top_k: int = RETRIEVAL_TOP_K) -> List[List[Dict]]:
        """
        å‘é‡æœç´¢
        
        Args:
            query_vectors: æŸ¥è¯¢å‘é‡
            top_k: è¿”å›Top-Kç»“æœ
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        if not self.collection:
            logger.error("è¯·å…ˆåŠ è½½é›†åˆ")
            return []
        
        search_params = {
            "metric_type": MILVUS_METRIC_TYPE,
            "params": {"nprobe": MILVUS_NPROBE}
        }
        
        results = self.collection.search(
            data=query_vectors.tolist() if isinstance(query_vectors, np.ndarray) else query_vectors,
            anns_field="embedding",
            param=search_params,
            limit=top_k,
            output_fields=["pmid", "chunk_text"]
        )
        
        # æ ¼å¼åŒ–ç»“æœ
        formatted_results = []
        for hits in results:
            batch_results = []
            for hit in hits:
                batch_results.append({
                    "id": hit.id,
                    "pmid": hit.entity.get("pmid"),
                    "text": hit.entity.get("chunk_text"),
                    "score": hit.score
                })
            formatted_results.append(batch_results)
        
        return formatted_results
    
    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        if not self.collection:
            return {}
        
        stats = {
            "name": self.collection.name,
            "num_entities": self.collection.num_entities,
            "description": self.collection.description
        }
        
        return stats


def rebuild_database(resume: bool = False, batch_size: int = 128):
    """
    é‡å»ºå‘é‡æ•°æ®åº“ï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰
    
    Args:
        resume: æ˜¯å¦æ–­ç‚¹ç»­ä¼ 
        batch_size: æ‰¹æ¬¡å¤§å°
    """
    import time
    import pandas as pd
    
    logger.info("="*60)
    logger.info("ğŸ”„ é‡å»ºå‘é‡æ•°æ®åº“")
    logger.info(f"   æ¨¡å¼: {'æ–­ç‚¹ç»­ä¼ ' if resume else 'ä»å¤´å¼€å§‹'}")
    logger.info(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
    logger.info("="*60)
    
    # ä¼˜å…ˆä½¿ç”¨ Parquet æ–‡ä»¶
    parquet_file = PROCESSED_DATA_DIR / "parquet" / "medical_chunks.parquet"
    chunks_file = PROCESSED_DATA_DIR / "medical_chunks.json"
    
    if parquet_file.exists():
        logger.info(f"åŠ è½½Parquetæ•°æ®: {parquet_file}")
        df = pd.read_parquet(parquet_file)
        chunks = df.to_dict('records')
        logger.info(f"æ€»chunks: {len(chunks):,}")
    elif chunks_file.exists():
        logger.info(f"åŠ è½½JSONæ•°æ®: {chunks_file}")
        with open(chunks_file, 'r', encoding='utf-8') as f:
            chunks = json.load(f)
        logger.info(f"æ€»chunks: {len(chunks):,}")
    else:
        logger.error(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {parquet_file} æˆ– {chunks_file}")
        return
    
    # åˆå§‹åŒ–
    from src.embedding.embedder import TextEmbedder
    embedder = TextEmbedder()
    milvus = MilvusManager()
    
    # æ£€æŸ¥æ–­ç‚¹ç»­ä¼ 
    start_index = 0
    if resume:
        try:
            milvus.load_collection()
            start_index = milvus.collection.num_entities
            logger.info(f"æ£€æµ‹åˆ°å·²æœ‰ {start_index:,} ä¸ªå‘é‡ï¼Œä»ç¬¬ {start_index+1} æ¡ç»§ç»­")
        except:
            resume = False
    
    if not resume or start_index == 0:
        milvus.create_collection()
        start_index = 0
    
    chunks_to_process = chunks[start_index:]
    if len(chunks_to_process) == 0:
        logger.info(f"æ‰€æœ‰ {len(chunks):,} ä¸ªchunkså·²å®Œæˆ")
        return
    
    # å‘é‡åŒ–å’Œå…¥åº“
    logger.info(f"å¾…å¤„ç†: {len(chunks_to_process):,} ä¸ªchunks")
    
    start_time = time.time()
    total_inserted = start_index
    buffer_embeddings = []
    buffer_metadata = []
    insert_interval = 50
    total_batches = (len(chunks_to_process) + batch_size - 1) // batch_size
    
    # æ–­ç‚¹ä¿å­˜é—´éš”
    checkpoint_file = EMBEDDING_DATA_DIR / "rebuild_checkpoint.json"
    save_checkpoint_interval = 10000  # æ¯1ä¸‡æ¡ä¿å­˜ä¸€æ¬¡æ–­ç‚¹
    
    for i in range(0, len(chunks_to_process), batch_size):
        batch = chunks_to_process[i:i + batch_size]
        batch_num = i // batch_size + 1
        
        texts = [c.get('chunk_text') or c.get('content', '') for c in batch]
        embeddings = embedder.encode_batch(texts)
        
        metadata_batch = [{'pmid': str(c.get('pmid', '')), 'chunk_text': t[:2000]} for c, t in zip(batch, texts)]
        
        buffer_embeddings.append(embeddings)
        buffer_metadata.extend(metadata_batch)
        
        if batch_num % insert_interval == 0 or batch_num == total_batches:
            batch_vectors = np.vstack(buffer_embeddings)
            milvus.insert_vectors(batch_vectors, buffer_metadata, batch_size=5000)
            total_inserted += len(batch_vectors)
            buffer_embeddings = []
            buffer_metadata = []
            
            progress = total_inserted / len(chunks) * 100
            elapsed = time.time() - start_time
            speed = (total_inserted - start_index) / elapsed if elapsed > 0 else 0
            eta_seconds = (len(chunks) - total_inserted) / speed if speed > 0 else 0
            eta_minutes = eta_seconds / 60
            
            logger.info(f"è¿›åº¦: {progress:.1f}% | å·²å…¥åº“: {total_inserted:,}/{len(chunks):,} | "
                       f"é€Ÿåº¦: {speed:.0f}æ¡/ç§’ | é¢„è®¡å‰©ä½™: {eta_minutes:.1f}åˆ†é’Ÿ")
            
            # ä¿å­˜æ–­ç‚¹
            if total_inserted % save_checkpoint_interval < batch_size * insert_interval:
                with open(checkpoint_file, 'w') as f:
                    json.dump({'processed': total_inserted, 'total': len(chunks)}, f)
                logger.info(f"  ğŸ’¾ æ–­ç‚¹å·²ä¿å­˜: {total_inserted:,}")
    
    milvus.create_index()
    milvus.load_collection()
    
    total_time = time.time() - start_time
    logger.info(f"âœ… é‡å»ºå®Œæˆ! å‘é‡æ•°: {total_inserted:,}, è€—æ—¶: {total_time/60:.1f}åˆ†é’Ÿ")


def main():
    """ä¸»å‡½æ•° - æ„å»ºå‘é‡æ•°æ®åº“"""
    logger.info("="*50)
    logger.info("æ„å»ºMilvuså‘é‡æ•°æ®åº“")
    logger.info("="*50)
    
    # 1. è¿æ¥Milvus
    manager = MilvusManager()
    
    # 2. åˆ›å»ºé›†åˆ
    manager.create_collection()
    
    # 3. è¯»å–å‘é‡æ•°æ®
    embedding_file = EMBEDDING_DATA_DIR / "medical_embeddings.npy"
    mapping_file = EMBEDDING_DATA_DIR / "medical_embeddings.mapping.json"
    
    if not embedding_file.exists():
        logger.error(f"å‘é‡æ–‡ä»¶ä¸å­˜åœ¨: {embedding_file}")
        logger.info("è¯·å…ˆè¿è¡Œ embedder.py ç”Ÿæˆå‘é‡")
        return
    
    logger.info(f"è¯»å–å‘é‡: {embedding_file}")
    embeddings = np.load(embedding_file)
    logger.info(f"  å‘é‡æ•°é‡: {len(embeddings):,}")
    logger.info(f"  å‘é‡ç»´åº¦: {embeddings.shape[1]}")
    
    logger.info(f"è¯»å–å…ƒæ•°æ®: {mapping_file}")
    with open(mapping_file, 'r', encoding='utf-8') as f:
        metadata = json.load(f)
    logger.info(f"  å…ƒæ•°æ®æ•°é‡: {len(metadata):,}")
    
    # 4. æ’å…¥å‘é‡
    manager.insert_vectors(embeddings, metadata)
    
    # 5. åˆ›å»ºç´¢å¼•
    manager.create_index()
    
    # 6. åŠ è½½é›†åˆ
    manager.load_collection()
    
    # 7. ç»Ÿè®¡ä¿¡æ¯
    stats = manager.get_stats()
    logger.info("\nå‘é‡æ•°æ®åº“ç»Ÿè®¡:")
    for key, value in stats.items():
        logger.info(f"  {key}: {value}")
    
    logger.info("\nâœ… å‘é‡æ•°æ®åº“æ„å»ºå®Œæˆï¼")


if __name__ == "__main__":
    main()
