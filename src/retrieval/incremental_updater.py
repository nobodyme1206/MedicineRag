# -*- coding: utf-8 -*-
"""
å¢é‡æ›´æ–°æ¨¡å—
åŠŸèƒ½: å‘é‡åº“å¢é‡æ›´æ–°ã€å®šæ—¶ä»»åŠ¡ã€å˜æ›´æ£€æµ‹
"""

import os
import time
import json
import hashlib
import schedule
import threading
from pathlib import Path
from typing import List, Dict, Optional, Set
from datetime import datetime
import numpy as np
import sys

sys.path.append(str(Path(__file__).parent.parent.parent))
from config.config import *
from src.utils.logger import setup_logger

logger = setup_logger("incremental_updater", LOGS_DIR / "incremental_update.log")


class ChangeDetector:
    """å˜æ›´æ£€æµ‹å™¨"""
    
    def __init__(self, state_file: Path = None):
        """
        åˆå§‹åŒ–å˜æ›´æ£€æµ‹å™¨
        
        Args:
            state_file: çŠ¶æ€æ–‡ä»¶è·¯å¾„
        """
        self.state_file = state_file or (PROCESSED_DATA_DIR / "update_state.json")
        self.state = self._load_state()
    
    def _load_state(self) -> Dict:
        """åŠ è½½çŠ¶æ€"""
        if self.state_file.exists():
            with open(self.state_file, 'r') as f:
                return json.load(f)
        return {
            "last_update": None,
            "file_hashes": {},
            "processed_pmids": []
        }
    
    def _save_state(self):
        """ä¿å­˜çŠ¶æ€"""
        self.state_file.parent.mkdir(parents=True, exist_ok=True)
        with open(self.state_file, 'w') as f:
            json.dump(self.state, f, indent=2)
    
    def _compute_file_hash(self, file_path: Path) -> str:
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œ"""
        hasher = hashlib.md5()
        with open(file_path, 'rb') as f:
            for chunk in iter(lambda: f.read(8192), b''):
                hasher.update(chunk)
        return hasher.hexdigest()
    
    def detect_changes(self, data_dir: Path = None) -> Dict:
        """
        æ£€æµ‹æ•°æ®å˜æ›´
        
        Returns:
            å˜æ›´ä¿¡æ¯
        """
        data_dir = data_dir or RAW_DATA_DIR
        
        changes = {
            "new_files": [],
            "modified_files": [],
            "deleted_files": [],
            "has_changes": False
        }
        
        # æ‰«æå½“å‰æ–‡ä»¶
        current_files = {}
        for file_path in data_dir.glob("*.json"):
            file_hash = self._compute_file_hash(file_path)
            current_files[str(file_path)] = file_hash
        
        # æ£€æµ‹æ–°å¢å’Œä¿®æ”¹
        for file_path, file_hash in current_files.items():
            if file_path not in self.state["file_hashes"]:
                changes["new_files"].append(file_path)
            elif self.state["file_hashes"][file_path] != file_hash:
                changes["modified_files"].append(file_path)
        
        # æ£€æµ‹åˆ é™¤
        for file_path in self.state["file_hashes"]:
            if file_path not in current_files:
                changes["deleted_files"].append(file_path)
        
        changes["has_changes"] = bool(
            changes["new_files"] or 
            changes["modified_files"] or 
            changes["deleted_files"]
        )
        
        # æ›´æ–°çŠ¶æ€
        if changes["has_changes"]:
            self.state["file_hashes"] = current_files
            self.state["last_update"] = datetime.now().isoformat()
            self._save_state()
        
        return changes
    
    def get_new_articles(self, articles: List[Dict]) -> List[Dict]:
        """
        è·å–æ–°å¢æ–‡ç« ï¼ˆæœªå¤„ç†è¿‡çš„ï¼‰
        
        Args:
            articles: æ–‡ç« åˆ—è¡¨
            
        Returns:
            æ–°å¢æ–‡ç« åˆ—è¡¨
        """
        processed_pmids = set(self.state.get("processed_pmids", []))
        new_articles = [a for a in articles if a.get("pmid") not in processed_pmids]
        return new_articles
    
    def mark_processed(self, pmids: List[str]):
        """æ ‡è®°å·²å¤„ç†çš„PMID"""
        self.state["processed_pmids"].extend(pmids)
        # ä¿æŒåˆ—è¡¨ä¸è¦å¤ªå¤§
        if len(self.state["processed_pmids"]) > 1000000:
            self.state["processed_pmids"] = self.state["processed_pmids"][-500000:]
        self._save_state()


class IncrementalUpdater:
    """å¢é‡æ›´æ–°å™¨"""
    
    def __init__(self, embedder=None, milvus_manager=None):
        """
        åˆå§‹åŒ–å¢é‡æ›´æ–°å™¨
        
        Args:
            embedder: å‘é‡åŒ–å™¨
            milvus_manager: Milvusç®¡ç†å™¨
        """
        self.embedder = embedder
        self.milvus = milvus_manager
        self.change_detector = ChangeDetector()
        
        self.stats = {
            "total_updates": 0,
            "total_vectors_added": 0,
            "last_update_time": None,
            "errors": 0
        }
        
        self._running = False
        self._scheduler_thread = None
    
    def _init_components(self):
        """å»¶è¿Ÿåˆå§‹åŒ–ç»„ä»¶"""
        if self.embedder is None:
            from src.embedding.embedder import TextEmbedder
            self.embedder = TextEmbedder()
            logger.info("âœ… å‘é‡åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
        
        if self.milvus is None:
            from src.retrieval.milvus_manager import MilvusManager
            self.milvus = MilvusManager()
            self.milvus.load_collection()
            logger.info("âœ… Milvusè¿æ¥å®Œæˆ")
    
    def update_from_file(self, file_path: Path, batch_size: int = 128) -> Dict:
        """
        ä»æ–‡ä»¶å¢é‡æ›´æ–°
        
        Args:
            file_path: æ•°æ®æ–‡ä»¶è·¯å¾„
            batch_size: æ‰¹æ¬¡å¤§å°
            
        Returns:
            æ›´æ–°ç»Ÿè®¡
        """
        logger.info(f"ğŸ“‚ ä»æ–‡ä»¶å¢é‡æ›´æ–°: {file_path}")
        
        self._init_components()
        
        # åŠ è½½æ•°æ®
        with open(file_path, 'r', encoding='utf-8') as f:
            articles = json.load(f)
        
        # è·å–æ–°å¢æ–‡ç« 
        new_articles = self.change_detector.get_new_articles(articles)
        
        if not new_articles:
            logger.info("   æ— æ–°å¢æ•°æ®")
            return {"added": 0, "skipped": len(articles)}
        
        logger.info(f"   æ–°å¢æ–‡ç« : {len(new_articles):,}")
        
        # å‘é‡åŒ–å¹¶æ’å…¥
        added = 0
        for i in range(0, len(new_articles), batch_size):
            batch = new_articles[i:i + batch_size]
            
            try:
                # æå–æ–‡æœ¬
                texts = [f"{a.get('title', '')} {a.get('abstract', '')}" for a in batch]
                
                # å‘é‡åŒ–
                embeddings = self.embedder.encode_batch(texts)
                
                # å‡†å¤‡å…ƒæ•°æ®
                metadata = [
                    {'pmid': str(a.get('pmid', '')), 'chunk_text': texts[j][:2000]}
                    for j, a in enumerate(batch)
                ]
                
                # æ’å…¥Milvus
                self.milvus.insert_vectors(embeddings, metadata)
                
                added += len(batch)
                
                # æ ‡è®°å·²å¤„ç†
                pmids = [a.get('pmid') for a in batch]
                self.change_detector.mark_processed(pmids)
                
            except Exception as e:
                logger.error(f"æ‰¹æ¬¡å¤„ç†å¤±è´¥: {e}")
                self.stats["errors"] += 1
        
        self.stats["total_vectors_added"] += added
        self.stats["total_updates"] += 1
        self.stats["last_update_time"] = datetime.now().isoformat()
        
        logger.info(f"âœ… å¢é‡æ›´æ–°å®Œæˆ: æ–°å¢ {added:,} æ¡å‘é‡")
        
        return {"added": added, "skipped": len(articles) - len(new_articles)}
    
    def check_and_update(self) -> Dict:
        """
        æ£€æŸ¥å˜æ›´å¹¶æ›´æ–°
        
        Returns:
            æ›´æ–°ç»“æœ
        """
        logger.info("ğŸ” æ£€æŸ¥æ•°æ®å˜æ›´...")
        
        changes = self.change_detector.detect_changes()
        
        if not changes["has_changes"]:
            logger.info("   æ— å˜æ›´")
            return {"status": "no_changes"}
        
        logger.info(f"   æ£€æµ‹åˆ°å˜æ›´:")
        logger.info(f"   - æ–°å¢æ–‡ä»¶: {len(changes['new_files'])}")
        logger.info(f"   - ä¿®æ”¹æ–‡ä»¶: {len(changes['modified_files'])}")
        logger.info(f"   - åˆ é™¤æ–‡ä»¶: {len(changes['deleted_files'])}")
        
        # å¤„ç†æ–°å¢å’Œä¿®æ”¹çš„æ–‡ä»¶
        results = []
        for file_path in changes["new_files"] + changes["modified_files"]:
            result = self.update_from_file(Path(file_path))
            results.append(result)
        
        total_added = sum(r.get("added", 0) for r in results)
        
        return {
            "status": "updated",
            "changes": changes,
            "total_added": total_added
        }
    
    def start_scheduler(self, interval_minutes: int = 60):
        """
        å¯åŠ¨å®šæ—¶æ›´æ–°ä»»åŠ¡
        
        Args:
            interval_minutes: æ›´æ–°é—´éš”ï¼ˆåˆ†é’Ÿï¼‰
        """
        logger.info(f"â° å¯åŠ¨å®šæ—¶æ›´æ–°ä»»åŠ¡ (é—´éš”: {interval_minutes}åˆ†é’Ÿ)")
        
        self._running = True
        
        # è®¾ç½®å®šæ—¶ä»»åŠ¡
        schedule.every(interval_minutes).minutes.do(self._scheduled_update)
        
        # å¯åŠ¨è°ƒåº¦çº¿ç¨‹
        def run_scheduler():
            while self._running:
                schedule.run_pending()
                time.sleep(10)
        
        self._scheduler_thread = threading.Thread(target=run_scheduler, daemon=True)
        self._scheduler_thread.start()
        
        logger.info("âœ… å®šæ—¶ä»»åŠ¡å·²å¯åŠ¨")
    
    def _scheduled_update(self):
        """å®šæ—¶æ›´æ–°ä»»åŠ¡"""
        logger.info(f"\n{'='*60}")
        logger.info(f"â° æ‰§è¡Œå®šæ—¶æ›´æ–° - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info(f"{'='*60}")
        
        try:
            result = self.check_and_update()
            logger.info(f"æ›´æ–°ç»“æœ: {result}")
        except Exception as e:
            logger.error(f"å®šæ—¶æ›´æ–°å¤±è´¥: {e}")
            self.stats["errors"] += 1
    
    def stop_scheduler(self):
        """åœæ­¢å®šæ—¶ä»»åŠ¡"""
        self._running = False
        schedule.clear()
        logger.info("â¹ï¸ å®šæ—¶ä»»åŠ¡å·²åœæ­¢")
    
    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            **self.stats,
            "milvus_vectors": self.milvus.collection.num_entities if self.milvus else 0
        }


def main():
    """ä¸»å‡½æ•°"""
    logger.info("=" * 60)
    logger.info("å¢é‡æ›´æ–°æ¨¡å—æ¼”ç¤º")
    logger.info("=" * 60)
    
    # åˆå§‹åŒ–
    updater = IncrementalUpdater()
    
    # æ£€æŸ¥å˜æ›´
    logger.info("\n1. æ£€æŸ¥æ•°æ®å˜æ›´...")
    changes = updater.change_detector.detect_changes()
    logger.info(f"   å˜æ›´æ£€æµ‹ç»“æœ: {changes}")
    
    # æ‰‹åŠ¨æ›´æ–°
    data_file = RAW_DATA_DIR / "pubmed_articles_all.json"
    if data_file.exists():
        logger.info("\n2. æ‰§è¡Œå¢é‡æ›´æ–°...")
        result = updater.update_from_file(data_file)
        logger.info(f"   æ›´æ–°ç»“æœ: {result}")
    
    # å¯åŠ¨å®šæ—¶ä»»åŠ¡ï¼ˆæ¼”ç¤ºï¼‰
    logger.info("\n3. å®šæ—¶ä»»åŠ¡æ¼”ç¤º...")
    logger.info("   ä½¿ç”¨æ–¹æ³•: updater.start_scheduler(interval_minutes=60)")
    logger.info("   åœæ­¢æ–¹æ³•: updater.stop_scheduler()")
    
    # æ‰“å°ç»Ÿè®¡
    logger.info(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯: {updater.get_stats()}")
    
    logger.info("\nâœ… æ¼”ç¤ºå®Œæˆ!")


if __name__ == "__main__":
    main()
