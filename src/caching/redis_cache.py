#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Redisç¼“å­˜å±‚å®ç°
æå‡æŸ¥è¯¢å“åº”é€Ÿåº¦ï¼Œå‡å°‘é‡å¤è®¡ç®—
"""

from __future__ import annotations

import json
import hashlib
import pickle
import time
from typing import Optional, List, Dict, Any, Union

import numpy as np
from numpy.typing import NDArray

from config.config import REDIS_HOST, REDIS_PORT, LOGS_DIR
from src.utils.logger import setup_logger
from src.utils.exceptions import CacheError, handle_errors, retry

logger = setup_logger("redis_cache", LOGS_DIR / "redis_cache.log")

# ç±»å‹åˆ«å
Vector = NDArray[np.float32]
CacheKey = str
CacheValue = Union[List[Dict], Vector, Dict, str]


class RedisCache:
    """Redisç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(
        self,
        host: str = REDIS_HOST,
        port: int = REDIS_PORT,
        db: int = 0,
        password: Optional[str] = None,
        ttl: int = 3600
    ) -> None:
        """
        åˆå§‹åŒ–Redisç¼“å­˜
        
        Args:
            host: Redisä¸»æœº
            port: Redisç«¯å£
            db: æ•°æ®åº“ç¼–å·
            password: å¯†ç 
            ttl: é»˜è®¤ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰
        """
        self.host = host
        self.port = port
        self.db = db
        self.ttl = ttl
        self.client = None
        
        self._connect(password)
    
    def _connect(self, password: Optional[str] = None) -> None:
        """å»ºç«‹Redisè¿æ¥"""
        try:
            import redis
            self.client = redis.Redis(
                host=self.host,
                port=self.port,
                db=self.db,
                password=password,
                decode_responses=False
            )
            self.client.ping()
            logger.info(f"âœ… Redisè¿æ¥æˆåŠŸ: {self.host}:{self.port}")
        except Exception as e:
            logger.warning(f"âš ï¸ Redisè¿æ¥å¤±è´¥: {e}")
            self.client = None
    
    @property
    def is_connected(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å·²è¿æ¥"""
        return self.client is not None
    
    def _generate_key(self, prefix: str, data: Any) -> CacheKey:
        """
        ç”Ÿæˆç¼“å­˜é”®
        
        Args:
            prefix: é”®å‰ç¼€
            data: æ•°æ®ï¼ˆç”¨äºç”Ÿæˆhashï¼‰
            
        Returns:
            ç¼“å­˜é”®
        """
        if isinstance(data, str):
            hash_str = data
        elif isinstance(data, (list, dict)):
            hash_str = json.dumps(data, sort_keys=True)
        elif isinstance(data, np.ndarray):
            hash_str = data.tobytes().hex()[:32]
        else:
            hash_str = str(data)
        
        hash_value = hashlib.md5(hash_str.encode()).hexdigest()
        return f"{prefix}:{hash_value}"
    
    # ==================== æŸ¥è¯¢ç¼“å­˜ ====================
    
    @handle_errors(default_return=None, log_level="warning")
    def get_query_cache(self, query: str) -> Optional[List[Dict]]:
        """
        è·å–æŸ¥è¯¢ç»“æœç¼“å­˜
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            
        Returns:
            ç¼“å­˜çš„ç»“æœï¼Œä¸å­˜åœ¨è¿”å›None
        """
        if not self.is_connected:
            return None
        
        key = self._generate_key("query", query)
        cached = self.client.get(key)
        
        if cached:
            logger.debug(f"ç¼“å­˜å‘½ä¸­: query={query[:50]}...")
            return pickle.loads(cached)
        return None
    
    @handle_errors(default_return=False, log_level="warning")
    def set_query_cache(
        self, 
        query: str, 
        results: List[Dict], 
        ttl: Optional[int] = None
    ) -> bool:
        """
        è®¾ç½®æŸ¥è¯¢ç»“æœç¼“å­˜
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            results: æŸ¥è¯¢ç»“æœ
            ttl: è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        if not self.is_connected:
            return False
        
        key = self._generate_key("query", query)
        ttl = ttl or self.ttl
        
        self.client.setex(key, ttl, pickle.dumps(results))
        logger.debug(f"ç¼“å­˜å·²è®¾ç½®: query={query[:50]}..., ttl={ttl}s")
        return True
    
    # ==================== å‘é‡ç¼“å­˜ ====================
    
    @handle_errors(default_return=None, log_level="debug")
    def get_vector_cache(self, text: str) -> Optional[Vector]:
        """
        è·å–æ–‡æœ¬å‘é‡ç¼“å­˜
        
        Args:
            text: æ–‡æœ¬
            
        Returns:
            å‘é‡ï¼Œä¸å­˜åœ¨è¿”å›None
        """
        if not self.is_connected:
            return None
        
        key = self._generate_key("vector", text)
        cached = self.client.get(key)
        
        if cached:
            return pickle.loads(cached)
        return None
    
    @handle_errors(default_return=False, log_level="debug")
    def set_vector_cache(
        self, 
        text: str, 
        vector: Vector, 
        ttl: Optional[int] = None
    ) -> bool:
        """
        è®¾ç½®æ–‡æœ¬å‘é‡ç¼“å­˜
        
        Args:
            text: æ–‡æœ¬
            vector: å‘é‡
            ttl: è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        if not self.is_connected:
            return False
        
        key = self._generate_key("vector", text)
        ttl = ttl or self.ttl
        
        self.client.setex(key, ttl, pickle.dumps(vector))
        return True
    
    # ==================== ç»Ÿè®¡å’Œç®¡ç† ====================
    
    @handle_errors(default_return={"status": "error"}, log_level="warning")
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        if not self.is_connected:
            return {"status": "disconnected"}
        
        info = self.client.info()
        
        return {
            "status": "connected",
            "total_keys": self.client.dbsize(),
            "used_memory_mb": round(info["used_memory"] / (1024**2), 2),
            "used_memory_human": info["used_memory_human"],
            "hit_rate": self._calculate_hit_rate(info)
        }
    
    def _calculate_hit_rate(self, info: Dict) -> float:
        """è®¡ç®—ç¼“å­˜å‘½ä¸­ç‡"""
        hits = info.get("keyspace_hits", 0)
        misses = info.get("keyspace_misses", 0)
        total = hits + misses
        return round(hits / total * 100, 2) if total > 0 else 0.0
    
    @handle_errors(default_return=0, log_level="warning")
    def clear_cache(self, pattern: Optional[str] = None) -> int:
        """
        æ¸…ç©ºç¼“å­˜
        
        Args:
            pattern: é”®æ¨¡å¼ï¼ˆå¦‚"query:*"ï¼‰ï¼ŒNoneæ¸…ç©ºæ‰€æœ‰
            
        Returns:
            åˆ é™¤çš„é”®æ•°é‡
        """
        if not self.is_connected:
            return 0
        
        if pattern:
            keys = self.client.keys(pattern)
            if keys:
                count = self.client.delete(*keys)
                logger.info(f"æ¸…ç©ºç¼“å­˜: {pattern}, åˆ é™¤ {count} ä¸ªé”®")
                return count
            return 0
        else:
            self.client.flushdb()
            logger.info("æ¸…ç©ºæ‰€æœ‰ç¼“å­˜")
            return -1
    
    def close(self) -> None:
        """å…³é—­è¿æ¥"""
        if self.client:
            self.client.close()
            self.client = None
            logger.info("Redisè¿æ¥å·²å…³é—­")


class SemanticCache:
    """
    è¯­ä¹‰ç¼“å­˜ï¼šåŸºäºå‘é‡ç›¸ä¼¼åº¦çš„æ™ºèƒ½ç¼“å­˜
    ç›¸ä¼¼é—®é¢˜å¯ä»¥å¤ç”¨å·²ç¼“å­˜çš„ç­”æ¡ˆï¼Œé¿å…é‡å¤æ£€ç´¢å’Œç”Ÿæˆ
    """
    
    def __init__(
        self,
        cache: RedisCache,
        embedder: Optional[Any] = None,
        similarity_threshold: float = 0.92,
        max_cache_size: int = 10000,
        ttl: int = 7200
    ) -> None:
        """
        åˆå§‹åŒ–è¯­ä¹‰ç¼“å­˜
        
        Args:
            cache: Redisç¼“å­˜å®ä¾‹
            embedder: å‘é‡åŒ–å™¨
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0.92è¡¨ç¤º92%ç›¸ä¼¼å³å‘½ä¸­ï¼‰
            max_cache_size: æœ€å¤§ç¼“å­˜æ¡ç›®æ•°
            ttl: ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰
        """
        self.cache = cache
        self.embedder = embedder
        self.similarity_threshold = similarity_threshold
        self.max_cache_size = max_cache_size
        self.ttl = ttl
        self.stats = {"hits": 0, "misses": 0, "stores": 0}
        
        # ç¼“å­˜ç´¢å¼•é”®
        self._index_key = "semantic_cache:index"
    
    def _get_embedder(self):
        """å»¶è¿Ÿè·å–embedder"""
        if self.embedder is None:
            from src.embedding.embedder import TextEmbedder
            self.embedder = TextEmbedder()
        return self.embedder
    
    def _compute_similarity(self, vec1: Vector, vec2: Vector) -> float:
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        dot = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        if norm1 == 0 or norm2 == 0:
            return 0.0
        return float(dot / (norm1 * norm2))
    
    def get(self, query: str) -> Optional[Dict[str, Any]]:
        """
        è¯­ä¹‰æŸ¥æ‰¾ï¼šæŸ¥æ‰¾ç›¸ä¼¼é—®é¢˜çš„ç¼“å­˜ç­”æ¡ˆ
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            
        Returns:
            ç¼“å­˜çš„ç­”æ¡ˆå’Œå…ƒæ•°æ®ï¼Œæœªå‘½ä¸­è¿”å›None
        """
        if not self.cache.is_connected:
            return None
        
        try:
            # è®¡ç®—æŸ¥è¯¢å‘é‡
            query_vec = self._get_embedder().encode_single(query)
            
            # è·å–ç¼“å­˜ç´¢å¼•
            index_data = self.cache.client.get(self._index_key)
            if not index_data:
                self.stats["misses"] += 1
                return None
            
            cache_index = pickle.loads(index_data)
            
            # éå†æŸ¥æ‰¾æœ€ç›¸ä¼¼çš„ç¼“å­˜
            best_match = None
            best_similarity = 0.0
            
            for cache_key, cached_vec in cache_index.items():
                similarity = self._compute_similarity(query_vec, cached_vec)
                if similarity > best_similarity and similarity >= self.similarity_threshold:
                    best_similarity = similarity
                    best_match = cache_key
            
            if best_match:
                # è·å–ç¼“å­˜çš„ç­”æ¡ˆ
                cached_data = self.cache.client.get(f"semantic:{best_match}")
                if cached_data:
                    self.stats["hits"] += 1
                    result = pickle.loads(cached_data)
                    result["cache_hit"] = True
                    result["similarity"] = best_similarity
                    logger.info(f"è¯­ä¹‰ç¼“å­˜å‘½ä¸­: similarity={best_similarity:.3f}, query={query[:50]}...")
                    return result
            
            self.stats["misses"] += 1
            return None
            
        except Exception as e:
            logger.warning(f"è¯­ä¹‰ç¼“å­˜æŸ¥æ‰¾å¤±è´¥: {e}")
            self.stats["misses"] += 1
            return None
    
    def set(self, query: str, answer: str, contexts: List[Dict], metadata: Dict = None) -> bool:
        """
        å­˜å‚¨ç­”æ¡ˆåˆ°è¯­ä¹‰ç¼“å­˜
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            answer: ç”Ÿæˆçš„ç­”æ¡ˆ
            contexts: æ£€ç´¢çš„ä¸Šä¸‹æ–‡
            metadata: é¢å¤–å…ƒæ•°æ®
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        if not self.cache.is_connected:
            return False
        
        try:
            # è®¡ç®—æŸ¥è¯¢å‘é‡
            query_vec = self._get_embedder().encode_single(query)
            
            # ç”Ÿæˆç¼“å­˜é”®
            cache_key = hashlib.md5(query.encode()).hexdigest()
            
            # å­˜å‚¨ç­”æ¡ˆæ•°æ®
            cache_data = {
                "query": query,
                "answer": answer,
                "contexts": contexts,
                "metadata": metadata or {},
                "timestamp": time.time()
            }
            self.cache.client.setex(
                f"semantic:{cache_key}",
                self.ttl,
                pickle.dumps(cache_data)
            )
            
            # æ›´æ–°ç´¢å¼•
            index_data = self.cache.client.get(self._index_key)
            if index_data:
                cache_index = pickle.loads(index_data)
            else:
                cache_index = {}
            
            # é™åˆ¶ç¼“å­˜å¤§å°
            if len(cache_index) >= self.max_cache_size:
                # åˆ é™¤æœ€æ—§çš„æ¡ç›®
                oldest_key = next(iter(cache_index))
                del cache_index[oldest_key]
                self.cache.client.delete(f"semantic:{oldest_key}")
            
            cache_index[cache_key] = query_vec
            self.cache.client.setex(
                self._index_key,
                self.ttl * 2,  # ç´¢å¼•è¿‡æœŸæ—¶é—´æ›´é•¿
                pickle.dumps(cache_index)
            )
            
            self.stats["stores"] += 1
            logger.debug(f"è¯­ä¹‰ç¼“å­˜å·²å­˜å‚¨: query={query[:50]}...")
            return True
            
        except Exception as e:
            logger.warning(f"è¯­ä¹‰ç¼“å­˜å­˜å‚¨å¤±è´¥: {e}")
            return False
    
    def clear(self) -> int:
        """æ¸…ç©ºè¯­ä¹‰ç¼“å­˜"""
        if not self.cache.is_connected:
            return 0
        
        try:
            # è·å–æ‰€æœ‰è¯­ä¹‰ç¼“å­˜é”®
            keys = self.cache.client.keys("semantic:*")
            count = 0
            if keys:
                count = self.cache.client.delete(*keys)
            
            # æ¸…ç©ºç´¢å¼•
            self.cache.client.delete(self._index_key)
            
            logger.info(f"è¯­ä¹‰ç¼“å­˜å·²æ¸…ç©º: {count} æ¡")
            return count
        except Exception as e:
            logger.warning(f"æ¸…ç©ºè¯­ä¹‰ç¼“å­˜å¤±è´¥: {e}")
            return 0
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        total = self.stats["hits"] + self.stats["misses"]
        hit_rate = self.stats["hits"] / total * 100 if total > 0 else 0
        
        # è·å–å½“å‰ç¼“å­˜å¤§å°
        cache_size = 0
        if self.cache.is_connected:
            try:
                index_data = self.cache.client.get(self._index_key)
                if index_data:
                    cache_index = pickle.loads(index_data)
                    cache_size = len(cache_index)
            except:
                pass
        
        return {
            **self.stats,
            "total_requests": total,
            "hit_rate_%": round(hit_rate, 2),
            "cache_size": cache_size,
            "threshold": self.similarity_threshold
        }


class VectorCacheManager:
    """å‘é‡ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(
        self, 
        cache: RedisCache, 
        embedder: Optional[Any] = None
    ) -> None:
        """
        åˆå§‹åŒ–å‘é‡ç¼“å­˜ç®¡ç†å™¨
        
        Args:
            cache: Redisç¼“å­˜å®ä¾‹
            embedder: å‘é‡åŒ–å™¨å®ä¾‹ï¼ˆå¯é€‰ï¼Œå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        """
        self.cache = cache
        self.embedder = embedder
        self.stats = {"hits": 0, "misses": 0, "prewarmed": 0}
    
    def _get_embedder(self):
        """å»¶è¿Ÿè·å–embedder"""
        if self.embedder is None:
            from src.embedding.embedder import TextEmbedder
            self.embedder = TextEmbedder()
        return self.embedder
    
    def get_or_compute_vector(self, text: str) -> Vector:
        """
        è·å–å‘é‡ï¼Œä¼˜å…ˆä»ç¼“å­˜ï¼Œå¦åˆ™è®¡ç®—å¹¶ç¼“å­˜
        
        Args:
            text: æ–‡æœ¬
            
        Returns:
            å‘é‡
        """
        # å°è¯•ç¼“å­˜
        cached = self.cache.get_vector_cache(text)
        if cached is not None:
            self.stats["hits"] += 1
            return cached
        
        self.stats["misses"] += 1
        
        # è®¡ç®—å‘é‡
        vector = self._get_embedder().encode_single(text)
        
        # ç¼“å­˜ï¼ˆ24å°æ—¶ï¼‰
        self.cache.set_vector_cache(text, vector, ttl=86400)
        
        return vector
    
    def batch_get_or_compute(self, texts: List[str]) -> Vector:
        """
        æ‰¹é‡è·å–å‘é‡ï¼Œæœ€å¤§åŒ–ç¼“å­˜åˆ©ç”¨
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            å‘é‡çŸ©é˜µ
        """
        vectors: List[tuple] = []
        texts_to_compute: List[str] = []
        indices_to_compute: List[int] = []
        
        # æ£€æŸ¥ç¼“å­˜
        for i, text in enumerate(texts):
            cached = self.cache.get_vector_cache(text)
            if cached is not None:
                vectors.append((i, cached))
                self.stats["hits"] += 1
            else:
                texts_to_compute.append(text)
                indices_to_compute.append(i)
                self.stats["misses"] += 1
        
        # æ‰¹é‡è®¡ç®—æœªç¼“å­˜çš„
        if texts_to_compute:
            computed = self._get_embedder().encode_batch(texts_to_compute)
            
            for idx, (orig_idx, text) in enumerate(zip(indices_to_compute, texts_to_compute)):
                vec = computed[idx]
                self.cache.set_vector_cache(text, vec, ttl=86400)
                vectors.append((orig_idx, vec))
        
        # æŒ‰åŸå§‹é¡ºåºæ’åº
        vectors.sort(key=lambda x: x[0])
        return np.array([v[1] for v in vectors])
    
    def prewarm_cache(
        self, 
        texts: List[str], 
        batch_size: int = 100
    ) -> int:
        """
        é¢„çƒ­ç¼“å­˜
        
        Args:
            texts: è¦é¢„çƒ­çš„æ–‡æœ¬åˆ—è¡¨
            batch_size: æ‰¹æ¬¡å¤§å°
            
        Returns:
            é¢„çƒ­çš„æ–‡æœ¬æ•°é‡
        """
        logger.info(f"ğŸ”¥ é¢„çƒ­ç¼“å­˜: {len(texts)} æ¡æ–‡æœ¬")
        
        embedder = self._get_embedder()
        
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i+batch_size]
            vectors = embedder.encode_batch(batch)
            
            for text, vec in zip(batch, vectors):
                self.cache.set_vector_cache(text, vec, ttl=86400 * 7)
                self.stats["prewarmed"] += 1
            
            logger.info(f"   é¢„çƒ­è¿›åº¦: {min(i+batch_size, len(texts))}/{len(texts)}")
        
        logger.info(f"âœ… é¢„çƒ­å®Œæˆ: {self.stats['prewarmed']} æ¡")
        return self.stats["prewarmed"]
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        total = self.stats["hits"] + self.stats["misses"]
        hit_rate = self.stats["hits"] / total * 100 if total > 0 else 0
        return {
            **self.stats,
            "total_requests": total,
            "hit_rate_%": round(hit_rate, 2)
        }


def main() -> None:
    """æµ‹è¯•Redisç¼“å­˜"""
    logger.info("=" * 50)
    logger.info("æµ‹è¯•Redisç¼“å­˜")
    logger.info("=" * 50)
    
    cache = RedisCache()
    
    if not cache.is_connected:
        logger.error("Redisæœªè¿æ¥ï¼Œè¯·å¯åŠ¨RedisæœåŠ¡")
        return
    
    # æµ‹è¯•æŸ¥è¯¢ç¼“å­˜
    test_query = "diabetes symptoms"
    test_results = [{"id": "1", "text": "test", "score": 0.9}]
    
    cache.set_query_cache(test_query, test_results)
    cached = cache.get_query_cache(test_query)
    
    assert cached == test_results, "æŸ¥è¯¢ç¼“å­˜æµ‹è¯•å¤±è´¥"
    logger.info("âœ… æŸ¥è¯¢ç¼“å­˜æµ‹è¯•é€šè¿‡")
    
    # æµ‹è¯•å‘é‡ç¼“å­˜
    test_vector = np.random.rand(512).astype(np.float32)
    cache.set_vector_cache("test text", test_vector)
    cached_vec = cache.get_vector_cache("test text")
    
    assert np.allclose(cached_vec, test_vector), "å‘é‡ç¼“å­˜æµ‹è¯•å¤±è´¥"
    logger.info("âœ… å‘é‡ç¼“å­˜æµ‹è¯•é€šè¿‡")
    
    # æ‰“å°ç»Ÿè®¡
    stats = cache.get_stats()
    logger.info(f"ç¼“å­˜ç»Ÿè®¡: {stats}")
    
    cache.close()


if __name__ == "__main__":
    main()
