#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Redisç¼“å­˜å±‚å®ç°
æå‡æŸ¥è¯¢å“åº”é€Ÿåº¦ï¼Œå‡å°‘é‡å¤è®¡ç®—
"""

import redis
import json
import hashlib
import numpy as np
from typing import Optional, List, Dict, Any
import pickle
from config.config import *
from src.utils.logger import setup_logger

logger = setup_logger("redis_cache", LOGS_DIR / "redis_cache.log")


class RedisCache:
    """Redisç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(
        self,
        host: str = "localhost",
        port: int = 6379,
        db: int = 0,
        password: Optional[str] = None,
        ttl: int = 3600  # é»˜è®¤1å°æ—¶è¿‡æœŸ
    ):
        """
        åˆå§‹åŒ–Redisç¼“å­˜
        
        Args:
            host: Redisä¸»æœº
            port: Redisç«¯å£
            db: æ•°æ®åº“ç¼–å·
            password: å¯†ç 
            ttl: ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰
        """
        try:
            self.client = redis.Redis(
                host=host,
                port=port,
                db=db,
                password=password,
                decode_responses=False  # æ”¯æŒäºŒè¿›åˆ¶æ•°æ®
            )
            self.client.ping()
            self.ttl = ttl
            logger.info(f"âœ… Redisè¿æ¥æˆåŠŸ: {host}:{port}")
        except redis.ConnectionError as e:
            logger.warning(f"âš ï¸ Redisè¿æ¥å¤±è´¥: {e}")
            self.client = None
    
    def _generate_key(self, prefix: str, data: Any) -> str:
        """
        ç”Ÿæˆç¼“å­˜é”®
        
        Args:
            prefix: é”®å‰ç¼€
            data: æ•°æ®ï¼ˆç”¨äºç”Ÿæˆhashï¼‰
            
        Returns:
            ç¼“å­˜é”®
        """
        if isinstance(data, str):
            hash_str = data
        elif isinstance(data, (list, dict)):
            hash_str = json.dumps(data, sort_keys=True)
        elif isinstance(data, np.ndarray):
            hash_str = data.tobytes().hex()[:32]  # ä½¿ç”¨å‰32å­—ç¬¦
        else:
            hash_str = str(data)
        
        hash_value = hashlib.md5(hash_str.encode()).hexdigest()
        return f"{prefix}:{hash_value}"
    
    def get_query_cache(self, query: str) -> Optional[List[Dict]]:
        """
        è·å–æŸ¥è¯¢ç»“æœç¼“å­˜
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            
        Returns:
            ç¼“å­˜çš„ç»“æœï¼Œå¦‚æœä¸å­˜åœ¨è¿”å›None
        """
        if not self.client:
            return None
        
        key = self._generate_key("query", query)
        
        try:
            cached = self.client.get(key)
            if cached:
                logger.info(f"âœ… ç¼“å­˜å‘½ä¸­: query={query[:50]}...")
                return pickle.loads(cached)
            return None
        except Exception as e:
            logger.error(f"âŒ è¯»å–ç¼“å­˜å¤±è´¥: {e}")
            return None
    
    def set_query_cache(self, query: str, results: List[Dict], ttl: Optional[int] = None) -> bool:
        """
        è®¾ç½®æŸ¥è¯¢ç»“æœç¼“å­˜
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            results: æŸ¥è¯¢ç»“æœ
            ttl: è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰ï¼Œå¦‚æœä¸ºNoneä½¿ç”¨é»˜è®¤å€¼
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        if not self.client:
            return False
        
        key = self._generate_key("query", query)
        ttl = ttl or self.ttl
        
        try:
            self.client.setex(
                key,
                ttl,
                pickle.dumps(results)
            )
            logger.info(f"âœ… ç¼“å­˜å·²è®¾ç½®: query={query[:50]}..., ttl={ttl}s")
            return True
        except Exception as e:
            logger.error(f"âŒ è®¾ç½®ç¼“å­˜å¤±è´¥: {e}")
            return False
    
    def get_vector_cache(self, text: str) -> Optional[np.ndarray]:
        """
        è·å–æ–‡æœ¬å‘é‡ç¼“å­˜
        
        Args:
            text: æ–‡æœ¬
            
        Returns:
            å‘é‡ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å›None
        """
        if not self.client:
            return None
        
        key = self._generate_key("vector", text)
        
        try:
            cached = self.client.get(key)
            if cached:
                logger.debug(f"âœ… å‘é‡ç¼“å­˜å‘½ä¸­: text={text[:50]}...")
                return pickle.loads(cached)
            return None
        except Exception as e:
            logger.error(f"âŒ è¯»å–å‘é‡ç¼“å­˜å¤±è´¥: {e}")
            return None
    
    def set_vector_cache(self, text: str, vector: np.ndarray, ttl: Optional[int] = None) -> bool:
        """
        è®¾ç½®æ–‡æœ¬å‘é‡ç¼“å­˜
        
        Args:
            text: æ–‡æœ¬
            vector: å‘é‡
            ttl: è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        if not self.client:
            return False
        
        key = self._generate_key("vector", text)
        ttl = ttl or self.ttl
        
        try:
            self.client.setex(
                key,
                ttl,
                pickle.dumps(vector)
            )
            logger.debug(f"âœ… å‘é‡ç¼“å­˜å·²è®¾ç½®: text={text[:50]}...")
            return True
        except Exception as e:
            logger.error(f"âŒ è®¾ç½®å‘é‡ç¼“å­˜å¤±è´¥: {e}")
            return False
    
    def get_chunks_cache(self, chunk_ids: List[str]) -> Optional[List[Dict]]:
        """
        æ‰¹é‡è·å–chunksç¼“å­˜
        
        Args:
            chunk_ids: chunk IDåˆ—è¡¨
            
        Returns:
            chunksåˆ—è¡¨ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å›None
        """
        if not self.client:
            return None
        
        try:
            keys = [f"chunk:{chunk_id}" for chunk_id in chunk_ids]
            cached = self.client.mget(keys)
            
            results = []
            for item in cached:
                if item:
                    results.append(pickle.loads(item))
                else:
                    return None  # å¦‚æœæœ‰ä»»ä½•ä¸€ä¸ªç¼ºå¤±ï¼Œè¿”å›None
            
            if results:
                logger.info(f"âœ… æ‰¹é‡ç¼“å­˜å‘½ä¸­: {len(results)} chunks")
                return results
            return None
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡è¯»å–ç¼“å­˜å¤±è´¥: {e}")
            return None
    
    def set_chunks_cache(self, chunks: List[Dict], ttl: Optional[int] = None) -> bool:
        """
        æ‰¹é‡è®¾ç½®chunksç¼“å­˜
        
        Args:
            chunks: chunksåˆ—è¡¨ï¼ˆå¿…é¡»åŒ…å«idå­—æ®µï¼‰
            ttl: è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        if not self.client:
            return False
        
        ttl = ttl or self.ttl
        
        try:
            pipe = self.client.pipeline()
            for chunk in chunks:
                key = f"chunk:{chunk['id']}"
                pipe.setex(key, ttl, pickle.dumps(chunk))
            pipe.execute()
            
            logger.info(f"âœ… æ‰¹é‡ç¼“å­˜å·²è®¾ç½®: {len(chunks)} chunks")
            return True
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡è®¾ç½®ç¼“å­˜å¤±è´¥: {e}")
            return False
    
    def get_stats(self) -> Dict[str, Any]:
        """
        è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        if not self.client:
            return {'status': 'disconnected'}
        
        try:
            info = self.client.info()
            
            # ç»Ÿè®¡å„ç±»å‹é”®çš„æ•°é‡
            query_keys = len(self.client.keys("query:*"))
            vector_keys = len(self.client.keys("vector:*"))
            chunk_keys = len(self.client.keys("chunk:*"))
            
            stats = {
                'status': 'connected',
                'total_keys': self.client.dbsize(),
                'query_cache_keys': query_keys,
                'vector_cache_keys': vector_keys,
                'chunk_cache_keys': chunk_keys,
                'used_memory_mb': info['used_memory'] / (1024**2),
                'used_memory_human': info['used_memory_human'],
                'total_commands': info['total_commands_processed'],
                'hit_rate': info.get('keyspace_hits', 0) / max(info.get('keyspace_hits', 0) + info.get('keyspace_misses', 1), 1) * 100
            }
            
            return stats
        except Exception as e:
            logger.error(f"âŒ è·å–ç»Ÿè®¡å¤±è´¥: {e}")
            return {'status': 'error', 'error': str(e)}
    
    def clear_cache(self, pattern: Optional[str] = None) -> int:
        """
        æ¸…ç©ºç¼“å­˜
        
        Args:
            pattern: é”®æ¨¡å¼ï¼ˆå¦‚"query:*"ï¼‰ï¼Œå¦‚æœä¸ºNoneæ¸…ç©ºæ‰€æœ‰
            
        Returns:
            åˆ é™¤çš„é”®æ•°é‡
        """
        if not self.client:
            return 0
        
        try:
            if pattern:
                keys = self.client.keys(pattern)
                if keys:
                    count = self.client.delete(*keys)
                    logger.info(f"âœ… æ¸…ç©ºç¼“å­˜: {pattern}, åˆ é™¤ {count} ä¸ªé”®")
                    return count
            else:
                self.client.flushdb()
                logger.info("âœ… æ¸…ç©ºæ‰€æœ‰ç¼“å­˜")
                return -1
            
            return 0
        except Exception as e:
            logger.error(f"âŒ æ¸…ç©ºç¼“å­˜å¤±è´¥: {e}")
            return 0
    
    def close(self):
        """å…³é—­è¿æ¥"""
        if self.client:
            self.client.close()
            logger.info("âœ… Redisè¿æ¥å·²å…³é—­")


class CachedRAGSystem:
    """å¸¦ç¼“å­˜çš„RAGç³»ç»Ÿå°è£…"""
    
    def __init__(self, rag_system, cache: RedisCache):
        self.rag_system = rag_system
        self.cache = cache
        self.cache_hits = 0
        self.cache_misses = 0
    
    def query(self, query: str, use_cache: bool = True, **kwargs) -> List[Dict]:
        if use_cache:
            cached_result = self.cache.get_query_cache(query)
            if cached_result is not None:
                self.cache_hits += 1
                logger.info(f"ğŸ¯ ç¼“å­˜å‘½ä¸­ç‡: {self.get_hit_rate():.1f}%")
                return cached_result
            self.cache_misses += 1
        
        results = self.rag_system.retrieve(query, **kwargs)
        
        if use_cache and results:
            self.cache.set_query_cache(query, results)
        
        return results
    
    def get_hit_rate(self) -> float:
        total = self.cache_hits + self.cache_misses
        return (self.cache_hits / total) * 100 if total > 0 else 0.0


class VectorCacheManager:
    """
    æ–¹æ¡ˆB: å¢å¼ºå‘é‡ç¼“å­˜ç®¡ç†å™¨
    æ”¯æŒæ‰¹é‡å‘é‡ç¼“å­˜ã€é¢„çƒ­ã€LRUæ·˜æ±°
    """
    
    def __init__(self, cache: RedisCache, embedder=None):
        self.cache = cache
        self.embedder = embedder
        self.stats = {"hits": 0, "misses": 0, "prewarmed": 0}
    
    def get_or_compute_vector(self, text: str) -> np.ndarray:
        """è·å–å‘é‡ï¼Œä¼˜å…ˆä»ç¼“å­˜ï¼Œå¦åˆ™è®¡ç®—å¹¶ç¼“å­˜"""
        # å°è¯•ç¼“å­˜
        cached = self.cache.get_vector_cache(text)
        if cached is not None:
            self.stats["hits"] += 1
            return cached
        
        self.stats["misses"] += 1
        
        # è®¡ç®—å‘é‡
        if self.embedder is None:
            from src.embedding.embedder import TextEmbedder
            self.embedder = TextEmbedder()
        
        vector = self.embedder.encode_single(text)
        
        # ç¼“å­˜ï¼ˆ24å°æ—¶ï¼‰
        self.cache.set_vector_cache(text, vector, ttl=86400)
        
        return vector
    
    def batch_get_or_compute(self, texts: List[str]) -> np.ndarray:
        """æ‰¹é‡è·å–å‘é‡ï¼Œæœ€å¤§åŒ–ç¼“å­˜åˆ©ç”¨"""
        vectors = []
        texts_to_compute = []
        indices_to_compute = []
        
        # æ£€æŸ¥ç¼“å­˜
        for i, text in enumerate(texts):
            cached = self.cache.get_vector_cache(text)
            if cached is not None:
                vectors.append((i, cached))
                self.stats["hits"] += 1
            else:
                texts_to_compute.append(text)
                indices_to_compute.append(i)
                self.stats["misses"] += 1
        
        # æ‰¹é‡è®¡ç®—æœªç¼“å­˜çš„
        if texts_to_compute:
            if self.embedder is None:
                from src.embedding.embedder import TextEmbedder
                self.embedder = TextEmbedder()
            
            computed = self.embedder.encode_batch(texts_to_compute)
            
            # ç¼“å­˜å¹¶æ·»åŠ ç»“æœ
            for idx, (orig_idx, text) in enumerate(zip(indices_to_compute, texts_to_compute)):
                vec = computed[idx]
                self.cache.set_vector_cache(text, vec, ttl=86400)
                vectors.append((orig_idx, vec))
        
        # æŒ‰åŸå§‹é¡ºåºæ’åº
        vectors.sort(key=lambda x: x[0])
        return np.array([v[1] for v in vectors])
    
    def prewarm_cache(self, texts: List[str], batch_size: int = 100):
        """é¢„çƒ­ç¼“å­˜ - æ‰¹é‡é¢„è®¡ç®—å¸¸ç”¨æŸ¥è¯¢çš„å‘é‡"""
        logger.info(f"ğŸ”¥ é¢„çƒ­ç¼“å­˜: {len(texts)} æ¡æ–‡æœ¬")
        
        if self.embedder is None:
            from src.embedding.embedder import TextEmbedder
            self.embedder = TextEmbedder()
        
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i+batch_size]
            vectors = self.embedder.encode_batch(batch)
            
            for text, vec in zip(batch, vectors):
                self.cache.set_vector_cache(text, vec, ttl=86400 * 7)  # 7å¤©
                self.stats["prewarmed"] += 1
            
            logger.info(f"   é¢„çƒ­è¿›åº¦: {min(i+batch_size, len(texts))}/{len(texts)}")
        
        logger.info(f"âœ… é¢„çƒ­å®Œæˆ: {self.stats['prewarmed']} æ¡")
    
    def get_stats(self) -> Dict:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        total = self.stats["hits"] + self.stats["misses"]
        hit_rate = self.stats["hits"] / total * 100 if total > 0 else 0
        return {
            **self.stats,
            "total_requests": total,
            "hit_rate_%": round(hit_rate, 2)
        }


def demo_cache_performance():
    """æ¼”ç¤ºç¼“å­˜æ€§èƒ½æå‡"""
    logger.info("=" * 70)
    logger.info("ğŸš€ Redisç¼“å­˜æ€§èƒ½æ¼”ç¤º")
    logger.info("=" * 70)
    
    # åˆå§‹åŒ–ç¼“å­˜
    cache = RedisCache()
    
    if not cache.client:
        print("âŒ Redisæœªè¿è¡Œï¼Œè¯·å¯åŠ¨RedisæœåŠ¡")
        print("Dockeræ–¹å¼: docker compose -f docker/docker-compose.yml up -d redis")
        return
    
    # æµ‹è¯•æ•°æ®
    test_queries = [
        "ä»€ä¹ˆæ˜¯ç³–å°¿ç—…çš„ç—‡çŠ¶ï¼Ÿ",
        "é«˜è¡€å‹çš„æ²»ç–—æ–¹æ³•",
        "ç™Œç—‡çš„é¢„é˜²æªæ–½",
        "å¿ƒè¡€ç®¡ç–¾ç—…çš„é£é™©å› ç´ ",
        "ä»€ä¹ˆæ˜¯ç³–å°¿ç—…çš„ç—‡çŠ¶ï¼Ÿ",  # é‡å¤æŸ¥è¯¢
    ]
    
    # æ¨¡æ‹ŸæŸ¥è¯¢ç»“æœ
    mock_results = [
        {"id": "1", "content": "ç³–å°¿ç—…ç—‡çŠ¶åŒ…æ‹¬...", "score": 0.95},
        {"id": "2", "content": "å¤šé¥®ã€å¤šå°¿...", "score": 0.90}
    ]
    
    print("\nğŸ“ æµ‹è¯•æŸ¥è¯¢ç¼“å­˜")
    print("-" * 70)
    
    for i, query in enumerate(test_queries, 1):
        print(f"\næŸ¥è¯¢ {i}: {query}")
        
        # æ£€æŸ¥ç¼“å­˜
        start = time.time()
        cached = cache.get_query_cache(query)
        
        if cached:
            elapsed = time.time() - start
            print(f"  âœ… ç¼“å­˜å‘½ä¸­ ({elapsed*1000:.2f}ms)")
        else:
            # æ¨¡æ‹Ÿå®é™…æŸ¥è¯¢ï¼ˆè¾ƒæ…¢ï¼‰
            time.sleep(0.5)  # æ¨¡æ‹ŸæŸ¥è¯¢å»¶è¿Ÿ
            cache.set_query_cache(query, mock_results)
            elapsed = time.time() - start
            print(f"  âŒ ç¼“å­˜æœªå‘½ä¸­ï¼Œå·²ç¼“å­˜ ({elapsed*1000:.2f}ms)")
    
    # ç»Ÿè®¡ä¿¡æ¯
    print("\n" + "=" * 70)
    print("ğŸ“Š ç¼“å­˜ç»Ÿè®¡")
    print("=" * 70)
    
    stats = cache.get_stats()
    for key, value in stats.items():
        print(f"  {key}: {value}")
    
    cache.close()


if __name__ == "__main__":
    import time
    demo_cache_performance()
