#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
MinIOå¯¹è±¡å­˜å‚¨æ¨¡å—
ç”¨äºå­˜å‚¨æ¨¡å‹ã€å‘é‡æ•°æ®åº“å¤‡ä»½ç­‰å¤§æ–‡ä»¶
"""

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent.parent))

from minio import Minio
from minio.error import S3Error
from typing import Optional
import os
import pandas as pd
from config.config import *
from src.utils.logger import setup_logger

logger = setup_logger("minio_storage", LOGS_DIR / "minio_storage.log")


class MinIOStorage:
    """MinIOå¯¹è±¡å­˜å‚¨ç®¡ç†å™¨"""
    
    def __init__(
        self,
        endpoint: str = "localhost:9000",
        access_key: str = "minioadmin",
        secret_key: str = "minioadmin",
        secure: bool = False
    ):
        """
        åˆå§‹åŒ–MinIOå®¢æˆ·ç«¯
        
        Args:
            endpoint: MinIOæœåŠ¡åœ°å€
            access_key: è®¿é—®å¯†é’¥
            secret_key: å¯†é’¥
            secure: æ˜¯å¦ä½¿ç”¨HTTPS
        """
        try:
            self.client = Minio(
                endpoint,
                access_key=access_key,
                secret_key=secret_key,
                secure=secure
            )
            logger.info(f"âœ… è¿æ¥MinIOæˆåŠŸ: {endpoint}")
        except Exception as e:
            logger.error(f"âŒ è¿æ¥MinIOå¤±è´¥: {e}")
            raise
    
    def create_bucket(self, bucket_name: str):
        """
        åˆ›å»ºå­˜å‚¨æ¡¶
        
        Args:
            bucket_name: æ¡¶åç§°
        """
        try:
            if not self.client.bucket_exists(bucket_name):
                self.client.make_bucket(bucket_name)
                logger.info(f"âœ… åˆ›å»ºbucket: {bucket_name}")
            else:
                logger.info(f"â„¹ï¸ Bucketå·²å­˜åœ¨: {bucket_name}")
        except S3Error as e:
            logger.error(f"âŒ åˆ›å»ºbucketå¤±è´¥: {e}")
            raise
    
    def upload_file(
        self,
        bucket_name: str,
        object_name: str,
        file_path: Path,
        content_type: Optional[str] = None
    ) -> bool:
        """
        ä¸Šä¼ æ–‡ä»¶åˆ°MinIO
        
        Args:
            bucket_name: æ¡¶åç§°
            object_name: å¯¹è±¡åç§° (å­˜å‚¨è·¯å¾„)
            file_path: æœ¬åœ°æ–‡ä»¶è·¯å¾„
            content_type: æ–‡ä»¶ç±»å‹
            
        Returns:
            æ˜¯å¦ä¸Šä¼ æˆåŠŸ
        """
        try:
            file_size = file_path.stat().st_size / (1024**2)  # MB
            logger.info(f"å¼€å§‹ä¸Šä¼ : {file_path.name} ({file_size:.2f} MB)")
            
            self.client.fput_object(
                bucket_name,
                object_name,
                str(file_path),
                content_type=content_type
            )
            
            logger.info(f"âœ… ä¸Šä¼ æˆåŠŸ: {bucket_name}/{object_name}")
            return True
            
        except S3Error as e:
            logger.error(f"âŒ ä¸Šä¼ å¤±è´¥: {e}")
            return False
    
    def download_file(
        self,
        bucket_name: str,
        object_name: str,
        file_path: Path
    ) -> bool:
        """
        ä»MinIOä¸‹è½½æ–‡ä»¶
        
        Args:
            bucket_name: æ¡¶åç§°
            object_name: å¯¹è±¡åç§°
            file_path: æœ¬åœ°ä¿å­˜è·¯å¾„
            
        Returns:
            æ˜¯å¦ä¸‹è½½æˆåŠŸ
        """
        try:
            logger.info(f"å¼€å§‹ä¸‹è½½: {bucket_name}/{object_name}")
            
            self.client.fget_object(
                bucket_name,
                object_name,
                str(file_path)
            )
            
            file_size = file_path.stat().st_size / (1024**2)
            logger.info(f"âœ… ä¸‹è½½æˆåŠŸ: {file_path} ({file_size:.2f} MB)")
            return True
            
        except S3Error as e:
            logger.error(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
            return False
    
    def list_objects(self, bucket_name: str, prefix: str = "") -> list:
        """
        åˆ—å‡ºbucketä¸­çš„å¯¹è±¡
        
        Args:
            bucket_name: æ¡¶åç§°
            prefix: å¯¹è±¡å‰ç¼€è¿‡æ»¤
            
        Returns:
            å¯¹è±¡åˆ—è¡¨
        """
        try:
            objects = self.client.list_objects(
                bucket_name,
                prefix=prefix,
                recursive=True
            )
            
            result = []
            for obj in objects:
                result.append({
                    'name': obj.object_name,
                    'size_mb': obj.size / (1024**2),
                    'last_modified': obj.last_modified
                })
            
            logger.info(f"åˆ—å‡ºå¯¹è±¡: {bucket_name}/{prefix}* - å…± {len(result)} ä¸ª")
            return result
            
        except S3Error as e:
            logger.error(f"âŒ åˆ—å‡ºå¯¹è±¡å¤±è´¥: {e}")
            return []
    
    def delete_object(self, bucket_name: str, object_name: str) -> bool:
        """
        åˆ é™¤å¯¹è±¡
        
        Args:
            bucket_name: æ¡¶åç§°
            object_name: å¯¹è±¡åç§°
            
        Returns:
            æ˜¯å¦åˆ é™¤æˆåŠŸ
        """
        try:
            self.client.remove_object(bucket_name, object_name)
            logger.info(f"âœ… åˆ é™¤æˆåŠŸ: {bucket_name}/{object_name}")
            return True
        except S3Error as e:
            logger.error(f"âŒ åˆ é™¤å¤±è´¥: {e}")
            return False
    
    def backup_models(self, bucket_name: str = "rag-models"):
        """
        å¤‡ä»½æ¨¡å‹æ–‡ä»¶åˆ°MinIO
        
        Args:
            bucket_name: æ¡¶åç§°
        """
        self.create_bucket(bucket_name)
        
        # å¤‡ä»½åµŒå…¥æ¨¡å‹
        models_dir = MODELS_DIR / "embedding"
        if models_dir.exists():
            for model_dir in models_dir.iterdir():
                if model_dir.is_dir():
                    logger.info(f"å¤‡ä»½æ¨¡å‹: {model_dir.name}")
                    # è¿™é‡Œå¯ä»¥æ‰“åŒ…åä¸Šä¼ 
                    # å®é™…å®ç°éœ€è¦taræ‰“åŒ…
        
        logger.info("æ¨¡å‹å¤‡ä»½å®Œæˆ")
    
    def backup_database(self, backup_path: Path, bucket_name: str = "rag-backups"):
        """
        å¤‡ä»½å‘é‡æ•°æ®åº“åˆ°MinIO
        
        Args:
            backup_path: å¤‡ä»½æ–‡ä»¶è·¯å¾„
            bucket_name: æ¡¶åç§°
        """
        self.create_bucket(bucket_name)
        
        object_name = f"milvus_backup_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.tar.gz"
        
        self.upload_file(
            bucket_name,
            object_name,
            backup_path,
            content_type="application/gzip"
        )


if __name__ == "__main__":
    # æµ‹è¯•MinIOå­˜å‚¨
    print("=" * 70)
    print("â˜ï¸ MinIOå¯¹è±¡å­˜å‚¨æµ‹è¯•")
    print("=" * 70)
    
    print("\nâš ï¸ è¯·å…ˆå¯åŠ¨MinIOæœåŠ¡:")
    print("docker run -d -p 9000:9000 -p 9001:9001 \\")
    print("  -e MINIO_ROOT_USER=minioadmin \\")
    print("  -e MINIO_ROOT_PASSWORD=minioadmin \\")
    print("  minio/minio server /data --console-address ':9001'")
    
    try:
        storage = MinIOStorage()
        
        # åˆ›å»ºæµ‹è¯•bucket
        storage.create_bucket("rag-test")
        
        # æµ‹è¯•ä¸Šä¼ 
        test_file = PROCESSED_DATA_DIR / "medical_chunks.json"
        if test_file.exists():
            print(f"\nğŸ“¤ æµ‹è¯•ä¸Šä¼ : {test_file.name}")
            storage.upload_file(
                "rag-test",
                "test/medical_chunks.json",
                test_file,
                "application/json"
            )
            
            # åˆ—å‡ºå¯¹è±¡
            print("\nğŸ“‹ åˆ—å‡ºå¯¹è±¡:")
            objects = storage.list_objects("rag-test", "test/")
            for obj in objects:
                print(f"  {obj['name']} - {obj['size_mb']:.2f} MB")
        
        print("\nâœ… MinIOå­˜å‚¨æµ‹è¯•å®Œæˆ!")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        print("æç¤º: è¯·ç¡®ä¿MinIOæœåŠ¡å·²å¯åŠ¨")
