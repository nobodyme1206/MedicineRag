# -*- coding: utf-8 -*-
"""
Kafka + Airflow é›†æˆæ•ˆæœè¯„ä¼°
å¯¹æ¯”å¼•å…¥å‰åçš„æ€§èƒ½å·®å¼‚
"""

import json
import time
import threading
from pathlib import Path
from typing import Dict, List
from datetime import datetime
import sys

sys.path.append(str(Path(__file__).parent.parent.parent))

from config.config import LOGS_DIR, RESULTS_DIR
from src.utils.logger import setup_logger

logger = setup_logger("kafka_airflow_eval", LOGS_DIR / "kafka_airflow_eval.log")


class KafkaAirflowEvaluator:
    """Kafka + Airflow é›†æˆæ•ˆæœè¯„ä¼°å™¨"""
    
    def __init__(self):
        self.results = {
            "timestamp": datetime.now().isoformat(),
            "before_integration": {},
            "after_integration": {},
            "comparison": {}
        }
    
    def evaluate_throughput_before(self, sample_size: int = 1000) -> Dict:
        """
        è¯„ä¼°é›†æˆå‰çš„ååé‡ï¼ˆä¸²è¡Œå¤„ç†ï¼‰
        
        æ¨¡æ‹ŸåŸæœ‰æµç¨‹ï¼šçˆ¬å– â†’ å¤„ç† â†’ å‘é‡åŒ–ï¼ˆä¸²è¡Œï¼‰
        """
        logger.info("=" * 60)
        logger.info("ğŸ“Š è¯„ä¼°é›†æˆå‰æ€§èƒ½ï¼ˆä¸²è¡Œæ¨¡å¼ï¼‰")
        logger.info("=" * 60)
        
        # æ¨¡æ‹Ÿæ•°æ®
        test_articles = self._generate_test_data(sample_size)
        
        # ä¸²è¡Œå¤„ç†
        start_time = time.time()
        
        # é˜¶æ®µ1: æ¨¡æ‹Ÿçˆ¬å–ï¼ˆIOå¯†é›†ï¼‰
        crawl_start = time.time()
        for article in test_articles:
            time.sleep(0.001)  # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
        crawl_time = time.time() - crawl_start
        
        # é˜¶æ®µ2: æ¨¡æ‹Ÿå¤„ç†ï¼ˆCPUå¯†é›†ï¼‰
        process_start = time.time()
        processed = []
        for article in test_articles:
            # æ¨¡æ‹Ÿæ–‡æœ¬å¤„ç†
            chunks = self._simulate_chunking(article.get("text", ""))
            processed.extend(chunks)
        process_time = time.time() - process_start
        
        # é˜¶æ®µ3: æ¨¡æ‹Ÿå‘é‡åŒ–ï¼ˆGPUå¯†é›†ï¼‰
        embed_start = time.time()
        for chunk in processed:
            time.sleep(0.0005)  # æ¨¡æ‹Ÿå‘é‡åŒ–
        embed_time = time.time() - embed_start
        
        total_time = time.time() - start_time
        
        results = {
            "mode": "serial",
            "sample_size": sample_size,
            "total_time_seconds": round(total_time, 2),
            "crawl_time": round(crawl_time, 2),
            "process_time": round(process_time, 2),
            "embed_time": round(embed_time, 2),
            "throughput_articles_per_sec": round(sample_size / total_time, 2),
            "chunks_generated": len(processed),
            "bottleneck": "ä¸²è¡Œç­‰å¾…ï¼Œå„é˜¶æ®µæ— æ³•å¹¶è¡Œ"
        }
        
        self.results["before_integration"] = results
        logger.info(f"ä¸²è¡Œæ¨¡å¼ç»“æœ: {json.dumps(results, indent=2, ensure_ascii=False)}")
        
        return results
    
    def evaluate_throughput_after(self, sample_size: int = 1000) -> Dict:
        """
        è¯„ä¼°é›†æˆåçš„ååé‡ï¼ˆKafkaå¼‚æ­¥è§£è€¦ï¼‰
        
        æ–°æµç¨‹ï¼šçˆ¬å– â†’ Kafka â†’ å¤„ç†æ¶ˆè´¹è€… â†’ Kafka â†’ å‘é‡åŒ–æ¶ˆè´¹è€…ï¼ˆå¹¶è¡Œï¼‰
        """
        logger.info("=" * 60)
        logger.info("ğŸ“Š è¯„ä¼°é›†æˆåæ€§èƒ½ï¼ˆKafkaå¼‚æ­¥æ¨¡å¼ï¼‰")
        logger.info("=" * 60)
        
        test_articles = self._generate_test_data(sample_size)
        
        # ä½¿ç”¨é˜Ÿåˆ—æ¨¡æ‹ŸKafka
        from queue import Queue
        import concurrent.futures
        
        raw_queue = Queue()
        processed_queue = Queue()
        results_list = []
        
        # å¹¶è¡Œå¤„ç†
        start_time = time.time()
        
        def producer():
            """ç”Ÿäº§è€…ï¼šçˆ¬å–å¹¶å‘é€åˆ°Kafka"""
            for article in test_articles:
                time.sleep(0.001)  # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
                raw_queue.put(article)
            raw_queue.put(None)  # ç»“æŸä¿¡å·
        
        def processor():
            """æ¶ˆè´¹è€…1ï¼šå¤„ç†æ•°æ®"""
            while True:
                article = raw_queue.get()
                if article is None:
                    processed_queue.put(None)
                    break
                chunks = self._simulate_chunking(article.get("text", ""))
                for chunk in chunks:
                    processed_queue.put(chunk)
        
        def embedder():
            """æ¶ˆè´¹è€…2ï¼šå‘é‡åŒ–"""
            count = 0
            while True:
                chunk = processed_queue.get()
                if chunk is None:
                    break
                time.sleep(0.0005)  # æ¨¡æ‹Ÿå‘é‡åŒ–
                count += 1
            return count
        
        # å¹¶è¡Œæ‰§è¡Œ
        with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
            producer_future = executor.submit(producer)
            processor_future = executor.submit(processor)
            embedder_future = executor.submit(embedder)
            
            producer_future.result()
            processor_future.result()
            embed_count = embedder_future.result()
        
        total_time = time.time() - start_time
        
        results = {
            "mode": "kafka_async",
            "sample_size": sample_size,
            "total_time_seconds": round(total_time, 2),
            "throughput_articles_per_sec": round(sample_size / total_time, 2),
            "chunks_processed": embed_count,
            "parallelism": "3ä¸ªé˜¶æ®µå¹¶è¡Œæ‰§è¡Œ",
            "benefits": [
                "é‡‡é›†å’Œå¤„ç†è§£è€¦ï¼Œäº’ä¸é˜»å¡",
                "æ¶ˆæ¯æŒä¹…åŒ–ï¼Œæ”¯æŒé‡æ”¾",
                "å¯æ°´å¹³æ‰©å±•æ¶ˆè´¹è€…æ•°é‡"
            ]
        }
        
        self.results["after_integration"] = results
        logger.info(f"Kafkaå¼‚æ­¥æ¨¡å¼ç»“æœ: {json.dumps(results, indent=2, ensure_ascii=False)}")
        
        return results
    
    def evaluate_fault_tolerance(self) -> Dict:
        """è¯„ä¼°å®¹é”™èƒ½åŠ›"""
        logger.info("=" * 60)
        logger.info("ğŸ“Š è¯„ä¼°å®¹é”™èƒ½åŠ›")
        logger.info("=" * 60)
        
        comparison = {
            "before": {
                "failure_recovery": "éœ€è¦ä»å¤´å¼€å§‹",
                "data_persistence": "å†…å­˜ä¸­ï¼Œè¿›ç¨‹å´©æºƒåˆ™ä¸¢å¤±",
                "retry_mechanism": "æ‰‹åŠ¨é‡è¯•",
                "monitoring": "æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶"
            },
            "after": {
                "failure_recovery": "ä»Kafka offsetç»§ç»­æ¶ˆè´¹",
                "data_persistence": "KafkaæŒä¹…åŒ–ï¼Œå¯ä¿ç•™7å¤©",
                "retry_mechanism": "Airflowè‡ªåŠ¨é‡è¯•3æ¬¡",
                "monitoring": "Airflow Web UI + Kafka UI"
            }
        }
        
        return comparison
    
    def evaluate_scalability(self) -> Dict:
        """è¯„ä¼°å¯æ‰©å±•æ€§"""
        logger.info("=" * 60)
        logger.info("ğŸ“Š è¯„ä¼°å¯æ‰©å±•æ€§")
        logger.info("=" * 60)
        
        comparison = {
            "before": {
                "horizontal_scaling": "éœ€è¦ä¿®æ”¹ä»£ç ",
                "max_parallelism": "å—é™äºå•æœºèµ„æº",
                "bottleneck": "æœ€æ…¢çš„é˜¶æ®µå†³å®šæ•´ä½“é€Ÿåº¦"
            },
            "after": {
                "horizontal_scaling": "å¢åŠ æ¶ˆè´¹è€…å®ä¾‹å³å¯",
                "max_parallelism": "Kafkaåˆ†åŒºæ•° Ã— æ¶ˆè´¹è€…æ•°",
                "bottleneck": "å„é˜¶æ®µç‹¬ç«‹æ‰©å±•ï¼Œæ¶ˆé™¤ç“¶é¢ˆ"
            },
            "scaling_example": {
                "scenario": "å¤„ç†é€Ÿåº¦ä¸å¤Ÿ",
                "before_solution": "ä¼˜åŒ–ä»£ç æˆ–å‡çº§ç¡¬ä»¶",
                "after_solution": "å¯åŠ¨æ›´å¤šå¤„ç†æ¶ˆè´¹è€…å®ä¾‹"
            }
        }
        
        return comparison
    
    def compare_results(self) -> Dict:
        """å¯¹æ¯”é›†æˆå‰åçš„ç»“æœ"""
        before = self.results.get("before_integration", {})
        after = self.results.get("after_integration", {})
        
        if not before or not after:
            return {}
        
        speedup = before.get("total_time_seconds", 1) / max(after.get("total_time_seconds", 1), 0.001)
        throughput_improvement = after.get("throughput_articles_per_sec", 0) / max(before.get("throughput_articles_per_sec", 1), 0.001)
        
        comparison = {
            "speedup": round(speedup, 2),
            "throughput_improvement": f"{round(throughput_improvement, 2)}x",
            "time_saved_percent": round((1 - 1/speedup) * 100, 1),
            "key_improvements": [
                f"å¤„ç†é€Ÿåº¦æå‡ {round(throughput_improvement, 1)}x",
                "æ”¯æŒæ–­ç‚¹ç»­ä¼ å’Œæ¶ˆæ¯é‡æ”¾",
                "å¯è§†åŒ–ä»»åŠ¡ç›‘æ§ï¼ˆAirflow UIï¼‰",
                "è‡ªåŠ¨å¤±è´¥é‡è¯•å’Œå‘Šè­¦",
                "æ°´å¹³æ‰©å±•èƒ½åŠ›"
            ]
        }
        
        self.results["comparison"] = comparison
        return comparison
    
    def run_full_evaluation(self, sample_size: int = 1000) -> Dict:
        """è¿è¡Œå®Œæ•´è¯„ä¼°"""
        logger.info("=" * 60)
        logger.info("ğŸš€ Kafka + Airflow é›†æˆæ•ˆæœè¯„ä¼°")
        logger.info("=" * 60)
        
        # 1. è¯„ä¼°é›†æˆå‰
        self.evaluate_throughput_before(sample_size)
        
        # 2. è¯„ä¼°é›†æˆå
        self.evaluate_throughput_after(sample_size)
        
        # 3. å¯¹æ¯”ç»“æœ
        self.compare_results()
        
        # 4. å®¹é”™èƒ½åŠ›
        self.results["fault_tolerance"] = self.evaluate_fault_tolerance()
        
        # 5. å¯æ‰©å±•æ€§
        self.results["scalability"] = self.evaluate_scalability()
        
        # ä¿å­˜ç»“æœ
        output_file = RESULTS_DIR / "kafka_airflow_evaluation.json"
        output_file.parent.mkdir(parents=True, exist_ok=True)
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"\nâœ… è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜: {output_file}")
        
        # æ‰“å°æ‘˜è¦
        self._print_summary()
        
        return self.results
    
    def _generate_test_data(self, size: int) -> List[Dict]:
        """ç”Ÿæˆæµ‹è¯•æ•°æ®"""
        return [
            {
                "pmid": f"test_{i}",
                "title": f"Test Article {i}",
                "text": "This is a test abstract. " * 50  # çº¦500å­—ç¬¦
            }
            for i in range(size)
        ]
    
    def _simulate_chunking(self, text: str) -> List[str]:
        """æ¨¡æ‹Ÿæ–‡æœ¬åˆ‡åˆ†"""
        chunk_size = 512
        chunks = []
        for i in range(0, len(text), chunk_size - 50):
            chunk = text[i:i+chunk_size]
            if len(chunk) > 100:
                chunks.append(chunk)
        return chunks
    
    def _print_summary(self):
        """æ‰“å°è¯„ä¼°æ‘˜è¦"""
        comparison = self.results.get("comparison", {})
        
        print("\n" + "=" * 60)
        print("ğŸ“Š Kafka + Airflow é›†æˆæ•ˆæœè¯„ä¼°æ‘˜è¦")
        print("=" * 60)
        print(f"\nğŸš€ æ€§èƒ½æå‡:")
        print(f"   - å¤„ç†é€Ÿåº¦: {comparison.get('throughput_improvement', 'N/A')}")
        print(f"   - æ—¶é—´èŠ‚çœ: {comparison.get('time_saved_percent', 'N/A')}%")
        print(f"\nâœ¨ å…³é”®æ”¹è¿›:")
        for improvement in comparison.get("key_improvements", []):
            print(f"   - {improvement}")
        print("\n" + "=" * 60)


def main():
    """è¿è¡Œè¯„ä¼°"""
    evaluator = KafkaAirflowEvaluator()
    evaluator.run_full_evaluation(sample_size=1000)


if __name__ == "__main__":
    main()
