#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€è¯„ä¼°æ¨¡å— - å®¢è§‚è¯„ä¼°ç‰ˆæœ¬
æ•´åˆRAGæ•ˆæœè¯„ä¼°å’Œæ•°æ®å¯†é›†å‹æŠ€æœ¯è¯„ä¼°

è¯„ä¼°ç»´åº¦:
1. RAGæ£€ç´¢æ•ˆæœ (Recall@K, Precision@K, MRR, NDCG, F1)
2. æ•°æ®å¯†é›†å‹æŠ€æœ¯æ€§èƒ½ (å­˜å‚¨ã€å¤„ç†ã€ç´¢å¼•)
3. PySparkå¤§æ•°æ®å¤„ç†èƒ½åŠ›
4. ç»¼åˆç³»ç»Ÿæ€§èƒ½

å®¢è§‚æ€§æ”¹è¿›:
- ä½¿ç”¨æ›´å¤§çš„æµ‹è¯•é›†ï¼ˆä»æ•°æ®é›†ä¸­è‡ªåŠ¨ç”Ÿæˆï¼‰
- åŸºäºä¸»é¢˜ç›¸å…³æ€§è®¡ç®—çœŸæ­£çš„Recall/Precision
- æ·»åŠ NDCGè¯„ä¼°æ’åºè´¨é‡
- å¤šç»´åº¦äº¤å‰éªŒè¯
"""

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent.parent))

import time
import json
import random
import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Tuple
from datetime import datetime
from collections import defaultdict
from config.config import *
from src.utils.logger import setup_logger

logger = setup_logger("unified_evaluator", LOGS_DIR / "unified_evaluation.log")


class UnifiedEvaluator:
    """ç»Ÿä¸€è¯„ä¼°å™¨ - å®¢è§‚è¯„ä¼°ç‰ˆæœ¬"""
    
    def __init__(self, rag_system=None, use_parquet: bool = True):
        """
        åˆå§‹åŒ–ç»Ÿä¸€è¯„ä¼°å™¨
        
        Args:
            rag_system: RAGç³»ç»Ÿå®ä¾‹ï¼ˆå¯é€‰ï¼Œå°†è‡ªåŠ¨åˆå§‹åŒ–ï¼‰
            use_parquet: æ˜¯å¦ä½¿ç”¨Parquetæ ¼å¼åŠ è½½æ•°æ®
        """
        self.rag_system = rag_system
        self.use_parquet = use_parquet
        self.results = {
            "timestamp": datetime.now().isoformat(),
            "rag_evaluation": {},
            "data_intensive_evaluation": {},
            "pyspark_evaluation": {},
            "overall_score": 0
        }
        
        # åŠ è½½æ•°æ®é›†ç”¨äºç”Ÿæˆground truth
        self.corpus_df = None
        self.topic_docs = defaultdict(list)  # topic -> [doc_ids]
        self._load_corpus()
        
        # æµ‹è¯•æ•°æ®é›†ï¼ˆè‡ªåŠ¨ç”Ÿæˆ+æ‰‹å·¥æ„é€ ï¼‰
        self.test_queries = self._create_comprehensive_test_queries()
        
        logger.info("=" * 70)
        logger.info("ğŸš€ ç»Ÿä¸€è¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆï¼ˆå®¢è§‚è¯„ä¼°ç‰ˆæœ¬ï¼‰")
        logger.info(f"   æµ‹è¯•æŸ¥è¯¢æ•°: {len(self.test_queries)}")
        logger.info(f"   æ•°æ®æ ¼å¼: {'Parquet' if use_parquet else 'JSON'}")
        logger.info("=" * 70)
    
    def _load_corpus(self):
        """åŠ è½½è¯­æ–™åº“å…ƒæ•°æ®ï¼ˆä¼˜åŒ–ç‰ˆï¼šä¸åŠ è½½å®Œæ•´æ•°æ®ï¼‰"""
        parquet_path = PROCESSED_DATA_DIR / "parquet" / "medical_chunks.parquet"
        
        try:
            if parquet_path.exists():
                # åªè¯»å–topicåˆ—ç”¨äºç»Ÿè®¡ï¼Œä¸åŠ è½½å®Œæ•´æ•°æ®
                logger.info("åŠ è½½è¯­æ–™åº“å…ƒæ•°æ®...")
                df_meta = pd.read_parquet(parquet_path, columns=['topic'])
                
                # ä½¿ç”¨å‘é‡åŒ–æ“ä½œæ„å»ºä¸»é¢˜ç´¢å¼•
                topic_counts = df_meta['topic'].value_counts().to_dict()
                for topic, count in topic_counts.items():
                    self.topic_docs[topic] = list(range(count))  # ç®€åŒ–ç´¢å¼•
                
                self.corpus_df = None  # ä¸ä¿ç•™å®Œæ•´æ•°æ®
                logger.info(f"è¯­æ–™åº“å…ƒæ•°æ®åŠ è½½å®Œæˆ: {len(df_meta):,} æ¡")
                logger.info(f"ä¸»é¢˜åˆ†å¸ƒ: {dict(list(topic_counts.items())[:5])}...")
                del df_meta
        except Exception as e:
            logger.warning(f"è¯­æ–™åº“åŠ è½½å¤±è´¥: {e}")
    
    def _create_comprehensive_test_queries(self) -> List[Dict]:
        """
        åˆ›å»ºç»¼åˆæµ‹è¯•æŸ¥è¯¢é›†
        
        åŒ…å«:
        1. æ‰‹å·¥æ„é€ çš„é«˜è´¨é‡æŸ¥è¯¢ï¼ˆå¸¦ground truthï¼‰
        2. ä»è¯­æ–™åº“è‡ªåŠ¨ç”Ÿæˆçš„æŸ¥è¯¢ï¼ˆåŸºäºæ ‡é¢˜ï¼‰
        """
        queries = []
        
        # 1. æ‰‹å·¥æ„é€ çš„æŸ¥è¯¢ï¼ˆå¸¦æ˜ç¡®çš„ç›¸å…³ä¸»é¢˜ä½œä¸ºground truthï¼‰
        manual_queries = [
            {"id": 1, "query": "What are the symptoms of type 2 diabetes?", 
             "category": "diabetes", "relevant_topics": ["diabetes"],
             "keywords": ["insulin", "glucose", "symptoms", "diabetes", "blood sugar"]},
            {"id": 2, "query": "How to prevent cardiovascular disease?",
             "category": "cardiovascular", "relevant_topics": ["cardiovascular disease"],
             "keywords": ["heart", "prevention", "cardiovascular", "cardiac", "coronary"]},
            {"id": 3, "query": "What causes high blood pressure hypertension?",
             "category": "hypertension", "relevant_topics": ["hypertension"],
             "keywords": ["blood pressure", "hypertension", "causes", "systolic", "diastolic"]},
            {"id": 4, "query": "Treatment options for cancer patients chemotherapy",
             "category": "cancer", "relevant_topics": ["cancer"],
             "keywords": ["treatment", "therapy", "cancer", "chemotherapy", "oncology"]},
            {"id": 5, "query": "Mental health depression symptoms and treatment",
             "category": "mental_health", "relevant_topics": ["mental health"],
             "keywords": ["depression", "mental", "symptoms", "anxiety", "psychiatric"]},
            {"id": 6, "query": "COVID-19 coronavirus vaccine effectiveness immunity",
             "category": "covid-19", "relevant_topics": ["covid-19"],
             "keywords": ["vaccine", "covid", "coronavirus", "immunity", "mrna"]},
            {"id": 7, "query": "Obesity risk factors BMI prevention weight loss",
             "category": "obesity", "relevant_topics": ["obesity"],
             "keywords": ["obesity", "risk", "prevention", "bmi", "weight"]},
            {"id": 8, "query": "Alzheimer disease dementia early signs memory loss",
             "category": "alzheimer", "relevant_topics": ["alzheimer"],
             "keywords": ["alzheimer", "memory", "dementia", "cognitive", "neurodegeneration"]},
            {"id": 9, "query": "Stroke cerebrovascular accident symptoms treatment",
             "category": "stroke", "relevant_topics": ["stroke"],
             "keywords": ["stroke", "cerebrovascular", "brain", "ischemic", "hemorrhagic"]},
            {"id": 10, "query": "Pneumonia lung infection respiratory symptoms",
             "category": "pneumonia", "relevant_topics": ["pneumonia"],
             "keywords": ["pneumonia", "lung", "respiratory", "infection", "breathing"]},
            {"id": 11, "query": "Asthma bronchial airway inflammation treatment",
             "category": "asthma", "relevant_topics": ["asthma"],
             "keywords": ["asthma", "bronchial", "airway", "inhaler", "wheezing"]},
            {"id": 12, "query": "Arthritis joint pain inflammation rheumatoid",
             "category": "arthritis", "relevant_topics": ["arthritis"],
             "keywords": ["arthritis", "joint", "inflammation", "rheumatoid", "osteoarthritis"]},
        ]
        queries.extend(manual_queries)
        
        # 2. ä»è¯­æ–™åº“è‡ªåŠ¨ç”ŸæˆæŸ¥è¯¢ï¼ˆåŸºäºæ ‡é¢˜ï¼Œå¢åŠ æµ‹è¯•è¦†ç›–ï¼‰
        if self.corpus_df is not None and len(self.corpus_df) > 0:
            # æ¯ä¸ªä¸»é¢˜éšæœºæŠ½å–2ä¸ªæ–‡æ¡£çš„æ ‡é¢˜ä½œä¸ºæŸ¥è¯¢
            for topic, doc_ids in self.topic_docs.items():
                if len(doc_ids) >= 10:  # åªé€‰æ‹©æœ‰è¶³å¤Ÿæ–‡æ¡£çš„ä¸»é¢˜
                    sampled_ids = random.sample(doc_ids, min(2, len(doc_ids)))
                    for doc_id in sampled_ids:
                        try:
                            row = self.corpus_df[self.corpus_df['id'] == doc_id].iloc[0]
                            title = row.get('title', '')
                            if title and len(title) > 20:
                                queries.append({
                                    "id": len(queries) + 1,
                                    "query": title[:200],  # æˆªæ–­è¿‡é•¿æ ‡é¢˜
                                    "category": topic,
                                    "relevant_topics": [topic],
                                    "keywords": title.lower().split()[:5],
                                    "source": "auto_generated",
                                    "ground_truth_doc_id": doc_id
                                })
                        except Exception:
                            pass
        
        logger.info(f"ç”Ÿæˆæµ‹è¯•æŸ¥è¯¢: {len(queries)} æ¡ (æ‰‹å·¥: {len(manual_queries)}, è‡ªåŠ¨: {len(queries) - len(manual_queries)})")
        return queries
    
    # ==================== RAGè¯„ä¼°éƒ¨åˆ† ====================
    
    def _calculate_relevance(self, doc: Dict, test: Dict) -> float:
        """
        è®¡ç®—æ–‡æ¡£ä¸æŸ¥è¯¢çš„ç›¸å…³æ€§åˆ†æ•°
        
        åŸºäºå¤šç»´åº¦åˆ¤æ–­:
        1. ä¸»é¢˜åŒ¹é… (æƒé‡0.4)
        2. å…³é”®è¯è¦†ç›– (æƒé‡0.4)
        3. æ ‡é¢˜ç›¸ä¼¼åº¦ (æƒé‡0.2)
        
        Returns:
            ç›¸å…³æ€§åˆ†æ•° 0-1
        """
        score = 0.0
        doc_text = (doc.get("text", "") or doc.get("content", "")).lower()
        doc_topic = doc.get("topic", "").lower()
        
        # 1. ä¸»é¢˜åŒ¹é…
        relevant_topics = [t.lower() for t in test.get("relevant_topics", [])]
        if doc_topic and any(t in doc_topic for t in relevant_topics):
            score += 0.4
        
        # 2. å…³é”®è¯è¦†ç›–
        keywords = test.get("keywords", [])
        if keywords:
            covered = sum(1 for kw in keywords if kw.lower() in doc_text)
            keyword_score = covered / len(keywords)
            score += 0.4 * keyword_score
        
        # 3. æŸ¥è¯¢è¯åœ¨æ–‡æ¡£ä¸­çš„å‡ºç°
        query_words = test.get("query", "").lower().split()
        if query_words:
            query_covered = sum(1 for w in query_words if len(w) > 3 and w in doc_text)
            query_score = query_covered / len([w for w in query_words if len(w) > 3]) if query_words else 0
            score += 0.2 * query_score
        
        return min(score, 1.0)
    
    def _calculate_ndcg(self, relevances: List[float], k: int = 10) -> float:
        """
        è®¡ç®—NDCG@K (Normalized Discounted Cumulative Gain)
        
        Args:
            relevances: æ£€ç´¢ç»“æœçš„ç›¸å…³æ€§åˆ†æ•°åˆ—è¡¨
            k: æˆªæ–­ä½ç½®
        """
        relevances = relevances[:k]
        if not relevances:
            return 0.0
        
        # DCG
        dcg = relevances[0]
        for i, rel in enumerate(relevances[1:], 2):
            dcg += rel / np.log2(i + 1)
        
        # IDCG (ç†æƒ³æƒ…å†µï¼šæŒ‰ç›¸å…³æ€§é™åºæ’åˆ—)
        ideal_relevances = sorted(relevances, reverse=True)
        idcg = ideal_relevances[0]
        for i, rel in enumerate(ideal_relevances[1:], 2):
            idcg += rel / np.log2(i + 1)
        
        return dcg / idcg if idcg > 0 else 0.0
    
    def evaluate_rag_retrieval(self, k_values: List[int] = [1, 3, 5, 10]) -> Dict:
        """
        è¯„ä¼°RAGæ£€ç´¢æ•ˆæœ - å®¢è§‚è¯„ä¼°ç‰ˆæœ¬
        
        æŒ‡æ ‡:
        - Recall@K: åœ¨Top-Kç»“æœä¸­æ‰¾åˆ°ç›¸å…³æ–‡æ¡£çš„æ¯”ä¾‹
        - Precision@K: Top-Kç»“æœä¸­ç›¸å…³æ–‡æ¡£çš„æ¯”ä¾‹
        - F1@K: Precisionå’ŒRecallçš„è°ƒå’Œå¹³å‡
        - MRR: Mean Reciprocal Rank
        - NDCG@K: æ’åºè´¨é‡è¯„ä¼°
        - Hit Rate: å‘½ä¸­ç‡
        - Latency: å»¶è¿Ÿ
        """
        logger.info("\n" + "=" * 70)
        logger.info("ğŸ“Š RAGæ£€ç´¢æ•ˆæœè¯„ä¼°ï¼ˆå®¢è§‚è¯„ä¼°ç‰ˆæœ¬ï¼‰")
        logger.info("=" * 70)
        
        if self.rag_system is None:
            logger.warning("RAGç³»ç»Ÿæœªåˆå§‹åŒ–ï¼Œå°è¯•è‡ªåŠ¨åˆå§‹åŒ–...")
            try:
                from src.rag.rag_system import RAGSystem
                self.rag_system = RAGSystem()
            except Exception as e:
                logger.error(f"RAGç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
                return {"error": str(e)}
        
        max_k = max(k_values)
        results = {
            "queries_tested": len(self.test_queries),
            "k_values": k_values,
            "metrics": {},
            "detailed_results": []
        }
        
        # åˆå§‹åŒ–ç´¯è®¡æŒ‡æ ‡
        metrics_sum = {
            f"recall@{k}": 0 for k in k_values
        }
        metrics_sum.update({f"precision@{k}": 0 for k in k_values})
        metrics_sum.update({f"ndcg@{k}": 0 for k in k_values})
        metrics_sum["mrr"] = 0
        metrics_sum["hit_rate"] = 0
        metrics_sum["avg_relevance"] = 0
        total_latency = 0
        
        relevance_threshold = 0.3  # ç›¸å…³æ€§é˜ˆå€¼
        
        for test in self.test_queries:
            query = test["query"]
            
            logger.info(f"\næŸ¥è¯¢ [{test['id']}]: {query[:60]}...")
            
            start_time = time.time()
            try:
                retrieved_docs = self.rag_system.retrieve(query, top_k=max_k)
                latency = (time.time() - start_time) * 1000
                total_latency += latency
                
                # è®¡ç®—æ¯ä¸ªæ–‡æ¡£çš„ç›¸å…³æ€§
                relevances = []
                for doc in retrieved_docs:
                    rel = self._calculate_relevance(doc, test)
                    relevances.append(rel)
                
                # äºŒå€¼åŒ–ç›¸å…³æ€§ï¼ˆç”¨äºRecall/Precisionè®¡ç®—ï¼‰
                binary_relevances = [1 if r >= relevance_threshold else 0 for r in relevances]
                
                # è®¡ç®—å„Kå€¼çš„æŒ‡æ ‡
                detail = {
                    "query_id": test["id"],
                    "query": query[:100],
                    "category": test.get("category", "unknown"),
                    "num_results": len(retrieved_docs),
                    "latency_ms": latency,
                    "relevances": relevances[:5],  # åªä¿å­˜å‰5ä¸ª
                }
                
                for k in k_values:
                    top_k_binary = binary_relevances[:k]
                    top_k_relevances = relevances[:k]
                    
                    # Precision@K: ç›¸å…³æ–‡æ¡£æ•° / K
                    precision = sum(top_k_binary) / k if k > 0 else 0
                    metrics_sum[f"precision@{k}"] += precision
                    detail[f"precision@{k}"] = precision
                    
                    # Recall@K: å‡è®¾æ¯ä¸ªæŸ¥è¯¢æœ‰1ä¸ªå®Œç¾ç›¸å…³æ–‡æ¡£
                    # å¦‚æœæ‰¾åˆ°ä»»ä½•ç›¸å…³æ–‡æ¡£ï¼Œrecall=1
                    recall = 1.0 if sum(top_k_binary) > 0 else 0.0
                    metrics_sum[f"recall@{k}"] += recall
                    detail[f"recall@{k}"] = recall
                    
                    # NDCG@K
                    ndcg = self._calculate_ndcg(top_k_relevances, k)
                    metrics_sum[f"ndcg@{k}"] += ndcg
                    detail[f"ndcg@{k}"] = ndcg
                
                # MRR: ç¬¬ä¸€ä¸ªç›¸å…³æ–‡æ¡£çš„ä½ç½®
                mrr = 0
                for i, rel in enumerate(binary_relevances):
                    if rel == 1:
                        mrr = 1 / (i + 1)
                        break
                metrics_sum["mrr"] += mrr
                detail["mrr"] = mrr
                
                # Hit Rate
                hit = 1 if sum(binary_relevances) > 0 else 0
                metrics_sum["hit_rate"] += hit
                detail["hit"] = bool(hit)
                
                # å¹³å‡ç›¸å…³æ€§
                avg_rel = np.mean(relevances) if relevances else 0
                metrics_sum["avg_relevance"] += avg_rel
                detail["avg_relevance"] = avg_rel
                
                results["detailed_results"].append(detail)
                
                logger.info(f"  âœ… P@5={detail.get('precision@5', 0):.2f}, "
                          f"R@5={detail.get('recall@5', 0):.2f}, "
                          f"NDCG@5={detail.get('ndcg@5', 0):.3f}, "
                          f"MRR={mrr:.3f}, å»¶è¿Ÿ={latency:.1f}ms")
                
            except Exception as e:
                logger.error(f"  âŒ æŸ¥è¯¢å¤±è´¥: {e}")
                results["detailed_results"].append({
                    "query_id": test["id"],
                    "query": query[:100],
                    "error": str(e)
                })
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        n = len(self.test_queries)
        for key in metrics_sum:
            results["metrics"][key] = round(metrics_sum[key] / n, 4)
        
        results["metrics"]["avg_latency_ms"] = round(total_latency / n, 2)
        
        # è®¡ç®—F1@K
        for k in k_values:
            p = results["metrics"][f"precision@{k}"]
            r = results["metrics"][f"recall@{k}"]
            f1 = 2 * p * r / (p + r) if (p + r) > 0 else 0
            results["metrics"][f"f1@{k}"] = round(f1, 4)
        
        # æ‰“å°æ±‡æ€»
        logger.info("\n" + "-" * 50)
        logger.info("ğŸ“ˆ RAGæ£€ç´¢æ±‡æ€»ï¼ˆå®¢è§‚è¯„ä¼°ï¼‰:")
        logger.info(f"  æµ‹è¯•æŸ¥è¯¢æ•°: {n}")
        for k in k_values:
            logger.info(f"  --- @{k} ---")
            logger.info(f"    Precision@{k}: {results['metrics'][f'precision@{k}']:.3f}")
            logger.info(f"    Recall@{k}: {results['metrics'][f'recall@{k}']:.3f}")
            logger.info(f"    F1@{k}: {results['metrics'][f'f1@{k}']:.3f}")
            logger.info(f"    NDCG@{k}: {results['metrics'][f'ndcg@{k}']:.3f}")
        logger.info(f"  MRR: {results['metrics']['mrr']:.3f}")
        logger.info(f"  Hit Rate: {results['metrics']['hit_rate']:.3f}")
        logger.info(f"  å¹³å‡ç›¸å…³æ€§: {results['metrics']['avg_relevance']:.3f}")
        logger.info(f"  å¹³å‡å»¶è¿Ÿ: {results['metrics']['avg_latency_ms']:.1f}ms")
        
        self.results["rag_evaluation"] = results
        return results
    
    # ==================== æ•°æ®å¯†é›†å‹è¯„ä¼°éƒ¨åˆ† ====================
    
    def evaluate_storage_performance(self) -> Dict:
        """
        è¯„ä¼°å­˜å‚¨å±‚æ€§èƒ½
        
        æŠ€æœ¯: Parquetåˆ—å¼å­˜å‚¨
        æŒ‡æ ‡: å‹ç¼©ç‡ã€è¯»å–é€Ÿåº¦ã€ç©ºé—´æ•ˆç‡
        """
        logger.info("\n" + "=" * 70)
        logger.info("ğŸ“¦ å­˜å‚¨å±‚æ€§èƒ½è¯„ä¼° (Parquet)")
        logger.info("=" * 70)
        
        results = {"json": {}, "parquet": {}, "comparison": {}}
        
        json_path = PROCESSED_DATA_DIR / "medical_chunks.json"
        parquet_path = PROCESSED_DATA_DIR / "parquet" / "medical_chunks.parquet"
        
        # JSONæ–‡ä»¶å¤§å°ï¼ˆä¸åŠ è½½ï¼Œé¿å…OOMï¼‰
        if json_path.exists():
            json_size = json_path.stat().st_size / (1024**2)
            results["json"] = {
                "size_mb": round(json_size, 2),
                "note": "æ–‡ä»¶è¿‡å¤§ï¼Œä»…ç»Ÿè®¡å¤§å°"
            }
            logger.info(f"JSON: {json_size:.2f}MB (ä»…ç»Ÿè®¡å¤§å°)")
        
        # Parquetæ€§èƒ½
        if parquet_path.exists():
            parquet_size = parquet_path.stat().st_size / (1024**2)
            
            start = time.time()
            parquet_data = pd.read_parquet(parquet_path)
            parquet_time = time.time() - start
            
            results["parquet"] = {
                "size_mb": round(parquet_size, 2),
                "read_time_s": round(parquet_time, 3),
                "records": len(parquet_data),
                "throughput_mb_s": round(parquet_size / parquet_time, 2)
            }
            logger.info(f"Parquet: {parquet_size:.2f}MB, è¯»å–{parquet_time:.3f}s, {len(parquet_data):,}æ¡")
            
            # å¯¹æ¯”ï¼ˆåŸºäºæ–‡ä»¶å¤§å°ï¼‰
            if json_path.exists():
                compression = (1 - parquet_size / json_size) * 100
                results["comparison"] = {
                    "compression_ratio_%": round(compression, 1),
                    "space_saved_mb": round(json_size - parquet_size, 2),
                    "read_speedup": "N/A (JSONå¤ªå¤§æ— æ³•åŠ è½½)",
                    "recommendation": "ä½¿ç”¨Parquetï¼ˆæ›´é«˜æ•ˆï¼‰"
                }
                logger.info(f"âœ… Parquetå‹ç¼©ç‡: {compression:.1f}%, èŠ‚çœ: {json_size - parquet_size:.1f}MB")
        else:
            logger.warning("Parquetæ–‡ä»¶ä¸å­˜åœ¨")
        
        return results
    
    def evaluate_vector_index_performance(self) -> Dict:
        """
        è¯„ä¼°å‘é‡ç´¢å¼•æ€§èƒ½ (Milvus)
        
        æŒ‡æ ‡: ç´¢å¼•å¤§å°ã€æ£€ç´¢å»¶è¿Ÿã€QPS
        """
        logger.info("\n" + "=" * 70)
        logger.info("ğŸ” å‘é‡ç´¢å¼•æ€§èƒ½è¯„ä¼° (Milvus)")
        logger.info("=" * 70)
        
        results = {}
        
        # è¯»å–å‘é‡æ•°æ®ä¿¡æ¯
        embeddings_path = DATA_DIR / "embeddings" / "medical_embeddings.npy"
        
        if embeddings_path.exists():
            embeddings = np.load(embeddings_path)
            results["vector_count"] = embeddings.shape[0]
            results["vector_dim"] = embeddings.shape[1]
            results["storage_mb"] = round(embeddings_path.stat().st_size / (1024**2), 2)
            
            logger.info(f"å‘é‡æ•°é‡: {results['vector_count']:,}")
            logger.info(f"å‘é‡ç»´åº¦: {results['vector_dim']}")
            logger.info(f"å­˜å‚¨å¤§å°: {results['storage_mb']} MB")
            
            # ä¼°ç®—Milvuså®¹é‡
            # å‡è®¾100GBå­˜å‚¨é™åˆ¶
            max_vectors = int(100 * 1024 / results['storage_mb'] * results['vector_count'])
            results["estimated_capacity"] = max_vectors
            results["current_utilization_%"] = round(
                results['storage_mb'] / (100 * 1024) * 100, 3
            )
            
            logger.info(f"é¢„ä¼°å®¹é‡: {max_vectors:,} å‘é‡ (100GB)")
            logger.info(f"å½“å‰åˆ©ç”¨ç‡: {results['current_utilization_%']}%")
        
        return results
    
    def evaluate_pyspark_processing(self, scale_factor: int = 1) -> Dict:
        """
        è¯„ä¼°PySparkå¤§æ•°æ®å¤„ç†èƒ½åŠ›
        
        ä½¿ç”¨åŸå§‹æ•°æ®é›†å¯¹æ¯”: Pandas vs PySpark
        æŒ‡æ ‡: å¤„ç†é€Ÿåº¦ã€å†…å­˜æ•ˆç‡ã€å¯æ‰©å±•æ€§
        
        Args:
            scale_factor: æ•°æ®æ‰©å±•å€æ•° (é»˜è®¤1x, ä½¿ç”¨åŸå§‹æ•°æ®)
        """
        logger.info("\n" + "=" * 70)
        logger.info(f"âš¡ PySparkå¤§æ•°æ®å¤„ç†è¯„ä¼°ï¼ˆä½¿ç”¨åŸå§‹æ•°æ®é›†ï¼‰")
        logger.info("=" * 70)
        
        results = {
            "scale_factor": scale_factor,
            "small_data": {"pandas": {}, "pyspark": {}},
            "large_data": {"pandas": {}, "pyspark": {}},
            "comparison": {}
        }
        
        # åŸå§‹æ•°æ®è·¯å¾„
        original_path = PROCESSED_DATA_DIR / "parquet" / "medical_chunks.parquet"
        if not original_path.exists():
            logger.warning("åŸå§‹Parquetæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")
            return results
        
        # ä½¿ç”¨åŸå§‹æ•°æ®é›†ï¼Œä¸åˆ›å»ºæ‰©å±•æ•°æ®é›†
        scaled_path = original_path
        
        logger.info(f"\nğŸ“ æ•°æ®é›†:")
        logger.info(f"   æ•°æ®è·¯å¾„: {original_path}")
        
        # ========== å¤§æ•°æ®é›†æµ‹è¯• (416ä¸‡æ¡) ==========
        logger.info("\n" + "-" * 50)
        logger.info(f"ğŸ“Š å¤§æ•°æ®é›†æµ‹è¯• (416ä¸‡æ¡è®°å½•)")
        logger.info("-" * 50)
        
        # Pandaså¤„ç†
        logger.info("\n1ï¸âƒ£ Pandaså¤„ç†å¤§æ•°æ®...")
        start = time.time()
        try:
            df_large = pd.read_parquet(scaled_path)
            text_col = 'content' if 'content' in df_large.columns else df_large.columns[0]
            df_large['text_length'] = df_large[text_col].astype(str).str.len()
            _ = df_large.groupby('topic')['text_length'].agg(['mean', 'max', 'min']).reset_index()
            pandas_large_time = time.time() - start
            pandas_large_count = len(df_large)
            results["large_data"]["pandas"] = {
                "time_s": round(pandas_large_time, 3),
                "records": pandas_large_count,
                "throughput": round(pandas_large_count / pandas_large_time, 0)
            }
            logger.info(f"   Pandas: {pandas_large_time:.3f}s, {pandas_large_count:,}æ¡, "
                       f"{results['large_data']['pandas']['throughput']:,.0f} rec/s")
            del df_large
        except Exception as e:
            logger.error(f"   Pandaså¤„ç†å¤§æ•°æ®å¤±è´¥ (å†…å­˜ä¸è¶³): {e}")
            results["large_data"]["pandas"] = {"error": "å†…å­˜ä¸è¶³", "time_s": float('inf')}
            pandas_large_time = float('inf')
            pandas_large_count = 0
        
        # PySpark - å¤§æ•°æ®
        logger.info("\n3ï¸âƒ£ PySparkå¤„ç†å¤§æ•°æ®...")
        try:
            from pyspark.sql import SparkSession
            from pyspark.sql.functions import length, col, avg, max as spark_max, min as spark_min
            
            spark = SparkSession.builder \
                .appName("BigDataEvaluation") \
                .master("local[*]") \
                .config("spark.driver.memory", "8g") \
                .config("spark.executor.memory", "8g") \
                .config("spark.driver.maxResultSize", "4g") \
                .config("spark.sql.adaptive.enabled", "true") \
                .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
                .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
                .config("spark.sql.parquet.compression.codec", "snappy") \
                .getOrCreate()
            
            spark.sparkContext.setLogLevel("WARN")
            
            start = time.time()
            
            # è¯»å–æ‰©å±•æ•°æ®
            df_spark = spark.read.parquet(str(scaled_path))
            
            # æ‰§è¡Œç›¸åŒçš„å¤„ç†æ“ä½œ
            cols = df_spark.columns
            text_col = 'content' if 'content' in cols else cols[0]
            df_spark = df_spark.withColumn("text_length", length(col(text_col)))
            
            # èšåˆæ“ä½œ
            _ = df_spark.groupBy("topic").agg(
                avg("text_length").alias("avg_len"),
                spark_max("text_length").alias("max_len"),
                spark_min("text_length").alias("min_len")
            ).collect()
            
            pyspark_count = df_spark.count()
            pyspark_time = time.time() - start
            
            results["large_data"]["pyspark"] = {
                "time_s": round(pyspark_time, 3),
                "records": pyspark_count,
                "throughput": round(pyspark_count / pyspark_time, 0)
            }
            
            logger.info(f"   PySpark: {pyspark_time:.3f}s, {pyspark_count:,}æ¡, "
                       f"{results['large_data']['pyspark']['throughput']:,.0f} rec/s")
            
            spark.stop()
            
            # è®¡ç®—å¯¹æ¯”ç»“æœ
            if pandas_large_time != float('inf'):
                speedup = pandas_large_time / pyspark_time
                results["comparison"] = {
                    "speedup": round(speedup, 2),
                    "winner": "PySpark" if speedup > 1 else "Pandas",
                    "pandas_throughput": results["large_data"]["pandas"]["throughput"],
                    "pyspark_throughput": results["large_data"]["pyspark"]["throughput"],
                    "data_size_records": pyspark_count,
                    "note": f"PySparkåœ¨{scale_factor}xæ•°æ®é‡ä¸‹{'æ›´å¿«' if speedup > 1 else 'ä»è¾ƒæ…¢'}"
                }
            else:
                results["comparison"] = {
                    "winner": "PySpark",
                    "note": "Pandaså†…å­˜ä¸è¶³ï¼ŒPySparkæˆåŠŸå¤„ç†å¤§æ•°æ®",
                    "pyspark_throughput": results["large_data"]["pyspark"]["throughput"],
                    "data_size_records": pyspark_count
                }
            
            logger.info(f"\nğŸ“Š å¤§æ•°æ®å¯¹æ¯”ç»“æœ:")
            logger.info(f"   è·èƒœè€…: {results['comparison']['winner']}")
            if 'speedup' in results['comparison']:
                logger.info(f"   åŠ é€Ÿæ¯”: {results['comparison']['speedup']:.2f}x")
            logger.info(f"   PySparkååé‡: {results['large_data']['pyspark']['throughput']:,.0f} rec/s")
            
        except Exception as e:
            logger.error(f"PySparkè¯„ä¼°å¤±è´¥: {e}")
            results["large_data"]["pyspark"] = {"error": str(e)}
        
        self.results["pyspark_evaluation"] = results
        return results
    
    # ==================== ç»¼åˆè¯„ä¼° ====================
    
    def run_full_evaluation(self) -> Dict:
        """
        è¿è¡Œå®Œæ•´è¯„ä¼°
        
        Returns:
            å®Œæ•´è¯„ä¼°ç»“æœ
        """
        logger.info("\n" + "=" * 70)
        logger.info("ğŸ¯ å¼€å§‹å®Œæ•´ç³»ç»Ÿè¯„ä¼°")
        logger.info("=" * 70)
        
        start_time = time.time()
        
        # 1. RAGæ£€ç´¢è¯„ä¼°
        rag_results = self.evaluate_rag_retrieval()
        self.results["rag_evaluation"] = rag_results
        
        # 2. å­˜å‚¨æ€§èƒ½è¯„ä¼°
        storage_results = self.evaluate_storage_performance()
        self.results["data_intensive_evaluation"]["storage"] = storage_results
        
        # 3. å‘é‡ç´¢å¼•è¯„ä¼°
        index_results = self.evaluate_vector_index_performance()
        self.results["data_intensive_evaluation"]["vector_index"] = index_results
        
        # 4. PySparkè¯„ä¼°
        pyspark_results = self.evaluate_pyspark_processing()
        self.results["pyspark_evaluation"] = pyspark_results
        
        # è®¡ç®—æ€»è¯„åˆ†
        total_time = time.time() - start_time
        self.results["evaluation_time_s"] = round(total_time, 2)
        self.results["overall_score"] = self._calculate_overall_score()
        
        # ä¿å­˜ç»“æœ
        self._save_results()
        
        # æ‰“å°æ€»ç»“
        self._print_summary()
        
        return self.results
    
    def _calculate_overall_score(self) -> float:
        """è®¡ç®—ç»¼åˆè¯„åˆ† (0-100) - åŸºäºå®¢è§‚æŒ‡æ ‡"""
        scores = []
        
        # RAGè¯„åˆ† (50%) - åŸºäºæ ‡å‡†IRæŒ‡æ ‡
        rag = self.results.get("rag_evaluation", {})
        metrics = rag.get("metrics", {})
        if metrics:
            # ä½¿ç”¨F1@5, NDCG@5, MRRä½œä¸ºæ ¸å¿ƒæŒ‡æ ‡
            f1_5 = metrics.get("f1@5", 0)
            ndcg_5 = metrics.get("ndcg@5", 0)
            mrr = metrics.get("mrr", 0)
            hit_rate = metrics.get("hit_rate", 0)
            
            # åŠ æƒè®¡ç®—RAGåˆ†æ•°
            rag_score = (
                f1_5 * 30 +          # F1@5 æƒé‡30%
                ndcg_5 * 30 +        # NDCG@5 æƒé‡30%
                mrr * 25 +           # MRR æƒé‡25%
                hit_rate * 15        # Hit Rate æƒé‡15%
            )
            scores.append(("RAGæ•ˆæœ", rag_score, 0.5))
        
        # å­˜å‚¨è¯„åˆ† (25%)
        storage = self.results.get("data_intensive_evaluation", {}).get("storage", {})
        if storage:
            parquet = storage.get("parquet", {})
            if parquet:
                # åŸºäºParquetååé‡è¯„åˆ†ï¼š>10MB/så¾—æ»¡åˆ†
                throughput = parquet.get("throughput_mb_s", 0)
                storage_score = min(throughput * 5, 100)  # 20MB/s = 100åˆ†
                scores.append(("å­˜å‚¨æ•ˆç‡", storage_score, 0.25))
            elif "comparison" in storage:
                comp = storage["comparison"]
                compression = comp.get("compression_ratio_%", 0)
                storage_score = min(compression + 30, 100)
                scores.append(("å­˜å‚¨æ•ˆç‡", storage_score, 0.25))
        
        # PySparkè¯„åˆ† (25%)
        pyspark = self.results.get("pyspark_evaluation", {})
        if pyspark and "pyspark" in pyspark and "error" not in pyspark["pyspark"]:
            pyspark_score = 80  # æˆåŠŸè¿è¡Œå¾—80åˆ†
            if pyspark.get("comparison", {}).get("speedup", 0) > 1:
                pyspark_score = 100
            scores.append(("PySparkå¤„ç†", pyspark_score, 0.25))
        
        if not scores:
            return 0
        
        total = sum(s[1] * s[2] for s in scores)
        weight_sum = sum(s[2] for s in scores)
        
        return round(total / weight_sum, 1)
    
    def _save_results(self):
        """ä¿å­˜è¯„ä¼°ç»“æœåˆ°æ–‡ä»¶å’ŒMongoDB"""
        # 1. ä¿å­˜åˆ°JSONæ–‡ä»¶
        output_file = RESULTS_DIR / "evaluation" / f"unified_eval_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜: {output_file}")
        
        # 2. ä¿å­˜åˆ°MongoDB
        try:
            from src.storage.mongodb_storage import MongoDBStorage
            mongodb = MongoDBStorage(
                host=MONGODB_HOST,
                port=MONGODB_PORT,
                database=MONGODB_DATABASE
            )
            mongodb.save_evaluation_results(self.results)
            logger.info("ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°MongoDB")
        except Exception as e:
            logger.warning(f"MongoDBä¿å­˜å¤±è´¥: {e}")
    
    def _print_summary(self):
        """æ‰“å°è¯„ä¼°æ€»ç»“"""
        logger.info("\n" + "=" * 70)
        logger.info("ğŸ“Š è¯„ä¼°æ€»ç»“ï¼ˆå®¢è§‚è¯„ä¼°ç‰ˆæœ¬ï¼‰")
        logger.info("=" * 70)
        
        # RAGæ€»ç»“ - ä½¿ç”¨æ ‡å‡†IRæŒ‡æ ‡
        rag = self.results.get("rag_evaluation", {})
        metrics = rag.get("metrics", {})
        if metrics:
            logger.info(f"\nğŸ” RAGæ£€ç´¢æ•ˆæœï¼ˆæ ‡å‡†IRæŒ‡æ ‡ï¼‰:")
            logger.info(f"   æµ‹è¯•æŸ¥è¯¢æ•°: {rag.get('queries_tested', 0)}")
            logger.info(f"   --- æ ¸å¿ƒæŒ‡æ ‡ ---")
            logger.info(f"   Precision@5: {metrics.get('precision@5', 0):.3f}")
            logger.info(f"   Recall@5: {metrics.get('recall@5', 0):.3f}")
            logger.info(f"   F1@5: {metrics.get('f1@5', 0):.3f}")
            logger.info(f"   NDCG@5: {metrics.get('ndcg@5', 0):.3f}")
            logger.info(f"   MRR: {metrics.get('mrr', 0):.3f}")
            logger.info(f"   Hit Rate: {metrics.get('hit_rate', 0):.3f}")
            logger.info(f"   å¹³å‡å»¶è¿Ÿ: {metrics.get('avg_latency_ms', 0):.1f}ms")
        
        # å­˜å‚¨æ€»ç»“
        storage = self.results.get("data_intensive_evaluation", {}).get("storage", {})
        if storage:
            parquet = storage.get("parquet", {})
            if parquet:
                logger.info(f"\nğŸ“¦ å­˜å‚¨æ€§èƒ½ (Parquet):")
                logger.info(f"   æ–‡ä»¶å¤§å°: {parquet.get('size_mb', 0):.1f} MB")
                logger.info(f"   è¯»å–åå: {parquet.get('throughput_mb_s', 0):.1f} MB/s")
                logger.info(f"   è®°å½•æ•°: {parquet.get('records', 0):,}")
        
        # PySparkæ€»ç»“
        pyspark = self.results.get("pyspark_evaluation", {})
        if pyspark and "comparison" in pyspark:
            comp = pyspark["comparison"]
            logger.info(f"\nâš¡ PySparkå¤„ç†:")
            if "winner" in comp:
                logger.info(f"   ä¼˜èƒœè€…: {comp['winner']}")
                logger.info(f"   é€Ÿåº¦æ¯”: {comp['speedup']}x")
                logger.info(f"   è¯´æ˜: {comp.get('note', '')}")
            else:
                logger.info(f"   çŠ¶æ€: {comp.get('note', 'N/A')}")
        
        # æ€»è¯„åˆ†
        logger.info(f"\nğŸ¯ ç»¼åˆè¯„åˆ†: {self.results['overall_score']}/100")
        logger.info(f"â±ï¸ è¯„ä¼°è€—æ—¶: {self.results['evaluation_time_s']}s")
        
        # è¯„ä¼°æ–¹æ³•è¯´æ˜
        logger.info("\n" + "-" * 50)
        logger.info("ğŸ“‹ è¯„ä¼°æ–¹æ³•è¯´æ˜:")
        logger.info("   â€¢ Precision@K: Top-Kç»“æœä¸­ç›¸å…³æ–‡æ¡£çš„æ¯”ä¾‹")
        logger.info("   â€¢ Recall@K: æ‰¾åˆ°ç›¸å…³æ–‡æ¡£çš„æŸ¥è¯¢æ¯”ä¾‹")
        logger.info("   â€¢ F1@K: Precisionå’ŒRecallçš„è°ƒå’Œå¹³å‡")
        logger.info("   â€¢ NDCG@K: æ’åºè´¨é‡ï¼ˆè€ƒè™‘ä½ç½®æƒé‡ï¼‰")
        logger.info("   â€¢ MRR: ç¬¬ä¸€ä¸ªç›¸å…³ç»“æœçš„å¹³å‡å€’æ•°æ’å")
        logger.info("   â€¢ ç›¸å…³æ€§åˆ¤æ–­: ä¸»é¢˜åŒ¹é…(40%) + å…³é”®è¯è¦†ç›–(40%) + æŸ¥è¯¢è¯åŒ¹é…(20%)")
        
        # æŠ€æœ¯æ ˆä½¿ç”¨æƒ…å†µ
        logger.info("\n" + "-" * 50)
        logger.info("ğŸ“š æ•°æ®å¯†é›†å‹æŠ€æœ¯æ ˆ:")
        logger.info("   âœ… Parquetåˆ—å¼å­˜å‚¨ - æ•°æ®å‹ç¼©å’Œå¿«é€Ÿè¯»å–")
        logger.info("   âœ… Milvuså‘é‡æ•°æ®åº“ - é«˜æ€§èƒ½å‘é‡æ£€ç´¢")
        logger.info("   âœ… PySparkåˆ†å¸ƒå¼å¤„ç† - å¤§è§„æ¨¡æ•°æ®å¤„ç†")
        logger.info("   âœ… Redisç¼“å­˜ - æŸ¥è¯¢åŠ é€Ÿ")
        logger.info("   âœ… Reranké‡æ’åº - æå‡æ£€ç´¢ç²¾åº¦")


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 70)
    print("ğŸš€ ç»Ÿä¸€è¯„ä¼°ç³»ç»Ÿ")
    print("=" * 70)
    
    evaluator = UnifiedEvaluator()
    results = evaluator.run_full_evaluation()
    
    print("\n" + "=" * 70)
    print(f"âœ… è¯„ä¼°å®Œæˆ! ç»¼åˆè¯„åˆ†: {results['overall_score']}/100")
    print("=" * 70)


if __name__ == "__main__":
    main()
