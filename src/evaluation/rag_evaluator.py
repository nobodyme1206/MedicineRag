#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RAGç³»ç»Ÿè¯„ä¼°æ¨¡å—
åŸºäºPubMedQAå…¬å¼€æ•°æ®é›†ï¼Œè¯„ä¼°æ£€ç´¢æ•ˆæœ

è¯„ä¼°å†…å®¹:
- BM25åŸºçº¿
- å‘é‡æ£€ç´¢åŸºçº¿
- æ··åˆRAGç³»ç»Ÿï¼ˆHybridï¼‰

æŒ‡æ ‡:
- Precision@K, Recall@K, F1@K
- MRR, MAP, NDCG@K
"""

from __future__ import annotations

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent.parent))

import json
import time
from typing import Dict, List, Optional, Callable, Any
from datetime import datetime
from dataclasses import dataclass, asdict

import numpy as np
import pandas as pd

from config.config import DATA_DIR, PROCESSED_DATA_DIR, RESULTS_DIR, LOGS_DIR
from src.utils.logger import setup_logger
from src.utils.exceptions import handle_errors

logger = setup_logger("rag_evaluator", LOGS_DIR / "rag_evaluation.log")

# ç±»å‹åˆ«å
SearchResult = Dict[str, Any]
Metrics = Dict[str, float]


@dataclass
class TestQuery:
    """æµ‹è¯•æŸ¥è¯¢"""
    id: str
    query: str
    relevant_doc_ids: List[str]
    relevance_grades: List[int]  # 0-3åˆ†çº§
    answer: Optional[str] = None
    source: str = "pubmedqa"


class RAGEvaluator:
    """RAGç³»ç»Ÿè¯„ä¼°å™¨"""
    
    def __init__(self):
        self.test_queries: List[TestQuery] = []
        self.cache_dir = DATA_DIR / "test_set"
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.results = {}
    
    # ==================== æ•°æ®åŠ è½½ ====================
    
    def load_pubmedqa(self, max_samples: int = 200) -> int:
        """
        åŠ è½½PubMedQAæ•°æ®é›†
        https://pubmedqa.github.io/
        """
        cache_file = self.cache_dir / "pubmedqa_test.json"
        
        if cache_file.exists():
            logger.info(f"ä»ç¼“å­˜åŠ è½½PubMedQA: {cache_file}")
            with open(cache_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            self.test_queries = [TestQuery(**item) for item in data[:max_samples]]
            return len(self.test_queries)
        
        logger.info("ä»HuggingFaceä¸‹è½½PubMedQA...")
        try:
            from datasets import load_dataset
            dataset = load_dataset("qiaojin/PubMedQA", "pqa_labeled", split="train")
            
            queries = []
            for i, item in enumerate(dataset):
                if i >= max_samples:
                    break
                
                pubid = str(item.get("pubid", i))
                query = TestQuery(
                    id=f"pubmedqa_{pubid}",
                    query=item["question"],
                    relevant_doc_ids=[pubid],
                    relevance_grades=[3],
                    answer=item.get("long_answer", ""),
                    source="pubmedqa"
                )
                queries.append(query)
            
            with open(cache_file, 'w', encoding='utf-8') as f:
                json.dump([asdict(q) for q in queries], f, ensure_ascii=False, indent=2)
            
            self.test_queries = queries
            logger.info(f"PubMedQAåŠ è½½å®Œæˆ: {len(queries)} æ¡")
            return len(queries)
            
        except Exception as e:
            logger.warning(f"PubMedQAåŠ è½½å¤±è´¥: {e}")
            return self._load_fallback_testset(max_samples)
    
    def _load_fallback_testset(self, max_samples: int = 100) -> int:
        """ä»è¯­æ–™åº“ç”Ÿæˆå¤‡ç”¨æµ‹è¯•é›†"""
        parquet_path = PROCESSED_DATA_DIR / "parquet" / "medical_chunks.parquet"
        
        if not parquet_path.exists():
            logger.error("è¯­æ–™åº“ä¸å­˜åœ¨")
            return 0
        
        logger.info("ä»è¯­æ–™åº“ç”Ÿæˆæµ‹è¯•é›†...")
        df = pd.read_parquet(parquet_path, columns=['id', 'title', 'topic'])
        
        queries = []
        for topic in df['topic'].unique():
            topic_docs = df[df['topic'] == topic]
            if len(topic_docs) < 10:
                continue
            
            sampled = topic_docs.sample(min(3, len(topic_docs)))
            for _, row in sampled.iterrows():
                title = row.get('title', '')
                if not title or len(str(title)) < 20:
                    continue
                
                relevant_ids = topic_docs['id'].tolist()[:10]
                queries.append(TestQuery(
                    id=f"synthetic_{row['id']}",
                    query=str(title),
                    relevant_doc_ids=relevant_ids,
                    relevance_grades=[2] * len(relevant_ids),
                    source="synthetic"
                ))
                
                if len(queries) >= max_samples:
                    break
            if len(queries) >= max_samples:
                break
        
        self.test_queries = queries
        logger.info(f"ç”Ÿæˆæµ‹è¯•é›†: {len(queries)} æ¡")
        return len(queries)
    
    # ==================== è¯„ä¼°æŒ‡æ ‡ ====================
    
    def _precision_at_k(self, retrieved: List[str], relevant: List[str], k: int) -> float:
        """Precision@K"""
        top_k = set(retrieved[:k])
        return len(top_k & set(relevant)) / k if k > 0 else 0.0
    
    def _recall_at_k(self, retrieved: List[str], relevant: List[str], k: int) -> float:
        """Recall@K"""
        if not relevant:
            return 0.0
        top_k = set(retrieved[:k])
        return len(top_k & set(relevant)) / len(relevant)
    
    def _mrr(self, retrieved: List[str], relevant: List[str]) -> float:
        """MRR"""
        relevant_set = set(relevant)
        for i, doc_id in enumerate(retrieved):
            if doc_id in relevant_set:
                return 1.0 / (i + 1)
        return 0.0
    
    def _ndcg_at_k(self, retrieved: List[str], relevance_map: Dict[str, int], k: int) -> float:
        """NDCG@K"""
        gains = [relevance_map.get(doc_id, 0) for doc_id in retrieved[:k]]
        dcg = sum((2 ** g - 1) / np.log2(i + 2) for i, g in enumerate(gains))
        ideal = sorted(relevance_map.values(), reverse=True)[:k]
        idcg = sum((2 ** g - 1) / np.log2(i + 2) for i, g in enumerate(ideal))
        return dcg / idcg if idcg > 0 else 0.0
    
    def _map_score(self, retrieved: List[str], relevant: List[str]) -> float:
        """MAP"""
        if not relevant:
            return 0.0
        relevant_set = set(relevant)
        precisions = []
        hit = 0
        for i, doc_id in enumerate(retrieved):
            if doc_id in relevant_set:
                hit += 1
                precisions.append(hit / (i + 1))
        return np.mean(precisions) if precisions else 0.0

    # ==================== æ£€ç´¢å™¨è¯„ä¼° ====================
    
    def _evaluate_retriever(self, retriever_fn: Callable, name: str,
                            k_values: List[int] = [5, 10, 20]) -> Dict:
        """é€šç”¨æ£€ç´¢å™¨è¯„ä¼°"""
        if not self.test_queries:
            logger.error("æµ‹è¯•é›†ä¸ºç©º")
            return {"error": "æµ‹è¯•é›†ä¸ºç©º"}
        
        max_k = max(k_values)
        metrics = {f"P@{k}": [] for k in k_values}
        metrics.update({f"R@{k}": [] for k in k_values})
        metrics.update({f"NDCG@{k}": [] for k in k_values})
        metrics["MRR"] = []
        metrics["MAP"] = []
        metrics["latency_ms"] = []
        
        logger.info(f"\nè¯„ä¼° {name}ï¼Œå…± {len(self.test_queries)} ä¸ªæŸ¥è¯¢...")
        
        for i, test in enumerate(self.test_queries):
            if (i + 1) % 50 == 0:
                logger.info(f"  è¿›åº¦: {i + 1}/{len(self.test_queries)}")
            
            try:
                start = time.time()
                results = retriever_fn(test.query, max_k)
                latency = (time.time() - start) * 1000
                
                # æå–æ£€ç´¢ç»“æœçš„PMIDï¼ˆå»æ‰chunkåç¼€ï¼Œå¦‚ "12345_0" -> "12345"ï¼‰
                retrieved_pmids = []
                for j, doc in enumerate(results):
                    pmid = doc.get("pmid") or doc.get("id") or doc.get("doc_id") or str(j)
                    pmid = str(pmid).split("_")[0]  # å»æ‰chunkåç¼€
                    retrieved_pmids.append(pmid)
                
                # å»é‡ä½†ä¿æŒé¡ºåºï¼ˆåŒä¸€PMIDçš„å¤šä¸ªchunkåªç®—ä¸€æ¬¡ï¼‰
                seen = set()
                retrieved_ids = []
                for pmid in retrieved_pmids:
                    if pmid not in seen:
                        seen.add(pmid)
                        retrieved_ids.append(pmid)
                
                relevance_map = dict(zip(test.relevant_doc_ids, test.relevance_grades))
                
                for k in k_values:
                    metrics[f"P@{k}"].append(self._precision_at_k(retrieved_ids, test.relevant_doc_ids, k))
                    metrics[f"R@{k}"].append(self._recall_at_k(retrieved_ids, test.relevant_doc_ids, k))
                    metrics[f"NDCG@{k}"].append(self._ndcg_at_k(retrieved_ids, relevance_map, k))
                
                metrics["MRR"].append(self._mrr(retrieved_ids, test.relevant_doc_ids))
                metrics["MAP"].append(self._map_score(retrieved_ids, test.relevant_doc_ids))
                metrics["latency_ms"].append(latency)
                
            except Exception as e:
                logger.warning(f"æŸ¥è¯¢å¤±è´¥ [{test.id}]: {e}")
        
        result = {"name": name, "num_queries": len(self.test_queries), "metrics": {}}
        for key, values in metrics.items():
            if values:
                result["metrics"][key] = round(np.mean(values), 4)
                result["metrics"][f"{key}_std"] = round(np.std(values), 4)
        
        for k in k_values:
            p = result["metrics"].get(f"P@{k}", 0)
            r = result["metrics"].get(f"R@{k}", 0)
            result["metrics"][f"F1@{k}"] = round(2 * p * r / (p + r), 4) if (p + r) > 0 else 0
        
        return result
    
    def evaluate_bm25(self, k_values: List[int] = [5, 10, 20]) -> Dict:
        """è¯„ä¼°BM25åŸºçº¿"""
        logger.info("\n" + "=" * 50)
        logger.info("ğŸ“Š BM25 åŸºçº¿è¯„ä¼°")
        logger.info("=" * 50)
        
        try:
            from src.retrieval.hybrid_searcher import HybridSearcher
            searcher = HybridSearcher()
            
            def bm25_retriever(query, k):
                # BM25è¿”å› [(chunk_idx, score), ...]
                results = searcher.bm25_search(query, top_k=k)
                # è½¬æ¢ä¸ºåŒ…å«pmidçš„å­—å…¸æ ¼å¼
                docs = []
                for idx, score in results:
                    if idx < len(searcher.chunks):
                        chunk = searcher.chunks[idx]
                        pmid = chunk.get('pmid', str(idx))
                        docs.append({"pmid": pmid, "score": score})
                return docs
            
            return self._evaluate_retriever(
                retriever_fn=bm25_retriever,
                name="BM25", k_values=k_values
            )
        except Exception as e:
            logger.error(f"BM25è¯„ä¼°å¤±è´¥: {e}")
            return {"name": "BM25", "error": str(e)}
    
    def evaluate_vector(self, k_values: List[int] = [5, 10, 20]) -> Dict:
        """è¯„ä¼°å‘é‡æ£€ç´¢åŸºçº¿"""
        logger.info("\n" + "=" * 50)
        logger.info("ğŸ“Š Vector åŸºçº¿è¯„ä¼°")
        logger.info("=" * 50)
        
        try:
            from src.rag.rag_system import RAGSystem
            rag = RAGSystem()
            return self._evaluate_retriever(
                retriever_fn=lambda q, k: rag.vector_search(q, top_k=k),
                name="Vector", k_values=k_values
            )
        except Exception as e:
            logger.error(f"Vectorè¯„ä¼°å¤±è´¥: {e}")
            return {"name": "Vector", "error": str(e)}
    
    def evaluate_hybrid(self, k_values: List[int] = [5, 10, 20], use_hyde: bool = False) -> Dict:
        """è¯„ä¼°æ··åˆRAGç³»ç»Ÿ"""
        logger.info("\n" + "=" * 50)
        logger.info(f"ğŸ“Š Hybrid RAG è¯„ä¼° (HyDE: {use_hyde})")
        logger.info("=" * 50)
        
        try:
            from src.rag.rag_system import RAGSystem
            # å…³é—­HyDEåŠ é€Ÿè¯„ä¼°ï¼Œä¿ç•™æ··åˆæ£€ç´¢å’ŒRerank
            rag = RAGSystem(use_hyde=use_hyde)
            return self._evaluate_retriever(
                retriever_fn=lambda q, k: rag.retrieve(q, top_k=k),
                name="Hybrid_RAG", k_values=k_values
            )
        except Exception as e:
            logger.error(f"Hybridè¯„ä¼°å¤±è´¥: {e}")
            return {"name": "Hybrid_RAG", "error": str(e)}
    
    # ==================== å®Œæ•´è¯„ä¼° ====================
    
    def run_evaluation(self, k_values: List[int] = [5, 10, 20]) -> Dict:
        """è¿è¡Œå®Œæ•´RAGè¯„ä¼°"""
        logger.info("\n" + "=" * 60)
        logger.info("ğŸš€ RAGç³»ç»Ÿè¯„ä¼° (BM25 / Vector / Hybrid)")
        logger.info("=" * 60)
        
        if not self.test_queries:
            self.load_pubmedqa()
        
        self.results = {
            "timestamp": datetime.now().isoformat(),
            "dataset": {"name": "PubMedQA", "num_queries": len(self.test_queries)},
            "methods": {}
        }
        
        self.results["methods"]["BM25"] = self.evaluate_bm25(k_values)
        self.results["methods"]["Vector"] = self.evaluate_vector(k_values)
        self.results["methods"]["Hybrid_RAG"] = self.evaluate_hybrid(k_values)
        
        self._print_comparison(k_values)
        self._save_results()
        
        return self.results
    
    def _print_comparison(self, k_values: List[int]):
        """æ‰“å°å¯¹æ¯”è¡¨æ ¼"""
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ“Š RAGè¯„ä¼°ç»“æœå¯¹æ¯”")
        logger.info("=" * 80)
        
        header = f"{'æ–¹æ³•':<15}"
        for k in k_values:
            header += f"{'P@'+str(k):<8}{'R@'+str(k):<8}{'F1@'+str(k):<8}"
        header += f"{'MRR':<8}{'MAP':<8}{'å»¶è¿Ÿms':<10}"
        logger.info(header)
        logger.info("-" * 80)
        
        for name, data in self.results["methods"].items():
            if "error" in data:
                logger.info(f"{name:<15} è¯„ä¼°å¤±è´¥: {data['error']}")
                continue
            m = data.get("metrics", {})
            row = f"{name:<15}"
            for k in k_values:
                row += f"{m.get(f'P@{k}', 0):<8.3f}{m.get(f'R@{k}', 0):<8.3f}{m.get(f'F1@{k}', 0):<8.3f}"
            row += f"{m.get('MRR', 0):<8.3f}{m.get('MAP', 0):<8.3f}{m.get('latency_ms', 0):<10.1f}"
            logger.info(row)
        
        logger.info("=" * 80)
    
    def _save_results(self):
        """ä¿å­˜ç»“æœ"""
        output_file = RESULTS_DIR / "evaluation" / "rag_evaluation.json"
        output_file.parent.mkdir(parents=True, exist_ok=True)
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        logger.info(f"\nç»“æœå·²ä¿å­˜: {output_file}")


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    import argparse
    parser = argparse.ArgumentParser(description="RAGç³»ç»Ÿè¯„ä¼°")
    parser.add_argument("--samples", type=int, default=200, help="æµ‹è¯•æ ·æœ¬æ•°")
    args = parser.parse_args()
    
    evaluator = RAGEvaluator()
    evaluator.load_pubmedqa(args.samples)
    evaluator.run_evaluation()


if __name__ == "__main__":
    main()
