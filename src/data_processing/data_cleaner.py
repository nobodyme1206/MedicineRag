# -*- coding: utf-8 -*-
"""
æ•°æ®é™å™ªä¸å¼‚å¸¸æ£€æµ‹æ¨¡å—
åŠŸèƒ½: æ–‡æœ¬è´¨é‡è¯„ä¼°ã€å»é‡ã€å¼‚å¸¸æ£€æµ‹ã€æ•°æ®æ¸…æ´—
"""

import re
import json
import hashlib
from pathlib import Path
from typing import List, Dict, Tuple, Set
from collections import Counter
import sys

sys.path.append(str(Path(__file__).parent.parent.parent))
from config.config import RAW_DATA_DIR, PROCESSED_DATA_DIR, LOGS_DIR
from src.utils.logger import setup_logger

logger = setup_logger("data_cleaner", LOGS_DIR / "data_cleaning.log")


class DataQualityChecker:
    """æ•°æ®è´¨é‡æ£€æŸ¥å™¨"""
    
    def __init__(self):
        # è´¨é‡é˜ˆå€¼
        self.min_abstract_length = 100  # æœ€å°æ‘˜è¦é•¿åº¦
        self.max_abstract_length = 10000  # æœ€å¤§æ‘˜è¦é•¿åº¦
        self.min_title_length = 10  # æœ€å°æ ‡é¢˜é•¿åº¦
        self.max_duplicate_ratio = 0.3  # æœ€å¤§é‡å¤å†…å®¹æ¯”ä¾‹
        
        # åƒåœ¾è¯åˆ—è¡¨
        self.spam_patterns = [
            r'click here', r'buy now', r'free download',
            r'http[s]?://(?!www\.ncbi|pubmed|doi)',  # éå­¦æœ¯é“¾æ¥
            r'[A-Z]{10,}',  # è¿ç»­å¤§å†™å­—æ¯
            r'(.)\1{5,}',  # é‡å¤å­—ç¬¦
        ]
        
        # å¿…éœ€å­—æ®µ
        self.required_fields = ['pmid', 'title', 'abstract']
    
    def check_article(self, article: Dict) -> Tuple[bool, List[str]]:
        """
        æ£€æŸ¥å•ç¯‡æ–‡ç« è´¨é‡
        
        Returns:
            (æ˜¯å¦é€šè¿‡, é—®é¢˜åˆ—è¡¨)
        """
        issues = []
        
        # 1. æ£€æŸ¥å¿…éœ€å­—æ®µ
        for field in self.required_fields:
            if not article.get(field):
                issues.append(f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
        
        if issues:
            return False, issues
        
        title = article.get('title', '')
        abstract = article.get('abstract', '')
        
        # 2. æ£€æŸ¥é•¿åº¦
        if len(abstract) < self.min_abstract_length:
            issues.append(f"æ‘˜è¦è¿‡çŸ­: {len(abstract)} < {self.min_abstract_length}")
        
        if len(abstract) > self.max_abstract_length:
            issues.append(f"æ‘˜è¦è¿‡é•¿: {len(abstract)} > {self.max_abstract_length}")
        
        if len(title) < self.min_title_length:
            issues.append(f"æ ‡é¢˜è¿‡çŸ­: {len(title)} < {self.min_title_length}")
        
        # 3. æ£€æŸ¥åƒåœ¾å†…å®¹
        text = f"{title} {abstract}".lower()
        for pattern in self.spam_patterns:
            if re.search(pattern, text, re.IGNORECASE):
                issues.append(f"æ£€æµ‹åˆ°åƒåœ¾å†…å®¹æ¨¡å¼: {pattern}")
        
        # 4. æ£€æŸ¥è¯­è¨€ï¼ˆç®€å•æ£€æµ‹æ˜¯å¦ä¸ºè‹±æ–‡ï¼‰
        english_ratio = len(re.findall(r'[a-zA-Z]', text)) / max(len(text), 1)
        if english_ratio < 0.5:
            issues.append(f"éè‹±æ–‡å†…å®¹æ¯”ä¾‹è¿‡é«˜: {1-english_ratio:.2%}")
        
        # 5. æ£€æŸ¥é‡å¤å†…å®¹
        words = text.split()
        if words:
            unique_ratio = len(set(words)) / len(words)
            if unique_ratio < (1 - self.max_duplicate_ratio):
                issues.append(f"é‡å¤è¯æ¯”ä¾‹è¿‡é«˜: {1-unique_ratio:.2%}")
        
        return len(issues) == 0, issues
    
    def calculate_quality_score(self, article: Dict) -> float:
        """
        è®¡ç®—æ–‡ç« è´¨é‡åˆ†æ•° (0-100)
        """
        score = 100.0
        
        title = article.get('title', '')
        abstract = article.get('abstract', '')
        
        # é•¿åº¦è¯„åˆ†
        if len(abstract) < 200:
            score -= 20
        elif len(abstract) < 500:
            score -= 10
        
        if len(title) < 20:
            score -= 10
        
        # ç»“æ„è¯„åˆ†ï¼ˆæœ‰ä½œè€…ã€æ—¥æœŸã€å…³é”®è¯ï¼‰
        if not article.get('authors'):
            score -= 10
        if not article.get('pub_date'):
            score -= 5
        if not article.get('mesh_terms') and not article.get('keywords'):
            score -= 10
        
        # å†…å®¹è´¨é‡è¯„åˆ†
        text = f"{title} {abstract}"
        
        # è¯æ±‡ä¸°å¯Œåº¦
        words = text.lower().split()
        if words:
            unique_ratio = len(set(words)) / len(words)
            score -= max(0, (0.5 - unique_ratio) * 30)
        
        # å¥å­å®Œæ•´æ€§ï¼ˆä»¥å¥å·ç»“å°¾çš„æ¯”ä¾‹ï¼‰
        sentences = re.split(r'[.!?]', abstract)
        if len(sentences) < 3:
            score -= 15
        
        return max(0, min(100, score))


class DataDeduplicator:
    """æ•°æ®å»é‡å™¨"""
    
    def __init__(self):
        self.seen_pmids: Set[str] = set()
        self.seen_hashes: Set[str] = set()
        self.duplicate_count = 0
    
    def _compute_hash(self, text: str) -> str:
        """è®¡ç®—æ–‡æœ¬å“ˆå¸Œ"""
        normalized = re.sub(r'\s+', ' ', text.lower().strip())
        return hashlib.md5(normalized.encode()).hexdigest()
    
    def is_duplicate(self, article: Dict) -> Tuple[bool, str]:
        """
        æ£€æŸ¥æ˜¯å¦é‡å¤
        
        Returns:
            (æ˜¯å¦é‡å¤, é‡å¤ç±»å‹)
        """
        pmid = article.get('pmid', '')
        
        # 1. PMIDé‡å¤
        if pmid in self.seen_pmids:
            return True, "pmid_duplicate"
        
        # 2. å†…å®¹é‡å¤ï¼ˆåŸºäºæ‘˜è¦å“ˆå¸Œï¼‰
        abstract = article.get('abstract', '')
        if abstract:
            content_hash = self._compute_hash(abstract)
            if content_hash in self.seen_hashes:
                return True, "content_duplicate"
            self.seen_hashes.add(content_hash)
        
        self.seen_pmids.add(pmid)
        return False, ""
    
    def deduplicate(self, articles: List[Dict]) -> Tuple[List[Dict], int]:
        """
        æ‰¹é‡å»é‡
        
        Returns:
            (å»é‡åçš„æ–‡ç« åˆ—è¡¨, ç§»é™¤æ•°é‡)
        """
        unique_articles = []
        removed = 0
        
        for article in articles:
            is_dup, dup_type = self.is_duplicate(article)
            if not is_dup:
                unique_articles.append(article)
            else:
                removed += 1
        
        return unique_articles, removed


class AnomalyDetector:
    """å¼‚å¸¸æ£€æµ‹å™¨"""
    
    def __init__(self):
        self.stats = {
            'abstract_lengths': [],
            'title_lengths': [],
            'author_counts': []
        }
    
    def collect_stats(self, articles: List[Dict]):
        """æ”¶é›†ç»Ÿè®¡ä¿¡æ¯"""
        for article in articles:
            self.stats['abstract_lengths'].append(len(article.get('abstract', '')))
            self.stats['title_lengths'].append(len(article.get('title', '')))
            self.stats['author_counts'].append(len(article.get('authors', [])))
    
    def _calculate_bounds(self, values: List[float], k: float = 2.5) -> Tuple[float, float]:
        """è®¡ç®—å¼‚å¸¸è¾¹ç•Œï¼ˆåŸºäºIQRï¼‰"""
        if not values:
            return 0, float('inf')
        
        sorted_vals = sorted(values)
        n = len(sorted_vals)
        q1 = sorted_vals[n // 4]
        q3 = sorted_vals[3 * n // 4]
        iqr = q3 - q1
        
        lower = q1 - k * iqr
        upper = q3 + k * iqr
        
        return max(0, lower), upper
    
    def detect_anomalies(self, article: Dict) -> List[str]:
        """æ£€æµ‹å•ç¯‡æ–‡ç« çš„å¼‚å¸¸"""
        anomalies = []
        
        # æ‘˜è¦é•¿åº¦å¼‚å¸¸
        abstract_len = len(article.get('abstract', ''))
        lower, upper = self._calculate_bounds(self.stats['abstract_lengths'])
        if abstract_len < lower or abstract_len > upper:
            anomalies.append(f"æ‘˜è¦é•¿åº¦å¼‚å¸¸: {abstract_len} (æ­£å¸¸èŒƒå›´: {lower:.0f}-{upper:.0f})")
        
        # æ ‡é¢˜é•¿åº¦å¼‚å¸¸
        title_len = len(article.get('title', ''))
        lower, upper = self._calculate_bounds(self.stats['title_lengths'])
        if title_len < lower or title_len > upper:
            anomalies.append(f"æ ‡é¢˜é•¿åº¦å¼‚å¸¸: {title_len} (æ­£å¸¸èŒƒå›´: {lower:.0f}-{upper:.0f})")
        
        # ä½œè€…æ•°é‡å¼‚å¸¸
        author_count = len(article.get('authors', []))
        lower, upper = self._calculate_bounds(self.stats['author_counts'])
        if author_count > upper:
            anomalies.append(f"ä½œè€…æ•°é‡å¼‚å¸¸: {author_count} (æ­£å¸¸ä¸Šé™: {upper:.0f})")
        
        return anomalies


class DataCleaner:
    """æ•°æ®æ¸…æ´—ä¸»ç±»"""
    
    def __init__(self):
        self.quality_checker = DataQualityChecker()
        self.deduplicator = DataDeduplicator()
        self.anomaly_detector = AnomalyDetector()
        
        self.stats = {
            'total': 0,
            'passed': 0,
            'failed_quality': 0,
            'duplicates': 0,
            'anomalies': 0,
            'quality_scores': []
        }
    
    def clean_dataset(self, input_file: Path, output_file: Path = None,
                     remove_anomalies: bool = False) -> Dict:
        """
        æ¸…æ´—æ•°æ®é›†
        
        Args:
            input_file: è¾“å…¥æ–‡ä»¶è·¯å¾„
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            remove_anomalies: æ˜¯å¦ç§»é™¤å¼‚å¸¸æ•°æ®
            
        Returns:
            æ¸…æ´—ç»Ÿè®¡
        """
        logger.info("=" * 60)
        logger.info("ğŸ§¹ å¼€å§‹æ•°æ®æ¸…æ´—")
        logger.info(f"   è¾“å…¥æ–‡ä»¶: {input_file}")
        logger.info("=" * 60)
        
        # åŠ è½½æ•°æ®
        logger.info("ğŸ“‚ åŠ è½½æ•°æ®...")
        with open(input_file, 'r', encoding='utf-8') as f:
            articles = json.load(f)
        
        self.stats['total'] = len(articles)
        logger.info(f"   æ€»æ–‡ç« æ•°: {len(articles):,}")
        
        # ç¬¬ä¸€éï¼šæ”¶é›†ç»Ÿè®¡ä¿¡æ¯
        logger.info("\nğŸ“Š æ”¶é›†ç»Ÿè®¡ä¿¡æ¯...")
        self.anomaly_detector.collect_stats(articles)
        
        # ç¬¬äºŒéï¼šæ¸…æ´—
        logger.info("\nğŸ” å¼€å§‹æ¸…æ´—...")
        cleaned_articles = []
        quality_issues = Counter()
        
        for i, article in enumerate(articles):
            if (i + 1) % 50000 == 0:
                logger.info(f"   è¿›åº¦: {i+1:,}/{len(articles):,}")
            
            # 1. è´¨é‡æ£€æŸ¥
            passed, issues = self.quality_checker.check_article(article)
            if not passed:
                self.stats['failed_quality'] += 1
                for issue in issues:
                    quality_issues[issue.split(':')[0]] += 1
                continue
            
            # 2. å»é‡æ£€æŸ¥
            is_dup, _ = self.deduplicator.is_duplicate(article)
            if is_dup:
                self.stats['duplicates'] += 1
                continue
            
            # 3. å¼‚å¸¸æ£€æµ‹
            anomalies = self.anomaly_detector.detect_anomalies(article)
            if anomalies:
                self.stats['anomalies'] += 1
                article['_anomalies'] = anomalies
                if remove_anomalies:
                    continue
            
            # 4. è®¡ç®—è´¨é‡åˆ†æ•°
            score = self.quality_checker.calculate_quality_score(article)
            article['_quality_score'] = score
            self.stats['quality_scores'].append(score)
            
            cleaned_articles.append(article)
            self.stats['passed'] += 1
        
        # ä¿å­˜ç»“æœ
        if output_file:
            output_file.parent.mkdir(parents=True, exist_ok=True)
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(cleaned_articles, f, ensure_ascii=False, indent=2)
            logger.info(f"\nğŸ’¾ å·²ä¿å­˜æ¸…æ´—åæ•°æ®: {output_file}")
        
        # æ‰“å°ç»Ÿè®¡
        self._print_stats(quality_issues)
        
        return {
            'stats': self.stats,
            'quality_issues': dict(quality_issues),
            'cleaned_articles': cleaned_articles
        }
    
    def _print_stats(self, quality_issues: Counter):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“Š æ¸…æ´—ç»Ÿè®¡")
        logger.info("=" * 60)
        logger.info(f"   æ€»æ–‡ç« æ•°: {self.stats['total']:,}")
        logger.info(f"   é€šè¿‡æ•°é‡: {self.stats['passed']:,} ({self.stats['passed']/self.stats['total']*100:.1f}%)")
        logger.info(f"   è´¨é‡ä¸åˆæ ¼: {self.stats['failed_quality']:,}")
        logger.info(f"   é‡å¤ç§»é™¤: {self.stats['duplicates']:,}")
        logger.info(f"   å¼‚å¸¸æ ‡è®°: {self.stats['anomalies']:,}")
        
        if self.stats['quality_scores']:
            avg_score = sum(self.stats['quality_scores']) / len(self.stats['quality_scores'])
            logger.info(f"   å¹³å‡è´¨é‡åˆ†: {avg_score:.1f}/100")
        
        if quality_issues:
            logger.info("\nğŸ“‹ è´¨é‡é—®é¢˜åˆ†å¸ƒ:")
            for issue, count in quality_issues.most_common(10):
                logger.info(f"   - {issue}: {count:,}")


def main():
    """ä¸»å‡½æ•°"""
    input_file = RAW_DATA_DIR / "pubmed_articles_all.json"
    output_file = PROCESSED_DATA_DIR / "pubmed_cleaned.json"
    
    if not input_file.exists():
        logger.error(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        return
    
    cleaner = DataCleaner()
    result = cleaner.clean_dataset(input_file, output_file, remove_anomalies=False)
    
    logger.info("\nâœ… æ•°æ®æ¸…æ´—å®Œæˆ!")


if __name__ == "__main__":
    main()
