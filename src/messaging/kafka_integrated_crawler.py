# -*- coding: utf-8 -*-
"""
Kafkaé›†æˆç‰ˆçˆ¬è™« - å°†åŸæœ‰çˆ¬è™«ä¸Kafkaé›†æˆ
çˆ¬å–çš„æ–‡ç« å®æ—¶å‘é€åˆ°Kafkaï¼Œå®ç°é‡‡é›†ä¸å¤„ç†è§£è€¦
"""

import asyncio
import json
import time
from pathlib import Path
from typing import List, Dict
import sys

sys.path.append(str(Path(__file__).parent.parent.parent))

from config.config import RAW_DATA_DIR, LOGS_DIR
from src.utils.logger import setup_logger
from src.messaging.kafka_producer import KafkaArticleProducer, KAFKA_TOPIC_RAW

logger = setup_logger("kafka_crawler", LOGS_DIR / "kafka_crawler.log")


class KafkaIntegratedCrawler:
    """
    Kafkaé›†æˆç‰ˆçˆ¬è™«
    
    ä¸åŸæœ‰AsyncPubMedCrawlerçš„åŒºåˆ«ï¼š
    1. çˆ¬å–çš„æ–‡ç« å®æ—¶å‘é€åˆ°Kafkaï¼Œä¸ç­‰å¾…å¤„ç†
    2. æ”¯æŒæ›´é«˜çš„ååé‡ï¼ˆé‡‡é›†å’Œå¤„ç†å¹¶è¡Œï¼‰
    3. æ¶ˆæ¯æŒä¹…åŒ–ï¼Œæ”¯æŒé‡æ”¾
    """
    
    def __init__(self, use_kafka: bool = True):
        """
        åˆå§‹åŒ–
        
        Args:
            use_kafka: æ˜¯å¦ä½¿ç”¨Kafkaï¼ˆFalseåˆ™å›é€€åˆ°åŸæœ‰æ¨¡å¼ï¼‰
        """
        self.use_kafka = use_kafka
        self.producer = None
        self.crawler = None
        
        # åˆå§‹åŒ–Kafkaç”Ÿäº§è€…
        if use_kafka:
            self.producer = KafkaArticleProducer()
            if not self.producer.producer:
                logger.warning("Kafkaä¸å¯ç”¨ï¼Œå›é€€åˆ°æœ¬åœ°æ¨¡å¼")
                self.use_kafka = False
        
        # åˆå§‹åŒ–åŸæœ‰çˆ¬è™«
        from src.data_processing.pubmed_crawler import AsyncPubMedCrawler
        self.crawler = AsyncPubMedCrawler()
        
        # ç»Ÿè®¡
        self.stats = {
            "crawled": 0,
            "sent_to_kafka": 0,
            "saved_local": 0
        }
    
    def _on_article_crawled(self, article: Dict) -> bool:
        """
        æ–‡ç« çˆ¬å–å›è°ƒ - å‘é€åˆ°Kafka
        
        Args:
            article: çˆ¬å–çš„æ–‡ç« 
            
        Returns:
            æ˜¯å¦æˆåŠŸå¤„ç†
        """
        self.stats["crawled"] += 1
        
        if self.use_kafka and self.producer:
            # å‘é€åˆ°Kafka
            success = self.producer.send_article(article, KAFKA_TOPIC_RAW)
            if success:
                self.stats["sent_to_kafka"] += 1
            return success
        else:
            # æœ¬åœ°æ¨¡å¼
            self.stats["saved_local"] += 1
            return True
    
    def crawl_with_kafka(self, topics: List[str] = None, max_concurrent: int = 3):
        """
        ä½¿ç”¨Kafkaçš„çˆ¬å–æ¨¡å¼
        
        çˆ¬å–æµç¨‹ï¼š
        1. çˆ¬è™«çˆ¬å–æ–‡ç« 
        2. æ–‡ç« å®æ—¶å‘é€åˆ°Kafka
        3. æ¶ˆè´¹è€…å¼‚æ­¥å¤„ç†ï¼ˆå¯ä»¥æ˜¯å¦ä¸€ä¸ªè¿›ç¨‹ï¼‰
        """
        logger.info("=" * 60)
        logger.info("ğŸš€ Kafkaé›†æˆçˆ¬è™«å¯åŠ¨")
        logger.info(f"   Kafkaæ¨¡å¼: {'å¯ç”¨' if self.use_kafka else 'ç¦ç”¨'}")
        logger.info("=" * 60)
        
        start_time = time.time()
        
        # ä½¿ç”¨åŸæœ‰çˆ¬è™«çˆ¬å–
        articles = self.crawler.crawl_all_topics(topics, max_concurrent)
        
        # æ‰¹é‡å‘é€åˆ°Kafkaï¼ˆå¦‚æœä¹‹å‰æ²¡æœ‰å®æ—¶å‘é€ï¼‰
        if self.use_kafka and self.producer:
            logger.info(f"ğŸ“¤ æ‰¹é‡å‘é€ {len(articles)} ç¯‡æ–‡ç« åˆ°Kafka...")
            sent = self.producer.send_batch(articles)
            self.stats["sent_to_kafka"] = sent
        
        elapsed = time.time() - start_time
        
        # æ‰“å°ç»Ÿè®¡
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“Š çˆ¬å–å®Œæˆç»Ÿè®¡")
        logger.info(f"   æ€»è€—æ—¶: {elapsed:.1f} ç§’")
        logger.info(f"   çˆ¬å–æ–‡ç« : {len(articles)}")
        logger.info(f"   å‘é€Kafka: {self.stats['sent_to_kafka']}")
        if self.producer:
            logger.info(f"   Kafkaç»Ÿè®¡: {self.producer.get_stats()}")
        logger.info("=" * 60)
        
        return articles
    
    def close(self):
        """å…³é—­èµ„æº"""
        if self.producer:
            self.producer.close()


def run_kafka_pipeline():
    """
    è¿è¡ŒKafkaé›†æˆçš„å®Œæ•´Pipeline
    
    æ¶æ„ï¼š
    [çˆ¬è™«] â†’ [Kafka: raw_articles] â†’ [å¤„ç†æ¶ˆè´¹è€…] â†’ [Kafka: processed] â†’ [å‘é‡åŒ–æ¶ˆè´¹è€…] â†’ [Milvus]
    """
    import multiprocessing
    from src.messaging.kafka_consumer import DataProcessingConsumer, EmbeddingConsumer
    
    logger.info("=" * 60)
    logger.info("ğŸš€ å¯åŠ¨Kafkaé›†æˆPipeline")
    logger.info("=" * 60)
    
    # å¯åŠ¨æ¶ˆè´¹è€…è¿›ç¨‹
    def run_processor():
        consumer = DataProcessingConsumer()
        consumer.start()
    
    def run_embedder():
        consumer = EmbeddingConsumer()
        consumer.start()
    
    # å¯åŠ¨å¤„ç†æ¶ˆè´¹è€…
    processor_process = multiprocessing.Process(target=run_processor, name="DataProcessor")
    processor_process.start()
    logger.info("âœ… æ•°æ®å¤„ç†æ¶ˆè´¹è€…å·²å¯åŠ¨")
    
    # å¯åŠ¨å‘é‡åŒ–æ¶ˆè´¹è€…
    embedder_process = multiprocessing.Process(target=run_embedder, name="Embedder")
    embedder_process.start()
    logger.info("âœ… å‘é‡åŒ–æ¶ˆè´¹è€…å·²å¯åŠ¨")
    
    # å¯åŠ¨çˆ¬è™«ï¼ˆç”Ÿäº§è€…ï¼‰
    crawler = KafkaIntegratedCrawler(use_kafka=True)
    
    try:
        crawler.crawl_with_kafka()
    finally:
        crawler.close()
        
        # ç­‰å¾…æ¶ˆè´¹è€…å¤„ç†å®Œæˆ
        logger.info("ç­‰å¾…æ¶ˆè´¹è€…å¤„ç†å®Œæˆ...")
        time.sleep(30)  # ç»™æ¶ˆè´¹è€…ä¸€äº›æ—¶é—´å¤„ç†å‰©ä½™æ¶ˆæ¯
        
        processor_process.terminate()
        embedder_process.terminate()
        
        processor_process.join()
        embedder_process.join()
        
        logger.info("âœ… Pipelineå®Œæˆ")


def main():
    """æµ‹è¯•å…¥å£"""
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--kafka", action="store_true", help="ä½¿ç”¨Kafkaæ¨¡å¼")
    parser.add_argument("--pipeline", action="store_true", help="è¿è¡Œå®Œæ•´Pipeline")
    args = parser.parse_args()
    
    if args.pipeline:
        run_kafka_pipeline()
    else:
        crawler = KafkaIntegratedCrawler(use_kafka=args.kafka)
        crawler.crawl_with_kafka()
        crawler.close()


if __name__ == "__main__":
    main()
