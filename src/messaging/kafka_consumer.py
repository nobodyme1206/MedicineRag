# -*- coding: utf-8 -*-
"""
Kafkaæ¶ˆè´¹è€… - ä»Kafkaæ¶ˆè´¹æ–‡ç« å¹¶å¤„ç†
æ”¯æŒå¤šæ¶ˆè´¹è€…å¹¶è¡Œå¤„ç†ï¼Œå®ç°é«˜ååé‡
"""

import json
import time
import threading
from typing import Dict, List, Callable, Optional
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
import sys

sys.path.append(str(Path(__file__).parent.parent.parent))

from config.config import LOGS_DIR, PROCESSED_DATA_DIR
from src.utils.logger import setup_logger

logger = setup_logger("kafka_consumer", LOGS_DIR / "kafka_consumer.log")

# Kafkaé…ç½®
KAFKA_BOOTSTRAP_SERVERS = "localhost:9092"
KAFKA_TOPIC_RAW = "medical_raw_articles"
KAFKA_TOPIC_PROCESSED = "medical_processed"
KAFKA_TOPIC_EMBEDDINGS = "medical_embeddings"
KAFKA_CONSUMER_GROUP = "medical_rag_processors"


class KafkaArticleConsumer:
    """Kafkaæ–‡ç« æ¶ˆè´¹è€… - æ¶ˆè´¹å¹¶å¤„ç†æ–‡ç« """
    
    def __init__(self, 
                 topic: str = KAFKA_TOPIC_RAW,
                 group_id: str = KAFKA_CONSUMER_GROUP,
                 bootstrap_servers: str = KAFKA_BOOTSTRAP_SERVERS,
                 auto_offset_reset: str = 'earliest'):
        """
        åˆå§‹åŒ–Kafkaæ¶ˆè´¹è€…
        
        Args:
            topic: è®¢é˜…çš„topic
            group_id: æ¶ˆè´¹è€…ç»„IDï¼ˆåŒç»„æ¶ˆè´¹è€…è‡ªåŠ¨è´Ÿè½½å‡è¡¡ï¼‰
            bootstrap_servers: KafkaæœåŠ¡å™¨åœ°å€
            auto_offset_reset: åç§»é‡é‡ç½®ç­–ç•¥
        """
        self.topic = topic
        self.group_id = group_id
        self.bootstrap_servers = bootstrap_servers
        self.consumer = None
        self.running = False
        
        # ç»Ÿè®¡
        self.stats = {
            "consumed": 0,
            "processed": 0,
            "failed": 0,
            "bytes_consumed": 0
        }
        
        # æ‰¹å¤„ç†ç¼“å†²
        self.buffer: List[Dict] = []
        self.buffer_size = 100  # æ¯100æ¡å¤„ç†ä¸€æ¬¡
        self.buffer_lock = threading.Lock()
        
        self._init_consumer(auto_offset_reset)
    
    def _init_consumer(self, auto_offset_reset: str):
        """åˆå§‹åŒ–Kafka Consumer"""
        try:
            from kafka import KafkaConsumer
            
            self.consumer = KafkaConsumer(
                self.topic,
                bootstrap_servers=self.bootstrap_servers,
                group_id=self.group_id,
                auto_offset_reset=auto_offset_reset,
                enable_auto_commit=True,
                auto_commit_interval_ms=5000,
                value_deserializer=lambda m: json.loads(m.decode('utf-8')),
                # æ€§èƒ½ä¼˜åŒ–
                fetch_min_bytes=1024 * 100,    # è‡³å°‘100KBæ‰è¿”å›
                fetch_max_wait_ms=500,          # æœ€å¤šç­‰å¾…500ms
                max_poll_records=500,           # æ¯æ¬¡æœ€å¤šæ‹‰å–500æ¡
                session_timeout_ms=30000,
                heartbeat_interval_ms=10000
            )
            logger.info(f"âœ… Kafka Consumerå·²è¿æ¥: {self.topic} (group: {self.group_id})")
            
        except ImportError:
            logger.warning("âš ï¸ kafka-pythonæœªå®‰è£…")
            self.consumer = None
        except Exception as e:
            logger.warning(f"âš ï¸ Kafkaè¿æ¥å¤±è´¥: {e}")
            self.consumer = None
    
    def consume_and_process(self, 
                           processor: Callable[[List[Dict]], None],
                           batch_size: int = 100,
                           timeout_ms: int = 1000):
        """
        æ¶ˆè´¹æ¶ˆæ¯å¹¶æ‰¹é‡å¤„ç†
        
        Args:
            processor: å¤„ç†å‡½æ•°ï¼Œæ¥æ”¶æ–‡ç« åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°
            timeout_ms: æ‹‰å–è¶…æ—¶æ—¶é—´
        """
        if not self.consumer:
            logger.error("Consumeræœªåˆå§‹åŒ–")
            return
        
        self.running = True
        self.buffer_size = batch_size
        
        logger.info(f"ğŸš€ å¼€å§‹æ¶ˆè´¹ {self.topic}ï¼Œæ‰¹æ¬¡å¤§å°: {batch_size}")
        
        try:
            while self.running:
                # æ‹‰å–æ¶ˆæ¯
                messages = self.consumer.poll(timeout_ms=timeout_ms)
                
                for topic_partition, records in messages.items():
                    for record in records:
                        article = record.value
                        self.stats["consumed"] += 1
                        
                        with self.buffer_lock:
                            self.buffer.append(article)
                            
                            # è¾¾åˆ°æ‰¹æ¬¡å¤§å°ï¼Œè§¦å‘å¤„ç†
                            if len(self.buffer) >= self.buffer_size:
                                self._process_buffer(processor)
                
                # å®šæœŸå¤„ç†å‰©ä½™æ•°æ®
                if self.buffer and len(self.buffer) > 0:
                    with self.buffer_lock:
                        if len(self.buffer) > 0:
                            self._process_buffer(processor)
                            
        except KeyboardInterrupt:
            logger.info("æ”¶åˆ°åœæ­¢ä¿¡å·")
        finally:
            self._flush_buffer(processor)
            self.stop()
    
    def _process_buffer(self, processor: Callable[[List[Dict]], None]):
        """å¤„ç†ç¼“å†²åŒºä¸­çš„æ•°æ®"""
        if not self.buffer:
            return
        
        batch = self.buffer.copy()
        self.buffer.clear()
        
        try:
            start_time = time.time()
            processor(batch)
            elapsed = time.time() - start_time
            
            self.stats["processed"] += len(batch)
            logger.info(f"âœ… å¤„ç†æ‰¹æ¬¡: {len(batch)} æ¡, è€—æ—¶: {elapsed:.2f}s, "
                       f"é€Ÿç‡: {len(batch)/elapsed:.1f} æ¡/ç§’")
            
        except Exception as e:
            logger.error(f"å¤„ç†å¤±è´¥: {e}")
            self.stats["failed"] += len(batch)
    
    def _flush_buffer(self, processor: Callable[[List[Dict]], None]):
        """åˆ·æ–°å‰©ä½™ç¼“å†²åŒº"""
        with self.buffer_lock:
            if self.buffer:
                self._process_buffer(processor)
    
    def stop(self):
        """åœæ­¢æ¶ˆè´¹"""
        self.running = False
        if self.consumer:
            self.consumer.close()
            logger.info("Kafka Consumerå·²å…³é—­")
    
    def get_stats(self) -> Dict:
        """è·å–æ¶ˆè´¹ç»Ÿè®¡"""
        return self.stats


class DataProcessingConsumer(KafkaArticleConsumer):
    """æ•°æ®å¤„ç†æ¶ˆè´¹è€… - æ¶ˆè´¹åŸå§‹æ–‡ç« ï¼Œå¤„ç†åå‘é€åˆ°ä¸‹ä¸€ä¸ªTopic"""
    
    def __init__(self):
        super().__init__(
            topic=KAFKA_TOPIC_RAW,
            group_id="data_processors"
        )
        self.output_producer = None
        self._init_output_producer()
    
    def _init_output_producer(self):
        """åˆå§‹åŒ–è¾“å‡ºç”Ÿäº§è€…"""
        from src.messaging.kafka_producer import KafkaArticleProducer
        self.output_producer = KafkaArticleProducer()
    
    def process_articles(self, articles: List[Dict]):
        """
        å¤„ç†æ–‡ç« ï¼šæ¸…æ´— + åˆ‡åˆ† + å‘é€åˆ°ä¸‹æ¸¸
        """
        from src.data_processing.data_processor import DataProcessor
        
        # ç®€åŒ–å¤„ç†ï¼šç›´æ¥åˆ‡åˆ†æ–‡æœ¬
        chunks = []
        for article in articles:
            text = article.get('full_text', '')
            if len(text) < 100:
                continue
            
            # ç®€å•åˆ‡åˆ†
            chunk_size = 512
            overlap = 50
            start = 0
            chunk_id = 0
            
            while start < len(text):
                end = start + chunk_size
                chunk_text = text[start:end].strip()
                
                if len(chunk_text) >= 100:
                    chunk = {
                        "pmid": article.get("pmid"),
                        "chunk_id": f"{article.get('pmid')}_{chunk_id}",
                        "chunk_text": chunk_text,
                        "title": article.get("title", ""),
                        "topic": article.get("topic", "")
                    }
                    chunks.append(chunk)
                    chunk_id += 1
                
                start += (chunk_size - overlap)
        
        # å‘é€åˆ°å¤„ç†åçš„Topic
        if self.output_producer and chunks:
            for chunk in chunks:
                self.output_producer.send_processing_request(chunk)
            self.output_producer.producer.flush() if self.output_producer.producer else None
        
        logger.info(f"å¤„ç†å®Œæˆ: {len(articles)} æ–‡ç«  â†’ {len(chunks)} chunks")
    
    def start(self):
        """å¯åŠ¨å¤„ç†"""
        self.consume_and_process(
            processor=self.process_articles,
            batch_size=100
        )


class EmbeddingConsumer(KafkaArticleConsumer):
    """å‘é‡åŒ–æ¶ˆè´¹è€… - æ¶ˆè´¹å¤„ç†åçš„chunksï¼Œç”Ÿæˆå‘é‡"""
    
    def __init__(self):
        super().__init__(
            topic=KAFKA_TOPIC_PROCESSED,
            group_id="embedding_workers"
        )
        self.embedder = None
        self.milvus_manager = None
    
    def _init_embedder(self):
        """å»¶è¿Ÿåˆå§‹åŒ–Embedderï¼ˆGPUèµ„æºï¼‰"""
        if self.embedder is None:
            from src.embedding.embedder import TextEmbedder
            self.embedder = TextEmbedder()
            logger.info("Embedderå·²åˆå§‹åŒ–")
    
    def _init_milvus(self):
        """å»¶è¿Ÿåˆå§‹åŒ–Milvus"""
        if self.milvus_manager is None:
            from src.retrieval.milvus_manager import MilvusManager
            self.milvus_manager = MilvusManager()
            logger.info("Milvuså·²è¿æ¥")
    
    def process_chunks(self, chunks: List[Dict]):
        """
        å¤„ç†chunksï¼šå‘é‡åŒ– + å…¥åº“
        """
        self._init_embedder()
        self._init_milvus()
        
        # æå–æ–‡æœ¬
        texts = [c.get("chunk_text", "") for c in chunks]
        
        # æ‰¹é‡å‘é‡åŒ–
        embeddings = self.embedder.encode_batch(texts, show_progress=False)
        
        # å‡†å¤‡å…¥åº“æ•°æ®
        entities = []
        for i, chunk in enumerate(chunks):
            entities.append({
                "chunk_id": chunk.get("chunk_id", f"chunk_{i}"),
                "pmid": chunk.get("pmid", ""),
                "chunk_text": chunk.get("chunk_text", ""),
                "embedding": embeddings[i].tolist()
            })
        
        # æ‰¹é‡æ’å…¥Milvus
        if self.milvus_manager:
            try:
                self.milvus_manager.insert_batch(entities)
                logger.info(f"å‘é‡å…¥åº“: {len(entities)} æ¡")
            except Exception as e:
                logger.error(f"Milvusæ’å…¥å¤±è´¥: {e}")
    
    def start(self):
        """å¯åŠ¨å‘é‡åŒ–å¤„ç†"""
        self.consume_and_process(
            processor=self.process_chunks,
            batch_size=64  # GPUæ‰¹æ¬¡å°ä¸€äº›
        )


def run_data_processor():
    """è¿è¡Œæ•°æ®å¤„ç†æ¶ˆè´¹è€…"""
    consumer = DataProcessingConsumer()
    consumer.start()


def run_embedding_worker():
    """è¿è¡Œå‘é‡åŒ–æ¶ˆè´¹è€…"""
    consumer = EmbeddingConsumer()
    consumer.start()


def main():
    """æµ‹è¯•æ¶ˆè´¹è€…"""
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--type", choices=["processor", "embedder"], default="processor")
    args = parser.parse_args()
    
    if args.type == "processor":
        run_data_processor()
    else:
        run_embedding_worker()


if __name__ == "__main__":
    main()
