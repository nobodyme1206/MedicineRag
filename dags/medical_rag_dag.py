# -*- coding: utf-8 -*-
"""
Airflow DAG - åŒ»å­¦RAGç³»ç»ŸPipelineç¼–æ’
å®ç°ä»»åŠ¡è°ƒåº¦ã€ä¾èµ–ç®¡ç†ã€å¤±è´¥é‡è¯•ã€ç›‘æ§å‘Šè­¦
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.operators.dummy import DummyOperator
from airflow.utils.trigger_rule import TriggerRule
from airflow.models import Variable
import os
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_DIR = os.environ.get('PROJECT_DIR', '/opt/airflow/medical-rag')
sys.path.insert(0, PROJECT_DIR)


# ==================== é»˜è®¤å‚æ•° ====================
default_args = {
    'owner': 'medical-rag',
    'depends_on_past': False,
    'email': ['admin@example.com'],
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
    'retry_exponential_backoff': True,
    'max_retry_delay': timedelta(minutes=30),
    'execution_timeout': timedelta(hours=6),
}


# ==================== ä»»åŠ¡å‡½æ•° ====================

def collect_pubmed_data(**context):
    """
    ä»»åŠ¡1: PubMedæ•°æ®é‡‡é›†
    - æ”¯æŒæ–­ç‚¹ç»­ä¼ 
    - é‡‡é›†ç»“æœå‘é€åˆ°Kafka
    """
    from src.data_processing.pubmed_crawler import AsyncPubMedCrawler
    from src.messaging.kafka_producer import KafkaArticleProducer
    
    # è·å–é…ç½®å‚æ•°
    max_per_topic = int(Variable.get("pubmed_max_per_topic", default_var=5000))
    
    crawler = AsyncPubMedCrawler()
    producer = KafkaArticleProducer()
    
    # çˆ¬å–æ•°æ®
    articles = crawler.crawl_all_topics()
    
    # å‘é€åˆ°Kafkaï¼ˆå¦‚æœKafkaå¯ç”¨ï¼‰
    if producer.producer:
        sent = producer.send_batch(articles)
        context['ti'].xcom_push(key='kafka_sent', value=sent)
    
    # è¿”å›ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'total_articles': len(articles),
        'topics_completed': len(crawler.completed_topics),
        'kafka_sent': producer.get_stats()['sent']
    }
    
    producer.close()
    
    return stats


def process_data_spark(**context):
    """
    ä»»åŠ¡2: Sparkæ•°æ®å¤„ç†
    - æ•°æ®æ¸…æ´—
    - æ–‡æœ¬åˆ‡åˆ†
    - ä¿å­˜ä¸ºParquet
    """
    from src.data_processing.data_processor import DataProcessor
    from config.config import RAW_DATA_DIR, PROCESSED_DATA_DIR
    
    processor = DataProcessor(use_cluster=False)
    
    try:
        # æŸ¥æ‰¾è¾“å…¥æ–‡ä»¶
        input_file = RAW_DATA_DIR / "pubmed_articles_all.json"
        if not input_file.exists():
            input_file = RAW_DATA_DIR / "pubmed_expanded.jsonl"
        
        if not input_file.exists():
            raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        
        output_file = PROCESSED_DATA_DIR / "medical_chunks"
        
        # å¤„ç†æ•°æ®
        processor.process_full_dataset(input_file, output_file)
        
        # è·å–å¤„ç†ç»“æœç»Ÿè®¡
        import pandas as pd
        df = pd.read_parquet(output_file.with_suffix('.parquet'))
        
        stats = {
            'input_file': str(input_file),
            'output_file': str(output_file),
            'chunk_count': len(df),
            'columns': list(df.columns)
        }
        
        return stats
        
    finally:
        processor.stop()


def generate_embeddings(**context):
    """
    ä»»åŠ¡3: å‘é‡åŒ–
    - GPUåŠ é€Ÿ
    - æ‰¹é‡å¤„ç†
    """
    from src.embedding.embedder import TextEmbedder
    from config.config import PROCESSED_DATA_DIR, EMBEDDING_DATA_DIR
    
    embedder = TextEmbedder()
    
    input_file = PROCESSED_DATA_DIR / "medical_chunks.parquet"
    output_file = EMBEDDING_DATA_DIR / "medical_embeddings"
    
    embeddings, metadata = embedder.embed_dataset(input_file, output_file)
    
    return {
        'total_vectors': metadata['total_count'],
        'dimension': metadata['dimension'],
        'throughput': metadata['throughput']
    }


def update_milvus_index(**context):
    """
    ä»»åŠ¡4: æ›´æ–°Milvuså‘é‡ç´¢å¼•
    - æ‰¹é‡æ’å…¥
    - ç´¢å¼•æ„å»º
    """
    from src.retrieval.milvus_manager import rebuild_database
    
    # æ‰§è¡Œé‡å»ºï¼ˆæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰
    rebuild_database(resume=True, batch_size=256)
    
    return {'status': 'completed'}


def run_evaluation(**context):
    """
    ä»»åŠ¡5: ç³»ç»Ÿè¯„ä¼°
    - RAGæ£€ç´¢è¯„ä¼°
    - æ€§èƒ½è¯„ä¼°
    """
    from src.evaluation.unified_evaluator import UnifiedEvaluator
    
    evaluator = UnifiedEvaluator()
    results = evaluator.run_full_evaluation()
    
    # æå–å…³é”®æŒ‡æ ‡
    metrics = {
        'overall_score': results.get('overall_score', 0),
        'precision_at_5': results.get('retrieval', {}).get('precision_at_5', 0),
        'mrr': results.get('retrieval', {}).get('mrr', 0),
        'hit_rate': results.get('retrieval', {}).get('hit_rate', 0)
    }
    
    # æ£€æŸ¥æ˜¯å¦è¾¾æ ‡
    if metrics['overall_score'] < 60:
        raise ValueError(f"è¯„ä¼°åˆ†æ•°è¿‡ä½: {metrics['overall_score']}/100")
    
    return metrics


def send_notification(**context):
    """
    ä»»åŠ¡6: å‘é€é€šçŸ¥
    - æ±‡æ€»Pipelineç»“æœ
    - å‘é€å‘Šè­¦/æŠ¥å‘Š
    """
    ti = context['ti']
    
    # æ”¶é›†å„ä»»åŠ¡ç»“æœ
    collect_stats = ti.xcom_pull(task_ids='collect_pubmed')
    process_stats = ti.xcom_pull(task_ids='process_data')
    embed_stats = ti.xcom_pull(task_ids='generate_embeddings')
    eval_stats = ti.xcom_pull(task_ids='run_evaluation')
    
    report = f"""
    ========== Medical RAG Pipeline å®ŒæˆæŠ¥å‘Š ==========
    
    ğŸ“Š æ•°æ®é‡‡é›†:
       - æ–‡ç« æ•°: {collect_stats.get('total_articles', 'N/A') if collect_stats else 'N/A'}
       - ä¸»é¢˜æ•°: {collect_stats.get('topics_completed', 'N/A') if collect_stats else 'N/A'}
    
    âš™ï¸ æ•°æ®å¤„ç†:
       - Chunksæ•°: {process_stats.get('chunk_count', 'N/A') if process_stats else 'N/A'}
    
    ğŸ”¢ å‘é‡åŒ–:
       - å‘é‡æ•°: {embed_stats.get('total_vectors', 'N/A') if embed_stats else 'N/A'}
       - ååé‡: {embed_stats.get('throughput', 'N/A'):.1f} æ¡/ç§’ if embed_stats else 'N/A'
    
    ğŸ“ˆ è¯„ä¼°ç»“æœ:
       - ç»¼åˆè¯„åˆ†: {eval_stats.get('overall_score', 'N/A') if eval_stats else 'N/A'}/100
       - Precision@5: {eval_stats.get('precision_at_5', 'N/A') if eval_stats else 'N/A'}
       - MRR: {eval_stats.get('mrr', 'N/A') if eval_stats else 'N/A'}
    
    ================================================
    """
    
    print(report)
    
    # è¿™é‡Œå¯ä»¥é›†æˆé’‰é’‰/ä¼ä¸šå¾®ä¿¡/é‚®ä»¶é€šçŸ¥
    # send_dingtalk_message(report)
    # send_email(report)
    
    return {'report': report}


# ==================== DAGå®šä¹‰ ====================

# DAG 1: æ¯æ—¥å¢é‡æ›´æ–°
with DAG(
    dag_id='medical_rag_daily',
    default_args=default_args,
    description='åŒ»å­¦RAGç³»ç»Ÿæ¯æ—¥å¢é‡æ›´æ–°',
    schedule_interval='0 2 * * *',  # æ¯å¤©å‡Œæ™¨2ç‚¹
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=['medical-rag', 'daily'],
    max_active_runs=1,
) as dag_daily:
    
    start = DummyOperator(task_id='start')
    
    collect_task = PythonOperator(
        task_id='collect_pubmed',
        python_callable=collect_pubmed_data,
        provide_context=True,
    )
    
    process_task = PythonOperator(
        task_id='process_data',
        python_callable=process_data_spark,
        provide_context=True,
    )
    
    embed_task = PythonOperator(
        task_id='generate_embeddings',
        python_callable=generate_embeddings,
        provide_context=True,
        # GPUä»»åŠ¡å¯èƒ½éœ€è¦æ›´å¤šèµ„æº
        pool='gpu_pool',
    )
    
    index_task = PythonOperator(
        task_id='update_milvus',
        python_callable=update_milvus_index,
        provide_context=True,
    )
    
    notify_task = PythonOperator(
        task_id='send_notification',
        python_callable=send_notification,
        provide_context=True,
        trigger_rule=TriggerRule.ALL_DONE,  # æ— è®ºæˆåŠŸå¤±è´¥éƒ½æ‰§è¡Œ
    )
    
    end = DummyOperator(task_id='end')
    
    # å®šä¹‰ä¾èµ–å…³ç³»
    start >> collect_task >> process_task >> embed_task >> index_task >> notify_task >> end


# DAG 2: æ¯å‘¨å®Œæ•´è¯„ä¼°
with DAG(
    dag_id='medical_rag_weekly_eval',
    default_args=default_args,
    description='åŒ»å­¦RAGç³»ç»Ÿæ¯å‘¨è¯„ä¼°',
    schedule_interval='0 6 * * 0',  # æ¯å‘¨æ—¥æ—©ä¸Š6ç‚¹
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=['medical-rag', 'evaluation'],
    max_active_runs=1,
) as dag_weekly:
    
    eval_task = PythonOperator(
        task_id='run_evaluation',
        python_callable=run_evaluation,
        provide_context=True,
    )
    
    notify_eval = PythonOperator(
        task_id='notify_evaluation',
        python_callable=send_notification,
        provide_context=True,
    )
    
    eval_task >> notify_eval


# DAG 3: æ‰‹åŠ¨è§¦å‘çš„å®Œæ•´Pipeline
with DAG(
    dag_id='medical_rag_full_pipeline',
    default_args=default_args,
    description='åŒ»å­¦RAGç³»ç»Ÿå®Œæ•´Pipelineï¼ˆæ‰‹åŠ¨è§¦å‘ï¼‰',
    schedule_interval=None,  # æ‰‹åŠ¨è§¦å‘
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=['medical-rag', 'manual'],
) as dag_full:
    
    start = DummyOperator(task_id='start')
    
    collect = PythonOperator(
        task_id='collect_pubmed',
        python_callable=collect_pubmed_data,
        provide_context=True,
    )
    
    process = PythonOperator(
        task_id='process_data',
        python_callable=process_data_spark,
        provide_context=True,
    )
    
    embed = PythonOperator(
        task_id='generate_embeddings',
        python_callable=generate_embeddings,
        provide_context=True,
    )
    
    index = PythonOperator(
        task_id='update_milvus',
        python_callable=update_milvus_index,
        provide_context=True,
    )
    
    evaluate = PythonOperator(
        task_id='run_evaluation',
        python_callable=run_evaluation,
        provide_context=True,
    )
    
    notify = PythonOperator(
        task_id='send_notification',
        python_callable=send_notification,
        provide_context=True,
        trigger_rule=TriggerRule.ALL_DONE,
    )
    
    end = DummyOperator(task_id='end')
    
    start >> collect >> process >> embed >> index >> evaluate >> notify >> end
